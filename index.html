
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Billy Ian's Short Leisure-time Wander</title>
  <meta name="author" content="Peng (Billy) Xu">

  
  <meta name="description" content="Disclaimer:
This post was completed in my spare time, with no relevance to my current work. And opinions in this post are my own, not my employers’. &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://billy-inn.github.io/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Billy Ian's Short Leisure-time Wander" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-69271262-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Billy Ian's Short Leisure-time Wander</a></h1>
  
    <h2>into learning, investment, intelligence and beyond</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="billy-inn.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <!--	<h2 class="entry-title" style="color:#FF0000">&emsp;&emsp;[Stick]<a href="blog/2015/12/27/trace-of-my-study-on-machine-learning/" style="color:#333333">Trace of My Study on Machine Learning</a></h2> -->

<div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models/">Shortcut Learning Hypothesis of Modern Language Models</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2022-03-21T17:33:56-04:00'><span class='date'>2022-03-21</span> <span class='time'>5:33 pm</span></time>
        
           | <a href="/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong><em>Disclaimer:</em></strong>
<strong><em>This post was completed in my spare time, with no relevance to my current work. And opinions in this post are my own, not my employers’.</em></strong></p>

<p>As the already gigantic modern language models become ever larger by the day, people seem to ignore many existing works discussing their limitations. In this blog post, I try to connect the results and observations from several such works to form the “shortcut learning hypothesis” of those language models. Such a hypothesis implies that modern language models are fundamentally flawed to effectively capture long-range dependencies or complicated structures of the data, <strong>if</strong> we still stick with the current training objective. Hopefully, this post can help people take a step back and ponder a bit first before going all-in into the current scale competition of language models.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://i.imgur.com/UQJLGPd.png" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">A 137B parameter language model fails to answer a simple question with prompting. (The example comes from <a href="https://arxiv.org/abs/2201.11903">this paper</a>)</td>
    </tr>
  </tbody>
</table>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2022/01/06/navigate-through-the-current-ai-job-market-a-retrospect/">Navigate Through the Current AI Job Market: A Retrospect</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2022-01-06T18:06:53-05:00'><span class='date'>2022-01-06</span> <span class='time'>6:06 pm</span></time>
        
           | <a href="/blog/2022/01/06/navigate-through-the-current-ai-job-market-a-retrospect/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2022/01/06/navigate-through-the-current-ai-job-market-a-retrospect/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Inspired by <a href="https://twitter.com/savvyRL/status/1446234152475377665?s=20">the fantastic talk</a> focusing on career path doing AI research by <a href="https://twitter.com/savvyRL">Rosanne Liu</a> and <a href="https://gordicaleksa.medium.com/how-i-got-a-job-at-deepmind-as-a-research-engineer-without-a-machine-learning-degree-1a45f2a781de">the amazing blog post</a> on landing a job at top-tier AI labs by <a href="https://twitter.com/gordic_aleksa">Aleksa Gordić</a>, I want to share my recent experience to offer a more pragmatic perspective. The position specturm in the current AI industry can be roughly depicted in the figure below:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://i.imgur.com/MB0SOte.png" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Figure 1: The AI Job Spectrum</td>
    </tr>
  </tbody>
</table>

<p> </p>

<p>While the aforementioned posts both focus on the research end of the spectrum, this post covers the whole range of the spectrum from my own experience. Out of the five virtual onsites I attended, I received three offers. One of them was a Machine Learning Software Engineer / Research Engineer role from Google. The other two were both Machine Learning Scientist roles from a large tech and a large financial company respectively. For the offer from Google, the team match process is quite lengthy; I’ve talked with eight different teams inside Google. All those different positions cover the whole spectrum from heavily research focused to purely product driven, as shown in the figure below:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://i.imgur.com/18UmZpq.png" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Figure 2: My options and their rough positions on the spectrum</td>
    </tr>
  </tbody>
</table>

<p> </p>

<p>It’s a long grind from submitting resumes to making the final decision. During this process, I’ve had more than 20 rounds of discussion with different hiring managers, directors and other more senior people from these three companies. In addition, I’ve consulted with many friends of mine in the industry to get advice.</p>

<p>At the beginning of my job search, my preferrence was leaning towards the research end of the spectrum. However, as the things developed, my perception on the AI industry also changed gradually. Evenutally I choose a team in Google leaning towards the applied end of the spectrum. It’s a team in Cloud AI working on dialogue systems which heavily depends on the latest NLP technology (my expertise), generates a lot of value already and has great growth potential on tap. At the same time, there are also plenty of opportunities to collaborate with the research teams inside Google.</p>

<p>In this post, I’d like to share my whole journey accommpanied with some high-level suggestions and my thought process towards the final decision. Hopefully, this retrospect can provide some useful information for those who are passionate about AI and hope to find a matched position in the industry.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2022/01/06/navigate-through-the-current-ai-job-market-a-retrospect/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2018/11/13/convex-optimization-5/">Notes on Convex Optimization (5): Newton's Method</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2018-11-13T23:25:12-05:00'><span class='date'>2018-11-13</span> <span class='time'>11:25 pm</span></time>
        
           | <a href="/blog/2018/11/13/convex-optimization-5/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2018/11/13/convex-optimization-5/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>For $x\in\mathbf{dom}\ f$, the vector</p>

<script type="math/tex; mode=display">\Delta x_{nt}=-\nabla^2 f(x)^{-1}\nabla f(x)</script>

<p>is called the <em>Newton step</em> (for $f$, at $x$).</p>

<h4 id="minimizer-of-second-order-approximation">Minimizer of second-order approximation</h4>

<p>The second-order Taylor approximation $\hat f$ of $f$ at $x$ is</p>

<p>\begin{equation}
\hat f(x+v) = f(x) + \nabla f(x)^T v + \frac12 v^T \nabla^2 f(x) v.
\tag{1} \label{eq:1}
\end{equation}</p>

<p>which is a convex quadratic function of $v$, and is minimized when $v=\Delta x_{nt}$. Thus, the Newton step $\Delta x_{nt}$ is what should be added to the point $x$ to minimize the second-order approximation of $f$ at $x$.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2018/11/13/convex-optimization-5/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2018/11/05/convex-optimization-4/">Notes on Convex Optimization (4): Gradient Descent Method</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2018-11-05T00:29:05-05:00'><span class='date'>2018-11-05</span> <span class='time'>12:29 am</span></time>
        
           | <a href="/blog/2018/11/05/convex-optimization-4/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2018/11/05/convex-optimization-4/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="descent-methods">Descent methods</h3>

<script type="math/tex; mode=display">x^{(k+1)}=x^{(k)} + t^{(k)}\Delta x^{(k)}</script>

<ul>
  <li>$f(x^{(k+1)}) &lt; f(x^{(k)})$</li>
  <li>$\Delta x$ is the <em>step</em> or <em>search direction</em>; $t$ is the <em>step size</em> or <em>step length</em></li>
  <li>from convexity, $\nabla f(x)^T \Delta x &lt;0$</li>
</ul>

<blockquote>
  <p><em>General descent method</em>. <br />
<strong>given</strong> a starting point $x \in \mathbf{dom} \enspace f$. <br />
<strong>repeat</strong> <br />
- Determine a descent direction $\Delta x$. <br />
- <em>Line search</em>. Choose a step size $t &gt; 0$. <br />
- Update. $x := x+ t\Delta x$.</p>

  <p><strong>until</strong> stopping criterion is satisfied</p>
</blockquote>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2018/11/05/convex-optimization-4/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/">Notes on Convex Optimization (3): Unconstrained Minimization Problems</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2018-09-29T15:15:12-04:00'><span class='date'>2018-09-29</span> <span class='time'>3:15 pm</span></time>
        
           | <a href="/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Unconstrained optimization problems are defined as follows:</p>

<p>\begin{equation}
\text{minimize}\quad f(x)
\tag{1} \label{eq:1}
\end{equation}</p>

<p>where $f: \mathbf{R}^n \rightarrow \mathbf{R}$ is convex and twice continously differentiable (which implies that $\mathbf{dom}\enspace f$ is open). We denote the optimal value $\inf_xf(x)=f(x^\ast)$, as $p^\ast$. Since $f$ is differentiable and convex, a necessary and sufficient condition for a point $x^\ast$ to be optimal is</p>

<p>\begin{equation}
\nabla f(x^\ast)=0.
\tag{2} \label{eq:2}
\end{equation}</p>

<p>Thus, solving the unconstrained minimization problem \eqref{eq:1} is the same as finding a solution of \eqref{eq:2}, which is a set of $n$ equations in the $n$ variables $x_1, \dots, x_n$. Usually, the problem must be solved by an iterative algorithm. By this we mean an algorithm that computes a sequence of points $x^{(0)}, x^{(1)}, \dots \in \mathbf{dom}\enspace f$ with $f(x^{(k)})\rightarrow p^\ast$ as $k\rightarrow\infty$. The algorithm is terminated when $f(x^{k}) - p^\ast \le \epsilon$, where $\epsilon&gt;0$ is some specified tolerance.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/12/14/esl-chapter10/">[Notes on Mathematics for ESL] Chapter 10: Boosting and Additive Trees</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-12-14T19:07:57-05:00'><span class='date'>2017-12-14</span> <span class='time'>7:07 pm</span></time>
        
           | <a href="/blog/2017/12/14/esl-chapter10/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2017/12/14/esl-chapter10/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="why-exponential-loss">10.5 Why Exponential Loss?</h3>

<h4 id="derivation-of-equation-1016">Derivation of Equation (10.16)</h4>

<p>Since $Y\in{-1,1}$, we can expand the expectation as follows:</p>

<script type="math/tex; mode=display">\text{E}_{Y\vert x}(e^{-Yf(x)}) = \Pr(Y=1 \vert x)e^{-f(x)} + 
\Pr(Y=-1\vert x)e^{f(x)}</script>

<p>In order to minimize the expectation, we equal derivatives w.r.t. $f(x)$ as zero:</p>

<script type="math/tex; mode=display">-\Pr(Y=1\vert x)e^{-f(x)}+\Pr(Y=-1\vert x)e^{f(x)}=0</script>

<p>which gives:</p>

<script type="math/tex; mode=display">f^*(x)=\frac12\log\frac{\Pr(Y=1\vert x)}{\Pr(Y=-1\vert x)}</script>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/12/14/esl-chapter10/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/10/27/esl-chapter-6/">[Notes on Mathematics for ESL] Chapter 6: Kernel Smoothing Methods</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-10-27T18:57:21-04:00'><span class='date'>2017-10-27</span> <span class='time'>6:57 pm</span></time>
        
           | <a href="/blog/2017/10/27/esl-chapter-6/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2017/10/27/esl-chapter-6/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="one-dimensional-kernel-smoothers">6.1 One-Dimensional Kernel Smoothers</h3>

<h4 id="notes-on-local-linear-regression">Notes on Local Linear Regression</h4>

<p>Locally weighted regression solves a separate weighted least squares problem at each target point $x_0$:</p>

<script type="math/tex; mode=display">\min_{\alpha(x_0),\beta(x_0)}\sum_{i=1}^NK_\lambda(x_0,x_i)[y-\alpha(x_0)-\beta(x_0)x_i]^2</script>

<p>The estimate is  $\hat f(x_0)=\hat\alpha(x_0)+\hat\beta(x_0)x_0$. Define the vector-value function $b(x)^T=(1,x)$. Let $\mathbf{B}$ be the $N \times 2$ regression matrix with $i$th row row $b(x_i)^T$, $\mathbf{W}(x_0)$ the $N\times N$ diagonal matrix with $i$th diagonal element $K_\lambda (x_0, x_i)$, and $\theta=(\alpha(x_0), \beta(x_0))^T$.</p>

<p>Then the above optimization problem can be rewritten as</p>

<script type="math/tex; mode=display">\min_\theta(y-\mathbf{B}\theta)^T\mathbf{W}(x_0)(y-\mathbf{B}\theta)</script>

<p>Equal the derivative w.r.t $\theta$ as zero, we get</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
&\mathbf{B}^T\mathbf{W}(x_0)(y-\mathbf{B}\hat\theta)=0 \\
&\mathbf{B}^T\mathbf{W}(x_0)\mathbf{B}\hat\theta = \mathbf{B}^T\mathbf{W}(x_0)y \\
&\hat\theta= (\mathbf{B}^T\mathbf{W}(x_0)\mathbf{B})^{-1}\mathbf{B}\mathbf{W}(x_0)y
\end{split} %]]></script>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/10/27/esl-chapter-6/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/10/24/esl-chapter-5/">[Notes on Mathematics for ESL] Chapter 5: Basis Expansions and Regularization</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-10-24T20:34:25-04:00'><span class='date'>2017-10-24</span> <span class='time'>8:34 pm</span></time>
        
           | <a href="/blog/2017/10/24/esl-chapter-5/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2017/10/24/esl-chapter-5/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="smoothing-splines">5.4 Smoothing Splines</h3>

<h4 id="derivation-of-equation-512">Derivation of Equation (5.12)</h4>

<p>Equal the derivative of <strong>Equation (5.11)</strong> as zero, we get</p>

<script type="math/tex; mode=display">\frac{\partial\text{RSS}(\theta,\lambda)}{\partial\theta} = -2N^T(y-N\theta)+2\lambda\Omega_N\theta = 0</script>

<p>Put the terms related to $\theta$ on one side and the others on the other side, we get</p>

<script type="math/tex; mode=display">(N^TN+\lambda\Omega_N)\theta = N^Ty</script>

<p>Multiply the inverse of $N^TN+\lambda\Omega_N$ on both sides completes the derivation of <strong>Equation (5.12)</strong></p>

<script type="math/tex; mode=display">\hat\theta = (N^TN+\lambda\Omega_N)^{-1}N^Ty</script>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/10/24/esl-chapter-5/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/10/21/convex-optimization-2/">Notes on Convex Optimization (2): Convex Functions</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-10-21T13:59:09-04:00'><span class='date'>2017-10-21</span> <span class='time'>1:59 pm</span></time>
        
           | <a href="/blog/2017/10/21/convex-optimization-2/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2017/10/21/convex-optimization-2/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="basic-properties-and-examples">1. Basic Properties and Examples</h3>

<h4 id="definition">1.1 Definition</h4>

<p>$f:\mathbb{R}^n \rightarrow \mathbb R$ is convex if $\mathbf{dom}\ f$ is a convex set and</p>

<script type="math/tex; mode=display">f(\theta x+(1-\theta)y)\le\theta f(x)+(1-\theta)f(y)</script>

<p>for all $x,y\in \mathbf{dom}\ f, 0\le\theta\le1$</p>

<ul>
  <li>$f$ is concave if $-f$ is convex</li>
  <li>$f$ is strictly convex if $\mathbf{dom}\ f$ is convex and <script type="math/tex">% <![CDATA[
f(\theta x+(1-\theta)y)<\theta f(x)+(1-\theta)f(y) %]]></script> for $x,y\in\mathbf{dom}\ f,x\ne y, 0&lt;\theta&lt;1$</li>
</ul>

<p>$f:\mathbb{R}^n \rightarrow \mathbb R$ is convex if and only if the function $g: \mathbb{R} \rightarrow \mathbb{R}$,</p>

<script type="math/tex; mode=display">g(t)=f(x+tv),\quad\mathbf{dom}\ g=\{t\mid x+tv\in\mathbf{dom}\ f\}</script>

<p>is convex (in $t$) for any $x \in \mathbf{dom}\ f, v\in\mathbb R^n$</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/10/21/convex-optimization-2/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/10/17/convex-optimization-1/">Notes on Convex Optimization (1): Convex Sets</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-10-17T02:22:23-04:00'><span class='date'>2017-10-17</span> <span class='time'>2:22 am</span></time>
        
           | <a href="/blog/2017/10/17/convex-optimization-1/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2017/10/17/convex-optimization-1/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="affine-and-convex-sets">1. Affine and Convex Sets</h3>

<p>Suppose $x_1\ne x_2$ are two points in $\mathbb{R}^n$.</p>

<h4 id="affine-sets">1.1 Affine sets</h4>

<p><strong>line</strong> through $x_1$, $x_2$: all points</p>

<script type="math/tex; mode=display">x=\theta x_1 + (1-\theta)x_2\quad(\theta \in \mathbb{R})</script>

<p><strong>affine set</strong>: contains the line through any two distinct points in the set</p>

<script type="math/tex; mode=display">x_1,x_2\in C,\ \theta\in\mathbb{R} \Longrightarrow \theta x_1 + (1-\theta)x_2 \in C</script>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/10/17/convex-optimization-1/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/2">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
	<ul id="about" class="nav nav-list">
		<li class="nav-header">About Me</a></li>
			<p>A researcher and engineer passionate about Machine Learning and Natural Language Processing</p>
      <p>A calculated speculator seeking for risk and reward asymmetries, and a value investor sticking to the margin of safety.</p>
			<p>Play piano and Nintendo Switch</p>
      <p>Read history, investment, Sci-Fi and fantasy</p>
      <p>Bodyweight fitness, hiking and skiing</p>
			<p>Fan of Portland Trail Blazers, Former SNH48 member Kiku and Blackpink member Ros&eacute</p>
      <p>Toronto, Edmonton, Beijing and Suzhou</p>
      <p><strong>Opinions posted in this blog are my own!</strong></p>
	</ul>
</section>
<section>
  <h1>Latest Tweets</h1>
  <a class="twitter-timeline" data-chrome="nofooter transparent noheader" data-tweet-limit="5" href="https://twitter.com/billy_nlp">Tweets by billy_nlp</a> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
  <a class="twitter-follow-button" href="https://twitter.com/billy_nlp">Follow @billy_nlp</a>
</section>
<section class="well">
 <ul id="categories_list" class="nav nav-list">
	<li class="nav-header">Categories</li>
	  <li id="categories"><li class='category'><a href='/blog/categories/life/'>life (3)</a></li>
<li class='category'><a href='/blog/categories/machine-learning/'>machine_learning (11)</a></li>
<li class='category'><a href='/blog/categories/nlp/'>nlp (1)</a></li>
<li class='category'><a href='/blog/categories/optimization/'>optimization (5)</a></li>
<li class='category'><a href='/blog/categories/pgm/'>pgm (1)</a></li>
<li class='category'><a href='/blog/categories/project/'>project (1)</a></li>
<li class='category'><a href='/blog/categories/reading/'>reading (2)</a></li>
<li class='category'><a href='/blog/categories/reinforcement-learning/'>reinforcement_learning (4)</a></li>
<li class='category'><a href='/blog/categories/statistics/'>statistics (3)</a></li>
</li>
 </ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models/">Shortcut Learning Hypothesis of Modern Language Models</a>
      </li>
    
      <li class="post">
        <a href="/blog/2022/01/06/navigate-through-the-current-ai-job-market-a-retrospect/">Navigate Through the Current AI Job Market: A Retrospect</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/13/convex-optimization-5/">Notes on Convex Optimization (5): Newton's Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/05/convex-optimization-4/">Notes on Convex Optimization (4): Gradient Descent Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/">Notes on Convex Optimization (3): Unconstrained Minimization Problems</a>
      </li>
    
  </ul>
</section>
<section class="well">
  <ul id="license" class="nav nav-list">
    <li class="nav-header">License</li>
<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.
	</li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2022 - Peng (Billy) Xu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		  jax: ["input/TeX", "output/HTML-CSS"],
		    tex2jax: {
				    inlineMath: [ ['$', '$'] ],
					    displayMath: [ ['$$', '$$']],
						    processEscapes: true,
							    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
								  },
								    messageStyle: "none",
									  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
								  });
							  </script>
							  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'billy-inn';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
