
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Billy Ian's Short Leisure-time Wander</title>
  <meta name="author" content="Peng (Billy) Xu">

  
  <meta name="description" content="2.4 Statistical Decision Theory Derivation of Equation (2.16) The expected predicted error (EPE) under the squared error loss: Taking derivatives &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://billy-inn.github.io/posts/2/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Billy Ian's Short Leisure-time Wander" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-69271262-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Billy Ian's Short Leisure-time Wander</a></h1>
  
    <h2>into language, learning, intelligence and beyond</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="billy-inn.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <!--	<h2 class="entry-title" style="color:#FF0000">&emsp;&emsp;[Stick]<a href="blog/2015/12/27/trace-of-my-study-on-machine-learning/" style="color:#333333">Trace of My Study on Machine Learning</a></h2> -->

<div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/09/01/esl-chapter-2/">[Notes on Mathematics for ESL] Chapter 2: Overview of Supervised Learning</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-09-01T03:12:36-04:00'><span class='date'>2017-09-01</span> <span class='time'>3:12 am</span></time>
        
           | <a href="/blog/2017/09/01/esl-chapter-2/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2017/09/01/esl-chapter-2/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="statistical-decision-theory">2.4 Statistical Decision Theory</h3>

<h4 id="derivation-of-equation-216">Derivation of Equation (2.16)</h4>

<p>The expected predicted error (EPE) under the squared error loss:</p>

<script type="math/tex; mode=display">\mathrm{EPE}(\beta) = \int (y-x^T\beta)^2\Pr(dx, dy).</script>

<p>Taking derivatives with respect to $\beta$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
\frac{\partial\mathrm{EFE}}{\partial\beta}&=-2\int(y-x^T\beta)x\Pr(dx, dy). \\
&= -2(E[yx]-E[xx^T\beta])
\end{split} %]]></script>

<p>In order to minimize the EFE, we make derivatives equal zero which gives <strong>Equation (2.16)</strong>:</p>

<script type="math/tex; mode=display">\beta=E[xx^T]^{-1}E[yx].</script>

<p><em>Note: $x^T\beta$ is a scalar, and $\beta$ is a constant.</em></p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/09/01/esl-chapter-2/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/08/18/lln-and-clt/">统计释疑(3)：大数定理和中心极限定理</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-08-18T01:49:29-04:00'><span class='date'>2017-08-18</span> <span class='time'>1:49 am</span></time>
        
           | <a href="/blog/2017/08/18/lln-and-clt/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2017/08/18/lln-and-clt/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>两个必须得记住并理解的统计学定理：大数定理和中心极限定理。有相当多的统计学理论是以这两个定理为基础。另外从我个人理解，这两个定理在一定程度上解释了为什么数据越多越好（为什么我们需要大数据）。</p>

<h3 id="section">收敛的类型</h3>

<p>为了更加准确的理解上述两个定理，我们需要理解概率层面的收敛，而非微积分里的收敛（如果对与任意$\epsilon&gt;0$和足够大的$n$，$\vert x_n -x \rvert &lt; \epsilon$，那么我们称这一实数列$x_n$收敛于极限$x$）。</p>

<p>在统计中，主要有两种类型的收敛：</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/08/18/lln-and-clt/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/08/10/probability-inequalities/">统计释疑(2)：概率不等式有什么用？</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-08-10T00:38:45-04:00'><span class='date'>2017-08-10</span> <span class='time'>12:38 am</span></time>
        
           | <a href="/blog/2017/08/10/probability-inequalities/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2017/08/10/probability-inequalities/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>在学习概率论或者一些统计课程的时候，往往会学到一系列各式各样稀奇古怪的不等式 (Inequalities)，然而却对于这些不等式的意义缺乏一个直观的认识。引申<em>“All of Statistics”</em>一书中的一个小例子可以给出一个很切合实际的解释。</p>

<h3 id="section">我的神经网络真的有效吗？</h3>

<p>假如我们用神经网络在MNIST数据集上训练了一个分类器，我们在测试集上得到了一个错误率，比如$0.05$。那么这是否意味着我们可以保证我们的神经网络一定能达到$95\%$的正确率呢？显然训练一次得出的结果是不可靠的。那么，我们有多大的把握（概率）来相信这一观察到的错误率呢？</p>

<p>这时候就需要一些统计的语言了：假设我们有$n$个测试样本，每个测试样本分类的正确与否都是一个随机变量$X_1,\dots,X_n$。如果分类错误$X_i=1$，否则$X_i=0$。显而易见，$\bar X_n=n^{-1}\sum_{i=1}^nX_i$就是观察到的错误率。我们可以把每个$X_i$当做一个均值为$p$服从Bernoulli分布的随机变量，从而$p$就是真正（但是永远无法准确知晓）的错误率。从我们的角度来看，我们希望$\bar X_n$应该接近$p$。那么$\bar X_n$和$p$的概率超过一个固定值$\epsilon$的概率有多大呢？
这个概率就是$\mathbb{P}(\lvert\bar X_n -p\rvert &gt; \epsilon)$，通常我们很难直接计算出它的值，这时我们就需要不等式来给这个概率设定一些边界 (bound)。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/08/10/probability-inequalities/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/07/28/p-value/">统计释疑(1)：什么是p值</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-28T22:33:16-04:00'><span class='date'>2017-07-28</span> <span class='time'>10:33 pm</span></time>
        
           | <a href="/blog/2017/07/28/p-value/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2017/07/28/p-value/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>某互联网公司招聘程序员，招聘的方法很简单，就是从LeetCode上找$10$道题，录取解出至少$8$道题的应试者。每隔一年，公司会根据新招聘程序员的表现评估上一年招聘的不合格率。公司的期望是每年招聘的不和格率要低于$5\%$。下面是历年的招聘数据：</p>

<table class="mbtablestyle">
  <thead>
    <tr>
      <th style="text-align: left">年份</th>
      <th style="text-align: left">面试人数</th>
      <th style="text-align: left">录取人数</th>
      <th style="text-align: left">不合格的人数</th>
      <th style="text-align: left">不合格率</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">2014</td>
      <td style="text-align: left">1000</td>
      <td style="text-align: left">350</td>
      <td style="text-align: left">30</td>
      <td style="text-align: left">8.57%</td>
    </tr>
    <tr>
      <td style="text-align: left">2015</td>
      <td style="text-align: left">1000</td>
      <td style="text-align: left">650</td>
      <td style="text-align: left">10</td>
      <td style="text-align: left">1.54%</td>
    </tr>
    <tr>
      <td style="text-align: left">2016</td>
      <td style="text-align: left">1000</td>
      <td style="text-align: left">200</td>
      <td style="text-align: left">10</td>
      <td style="text-align: left">5.00%</td>
    </tr>
  </tbody>
</table>

<p>那么这家公司的招聘策略是否有效呢？</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/07/28/p-value/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/01/11/lose-control-2/">【失控:机器、社会与经济的新生物学】漫谈（二）</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-01-11T18:13:46-05:00'><span class='date'>2017-01-11</span> <span class='time'>6:13 pm</span></time>
        
           | <a href="/blog/2017/01/11/lose-control-2/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2017/01/11/lose-control-2/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>随着人工智能大潮的火热，各种噱头在大公司和媒体的鼓吹下刺激着大众的神经。关于智械的各种“浪漫”幻想也不再仅仅诉诸于电影和小说，而是开始被严肃的讨论了起来。在这本书里，我看到了一个之前没有见过的有趣的观点：“机器是人类的一种进化形式”。显然这并非通常的经过亿万年自然选择产生的进化，而是一种定向的选择。就如同培育有机食品、杂交水稻一般，人工智能是否也可以定向的让人类自身变得更为强大呢？相比于制造通用智能，越来越多业界的人也都认为AI被定义为Augmented Intelligence（增强智能）而非Artifical Intelligence（人工智能）更加实际和贴切。总的来说，现在的AI还是高度面向任务的，大量人工标注的数据加上工程上的细节才能使计算机在特定任务上战胜人类，而这些努力一旦换了一个领域就难再有大的用武之地。另一方面，希望计算机或者机器人在某些领域完全取代人类也是不现实的，除了一些很基础的任务外，人类的介入在现阶段还是很有必要的，比如机器翻译，智能助理等等。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/01/11/lose-control-2/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/12/14/lose-control-1/">【失控: 机器、社会与经济的新生物学】漫谈（一）</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-12-14T23:44:01-05:00'><span class='date'>2016-12-14</span> <span class='time'>11:44 pm</span></time>
        
           | <a href="/blog/2016/12/14/lose-control-1/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2016/12/14/lose-control-1/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>重拾阅读后的第一本书，读而不思则罔，于是决定随着阅读随便写点什么，谓之“漫谈”。</p>

<p>虽然成书于94年，但书中种种观点和当今社会与技术的发展却有诸多不谋而合之处。读罢前几章，最为深刻的印象就是蜂群思维，分布式系统，去中心化等等一系列相关的概念。总而言之，论述的是一种与传统自上而下的系统相悖的自下而上的系统。这里的系统是一个非常宽泛的说法，它可以是机器，可以是软件，也可以是动物，乃至于人类社会、政治体系、万维网等等。可能是我实在是孤陋寡闻，在阅读这本书之前，我潜意识里确实认为绝大多数系统都应该有一个中心，拥有绝对的权威并下达指令，比如PC的CPU，古代的皇帝，人的大脑等等。然而这本书却提出了一个截然不同的系统，简而言之就是没有一个绝对的中心，每一个个体的行为决定了整体的行动（也可以简单的理解为少数服从多数，但实际情况往往更加复杂）并由简单的行为（操作）逐层向上模块化的增加更加复杂的行为（操作），另外分布式的存在方式令其拥有更强的容错性和在部分失灵的情况下能够继续运转的稳定性。现代科学在群体动物（蜂群、蚁群）中发现了这样的系统，而脑科学的发展也说明大脑并非我们原本想象的那样控制着人体的一切。无数的神经的共同作用造就了大脑，而大脑与身体的各种感官也似乎并非简单的从属关系。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2016/12/14/lose-control-1/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/">Notes on Reinforcement Learning (4): Temporal-Difference Learning</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-10-16T19:47:27-04:00'><span class='date'>2016-10-16</span> <span class='time'>7:47 pm</span></time>
        
           | <a href="/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Temporal-difference (TD) learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.</p>

<h3 id="td-prediction">TD Prediction</h3>

<p>Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\pi$, both methods update their estimate $v$ of $v_\pi$ for the nonterminal states $S_t$ occurring in that experience. Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need wait only until the next time step. The simplest TD method, known as TD(0), is</p>

<script type="math/tex; mode=display">V(S_t) = V(S_t) + \alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)].</script>

<p>TD methods combine the sampling of Monte Carlo with the bootstrapping of DP. As we shall see, with care and imagination this can take us a long way toward obtaining the advantages of both Monte Carlo and DP methods.</p>

<p><img src="/images/rl4.1.png" alt="Alt text" /></p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/10/14/notes-on-reinforcement-learning-3-monte-carlo-methods/">Notes on Reinforcement Learning (3): Monte Carlo Methods</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-10-14T18:07:35-04:00'><span class='date'>2016-10-14</span> <span class='time'>6:07 pm</span></time>
        
           | <a href="/blog/2016/10/14/notes-on-reinforcement-learning-3-monte-carlo-methods/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2016/10/14/notes-on-reinforcement-learning-3-monte-carlo-methods/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns. To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks.</p>

<h3 id="monte-carlo-prediction">Monte Carlo Prediction</h3>

<p>An obvious way to estimate the state-value function which is the expected return starting from that state from experience, is to average the returns observed after visits to that state. This idea underlies all Monte Carlo methods.</p>

<p>In particular, suppose we wish to estimate $v_\pi(s)$, the value of a state $s$ under policy $\pi$, given a set of episodes obtained by following $\pi$ and passing through $s$. Each occurrence of state $s$ in an episode is called a visit to $s$. The first-visit MC method estimates $v_\pi(s)$ as the average of the returns following from first visits to $s$, whereas every-visit MC method averages the returns following all visits to $s$.</p>

<p><img src="/images/rl3.1.png" alt="Alt text" /></p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2016/10/14/notes-on-reinforcement-learning-3-monte-carlo-methods/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/10/06/notes-on-reinforcement-learning-2-dynamic-programming/">Notes on Reinforcement Learning (2): Dynamic Programming</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-10-06T19:37:02-04:00'><span class='date'>2016-10-06</span> <span class='time'>7:37 pm</span></time>
        
           | <a href="/blog/2016/10/06/notes-on-reinforcement-learning-2-dynamic-programming/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2016/10/06/notes-on-reinforcement-learning-2-dynamic-programming/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="policy-evaluation">Policy Evaluation</h3>

<p>Consider a sequence of approximate value functions $v_0, v_1, v_2, \dots,$ each mapping $\mathcal{S}^+$ to $\mathbb{R}$. The initial approximation, $v_0$ is chosen arbitrarily, and each successive approximation is obtained by using the Bellman equation for $v_\pi$ as an update rule:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
v_{k+1}(s)&=\mathbb{E}_\pi[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s]\\
&= \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_k(s')],
\end{split} %]]></script>

<p>for all $s\in\mathcal{S}$.</p>

<p><img src="/images/rl2.1.png" alt="Alt text" /></p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2016/10/06/notes-on-reinforcement-learning-2-dynamic-programming/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes/">Notes on Reinforcement Learning (1): Finite Markov Decision Processes</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-10-05T16:55:31-04:00'><span class='date'>2016-10-05</span> <span class='time'>4:55 pm</span></time>
        
           | <a href="/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes/#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="the-agent-environment-interface">The Agent-Environment Interface</h3>

<p><img src="/images/rl1.1.png" alt="Alt text" /></p>

<ul>
  <li>The agent and environment interact at each of a sequence of discrete time steps, $t=0,1,2,3,\dots$</li>
  <li>At each time step $t$, the agent receives some representation of the environment’s state, $S_t\in\mathcal{S}$, where $\mathcal{S}$ is the set of possible states.</li>
  <li>On that basis, the agent selects an action, $A_t \in \mathcal{A}(S_t)$, where $\mathcal{A}(S_t)$ is the set of actions available in state $S_t$.</li>
  <li>One time step later, in part as a consequence of its action, the agent receives a numerical reward, $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$, and finds itself in a new state, $S_{t+1}$.</li>
</ul>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/3">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/index.html">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
	<ul id="about" class="nav nav-list">
		<li class="nav-header">About Me</a></li>
			<p>A researcher and engineer passionate about Machine Learning and Natural Language Processing</p>
			<p>Play piano and Nintendo Switch</p>
      <p>Read history, investment, Sci-Fi and fantasy</p>
			<p>Fan of Portland Trail Blazers, Former SNH48 member Kiku and Blackpink member Ros&eacute</p>
			<p>微博: <a href="http://www.weibo.com/u/2540837283">百里云_bly</a></p>
			<p>GitHub: <a href="http://github.com/billy-inn">billy-inn</a></p>
			<p>Email:<code>pxu4 [at] ualberta.ca</code></p>
	</ul>
</section>
<section>
  <h1>Latest Tweets</h1>
  <a class="twitter-timeline" data-chrome="nofooter transparent noheader" data-tweet-limit="5" href="https://twitter.com/billy_nlp">Tweets by billy_nlp</a> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
  <a class="twitter-follow-button" href="https://twitter.com/billy_nlp">Follow @billy_nlp</a>
</section>
<section class="well">
 <ul id="categories_list" class="nav nav-list">
	<li class="nav-header">Categories</li>
	  <li id="categories"><li class='category'><a href='/blog/categories/life/'>life (2)</a></li>
<li class='category'><a href='/blog/categories/machine-learning/'>machine_learning (11)</a></li>
<li class='category'><a href='/blog/categories/optimization/'>optimization (5)</a></li>
<li class='category'><a href='/blog/categories/pgm/'>pgm (1)</a></li>
<li class='category'><a href='/blog/categories/project/'>project (1)</a></li>
<li class='category'><a href='/blog/categories/reading/'>reading (2)</a></li>
<li class='category'><a href='/blog/categories/reinforcement-learning/'>reinforcement_learning (4)</a></li>
<li class='category'><a href='/blog/categories/statistics/'>statistics (3)</a></li>
</li>
 </ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2018/11/13/convex-optimization-5/">Notes on Convex Optimization (5): Newton's Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/05/convex-optimization-4/">Notes on Convex Optimization (4): Gradient Descent Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/">Notes on Convex Optimization (3): Unconstrained Minimization Problems</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/12/14/esl-chapter10/">[Notes on Mathematics for ESL] Chapter 10: Boosting and Additive Trees</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/10/27/esl-chapter-6/">[Notes on Mathematics for ESL] Chapter 6: Kernel Smoothing Methods</a>
      </li>
    
  </ul>
</section>
<section class="well">
  <ul id="license" class="nav nav-list">
    <li class="nav-header">License</li>
<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.
	</li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2021 - Peng (Billy) Xu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		  jax: ["input/TeX", "output/HTML-CSS"],
		    tex2jax: {
				    inlineMath: [ ['$', '$'] ],
					    displayMath: [ ['$$', '$$']],
						    processEscapes: true,
							    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
								  },
								    messageStyle: "none",
									  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
								  });
							  </script>
							  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'billy-inn';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
