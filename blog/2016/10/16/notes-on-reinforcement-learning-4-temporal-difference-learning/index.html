
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Notes on Reinforcement Learning (4): Temporal-Difference Learning - KRIS</title>
  <meta name="author" content="Peng Xu">

  
  <meta name="description" content="">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/bootstrap/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/bootstrap/responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <style type="text/css">
    body {
      padding-bottom: 40px;
    }
    h1 {
      margin-bottom: 15px;
    }
    img {
      max-width: 100%;
    }
    .sharing, .meta, .pager {
      margin: 20px 0px 20px 0px;
    }
    .page-footer p {
      text-align: center;
    }
  </style>
  <script src="/javascripts/libs/jquery.js"></script>
  <script src="/javascripts/libs/modernizr-2.0.js"></script>
  <script src="/javascripts/libs/bootstrap.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="KRIS" type="application/atom+xml">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
                       tex2jax: {
                       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                       processEscapes: true
                       }
                       });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
                       tex2jax: {
                       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                       }
                       });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
                      var all = MathJax.Hub.getAllJax(), i;
                      for(i=0; i < all.length; i += 1) {
                      all[i].SourceElement().parentNode.className += ' has-jax';
                      }
                      });
</script>
<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-69271262-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <nav role="navigation"><div class="navbar navbar-inverse">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">KRIS</a>

      <div class="nav-collapse">
        <ul class="nav">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>


        <ul class="nav pull-right" data-subscription="rss">
          <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
          
        </ul>

        
          <form class="pull-right navbar-search" action="https://www.google.com/search" method="get">
            <fieldset role="search">
              <input type="hidden" name="q" value="site:billy-inn.github.io" />
              <input class="search-query" type="text" name="q" results="0" placeholder="Search"/>
            </fieldset>
          </form>
        
      </div>
    </div>
  </div>
</div>
</nav>
  <div class="container">
    <div class="row-fluid">
      
<article class="hentry span9" role="article">

  
  <header class="page-header">
    
      <h1 class="entry-title">Notes on Reinforcement Learning (4): Temporal-Difference Learning</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-10-16T17:47:27-06:00'><span class='date'>2016-10-16</span> <span class='time'>5:47 pm</span></time>
        
         | <a href="#disqus_thread">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Temporal-difference (TD) learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.</p>

<h3 id="td-prediction">TD Prediction</h3>

<p>Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\pi$, both methods update their estimate $v$ of $v_\pi$ for the nonterminal states $S_t$ occurring in that experience. Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need wait only until the next time step. The simplest TD method, known as TD(0), is</p>

<script type="math/tex; mode=display">V(S_t) = V(S_t) + \alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)].</script>

<p>TD methods combine the sampling of Monte Carlo with the bootstrapping of DP. As we shall see, with care and imagination this can take us a long way toward obtaining the advantages of both Monte Carlo and DP methods.</p>

<p><img src="/images/rl4.1.png" alt="Alt text" /></p>

<!--more-->

<p>Note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_t+\gamma V(S_{t+1})$. This quantity, called the TD error, arises in various forms throughout reinforcement learning:</p>

<script type="math/tex; mode=display">\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t).</script>

<p>Also note that the Monte Carlo error can be written as a sum of TD errors:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
G_t - V(s_t) &= R_{t+1}+\gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1}) \\
&= \delta_t + \gamma (G_{t+1}-V(S_{t+1})) \\
&= \delta_t + \gamma \delta_{t+1} + \dots + \gamma^{T-t} (G_T - V(S_T))\\
&= \sum_{k=0}^{T-t-1}\gamma^k\delta_{t+k}
\end{split} %]]></script>

<p>This fact and its generalizations play important roles in the theory of TD learning.</p>

<h3 id="optimality-of-td0">Optimality of TD(0)</h3>

<p>Suppose there is available only a finite amount of experience, say 10 episodes or 100 time steps. In this case, a common approach with incremental learning methods is to present the experience repeatedly until the method converges upon an answer. Updates are made only after processing each complete batch of training data. We call this batch updating.</p>

<p>Batch Monte Carlo methods always find the estimates that minimize mean-squared error on the training set, whereas batch TD(0) always finds the estimates that would be exactly correct for the maximum-likelihood model of the Markov process. In this case, the maximum-likelihood estimate is the model of the Markov process formed in the obvious way from the observed episodes: the estimated transition probability from $i$ to $j$ is the fraction of observed transitions from $i$ that went to $j$, and the associated expected reward is the average of the rewards observed on those transitions. Given this model, we can compute the estimate of the value function that would be exactly correct if the model were exactly correct. This is called the certainty-equivalence estimate because it is equivalent to assuming that the estimate of the underlying process was known with certainty rather than being approximated. In general, batch TD(0) converges to the certainty-equivalence estimate.</p>

<h3 id="sarsa-on-policy-td-control">Sarsa: On-Policy TD Control</h3>

<p>As usual, we follow the pattern of generalized policy iteration (GPI), only this time using TD methods for the evaluation or prediction part.</p>

<p>In the previous section we considered transitions from state to state and learned the values of states. Now we consider transitions from state–action pair to state–action pair, and learn the values of state–action pairs. The theorems assuring the convergence of state values under TD(0) also apply to the corresponding algorithm for action values:</p>

<script type="math/tex; mode=display">Q(S_t,A_t)=Q(S_t,A_t)+\alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t+1}) - Q(S_t, A_t)].</script>

<p><img src="/images/rl4.2.png" alt="Alt text" /></p>

<h3 id="q-learning-off-policy-td-control">Q-learning: Off-Policy TD Control</h3>

<p>One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as Q-learning, defined by:</p>

<script type="math/tex; mode=display">Q(S_t, A_t)=Q(S_t, A_t)+\alpha[R_{t+1}+\gamma\max_aQ(S_{t+1},a)-Q(S_t,A_t)]</script>

<p>In this case, the learned action-value function, $Q$, directly approximates $q^*$, the optimal action-value function, independent of the policy being followed.</p>

<p><img src="/images/rl4.3.png" alt="Alt text" /></p>

<h3 id="expected-sarsa">Expected Sarsa</h3>

<p>Consider the algorithm with the update rule:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
Q(S_t,A_t)&=Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma \mathbb{E}[Q(S_{t+1},A_{t+1})|S_{t+1}]-Q(S_t,A_t)\right]\\
&=Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma\sum_a\pi(a|S_{t+1})Q(S_{t+1},a)-Q(S_t,A_t)\right]
\end{split} %]]></script>

<p>but that otherwise follows the schema of Q-learning. Given the next state $S_{t+1}$, this algorithm moves deterministically in the same direction as Sarsa moves in expectation, and accordingly it is called expected Sarsa.</p>

<p><img src="/images/rl4.4.png" alt="Alt text" /></p>

<h3 id="maximization-bias-and-double-learning">Maximization Bias and Double Learning</h3>

<p>All the control algorithms that we have discussed so far involve maximization in the construction of their target policies. In these algorithms, a maximum over estimated values is used implicitly as an estimate of the maximum value, which can lead to a significant positive bias. We call this maximization bias.</p>

<p>One way to view the problem is that it is due to using the same samples (plays) both to determine the maximizing action and to estimate its value. Suppose we divided the plays in two sets and used them to learn two independent estimates, call them $Q_1(a)$ and $Q_2(a)$, each an estimate of the true value $q(a)$, for all $a\in\mathcal{A}$. We could then use one estimate, say $Q_1$, 
to determine the maximizing action $A^*=\mathrm{argmax}_aQ_1(a)$, and the other, $Q_2$, to provide the estimate of its value,
$Q_2(A^*)=Q_2(\mathrm{argmax}_aQ_1(a))$. This estimate will then be unbiased in the sense that 
$\mathbb{E}[Q_2(A^*)]=q(A^*)$. We can also repeat the process with the role of the two estimates reversed to yield a second unbiased estimate 
$Q_1(A^*)=Q_1(\mathrm{argmax}_aQ_2(a))$. This is the idea of doubled learning.</p>

<p><img src="/images/rl4.5.png" alt="Alt text" /></p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Peng Xu</span></span>

      




<time class='entry-date' datetime='2016-10-16T17:47:27-06:00'><span class='date'>2016-10-16</span> <span class='time'>5:47 pm</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/reinforcement-learning/'>reinforcement_learning</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/" data-via="" data-counturl="http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/" >Tweet</a>
  
  
  
</div>

    
    
    <section>
      <h1>Comments</h1>
      <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
    </section>
    
    <ul class="pager">
      
      <li class="previous"><a class="basic-alignment left"
        href="/blog/2016/10/14/notes-on-reinforcement-learning-3-monte-carlo-methods/" title="Previous Post:
        Notes on Reinforcement Learning (3): Monte Carlo Methods">&laquo; Notes on Reinforcement Learning (3): Monte Carlo Methods</a></li>
      
      <li><a href="/blog/archives">Blog Archives</a></li>
      
    </ul>
  </footer>
</article>

<aside class="sidebar-nav span3">
  
    <section>
	<ul id="subtitle" class="nav nav-list">
		<img src="/images/subtitle.png">
		<p></p>
	</ul>
</section>
<section class="well">
 <ul id="about" class="nav nav-list">
	 <li class="nav-header"><a href="/about">About Me</a></li>
	 <p>Machine Learning Lover.</p>
	 <p>Now working hard on related mathematics, e.g. Statistics, Optimization.</p>
	 <p>Also interested in the applied domains like NLP, DM, CV, FE.</p>
	 <p>Welcome to communicate with me among above topics.</p>
	 <p>微博: <a href="http://www.weibo.com/u/2540837283">百里云_bly</a></p>
	 <p>GitHub: <a href="http://github.com/billy-inn">billy-inn</a></p>
	 <p>Email:<code>bly930725 [at] gmail.com</code></p>
 </ul>
</section>
<section class="well">
 <ul id="categories_list" class="nav nav-list">
	<li class="nav-header">Categories</li>
	  <li id="categories"><li class='category'><a href='/blog/categories/life/'>life (2)</a></li>
<li class='category'><a href='/blog/categories/machine-learning/'>machine_learning (5)</a></li>
<li class='category'><a href='/blog/categories/pgm/'>pgm (1)</a></li>
<li class='category'><a href='/blog/categories/project/'>project (1)</a></li>
<li class='category'><a href='/blog/categories/reinforcement-learning/'>reinforcement_learning (4)</a></li>
</li>
 </ul>
</section>
<section class="well">
  <ul id="recent_posts" class="nav nav-list">
    <li class="nav-header">Recent Posts</li>
    
      <li class="post">
        <a href="/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/">Notes on Reinforcement Learning (4): Temporal-Difference Learning</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/10/14/notes-on-reinforcement-learning-3-monte-carlo-methods/">Notes on Reinforcement Learning (3): Monte Carlo Methods</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/10/06/notes-on-reinforcement-learning-2-dynamic-programming/">Notes on Reinforcement Learning (2): Dynamic Programming</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes/">Notes on Reinforcement Learning (1): Finite Markov Decision Processes</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/09/20/ml-with-r-3-logistic-regression/">ML with R (3): Logistic Regression</a>
      </li>
    
  </ul>
</section>
<section class="well">
  <ul id="license" class="nav nav-list">
    <li class="nav-header">License</li>
<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.
	</li>
  </ul>
</section>

  
</aside>


    </div>
  </div>
  <footer role="contentinfo" class="page-footer"><hr>
<p>
  Copyright &copy; 2016 - Peng Xu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'billy-inn';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/';
        var disqus_url = 'http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
