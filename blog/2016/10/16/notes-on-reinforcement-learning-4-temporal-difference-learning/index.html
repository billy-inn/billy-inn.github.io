
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Notes on Reinforcement Learning (4): Temporal-Difference Learning - Billy Ian's Short Leisure-time Wander</title>
  <meta name="author" content="Peng (Billy) Xu">

  
  <meta name="description" content="Temporal-difference (TD) learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. TD Prediction Both TD and Monte Carlo &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Billy Ian's Short Leisure-time Wander" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-69271262-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Billy Ian's Short Leisure-time Wander</a></h1>
  
    <h2>into learning, investment, intelligence and beyond</h2>
  
</hgroup>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="/mathjax/tex-chtml.js" type="text/javascript"></script>
<!--script src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML" type="text/javascript"></script-->


</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="billy-inn.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Notes on Reinforcement Learning (4): Temporal-Difference Learning</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-10-16T19:47:27-04:00'><span class='date'>2016-10-16</span> <span class='time'>7:47 pm</span></time>
        
           | <a href="#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Temporal-difference (TD) learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.</p>

<h3 id="td-prediction">TD Prediction</h3>

<p>Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\pi$, both methods update their estimate $v$ of $v_\pi$ for the nonterminal states $S_t$ occurring in that experience. Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need wait only until the next time step. The simplest TD method, known as TD(0), is</p>

<script type="math/tex; mode=display">V(S_t) = V(S_t) + \alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)].</script>

<p>TD methods combine the sampling of Monte Carlo with the bootstrapping of DP. As we shall see, with care and imagination this can take us a long way toward obtaining the advantages of both Monte Carlo and DP methods.</p>

<p><img src="/images/rl4.1.png" alt="Alt text" /></p>

<!--more-->

<p>Note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_t+\gamma V(S_{t+1})$. This quantity, called the TD error, arises in various forms throughout reinforcement learning:</p>

<script type="math/tex; mode=display">\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t).</script>

<p>Also note that the Monte Carlo error can be written as a sum of TD errors:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
G_t - V(s_t) &= R_{t+1}+\gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1}) \\
&= \delta_t + \gamma (G_{t+1}-V(S_{t+1})) \\
&= \delta_t + \gamma \delta_{t+1} + \dots + \gamma^{T-t} (G_T - V(S_T))\\
&= \sum_{k=0}^{T-t-1}\gamma^k\delta_{t+k}
\end{split} %]]></script>

<p>This fact and its generalizations play important roles in the theory of TD learning.</p>

<h3 id="optimality-of-td0">Optimality of TD(0)</h3>

<p>Suppose there is available only a finite amount of experience, say 10 episodes or 100 time steps. In this case, a common approach with incremental learning methods is to present the experience repeatedly until the method converges upon an answer. Updates are made only after processing each complete batch of training data. We call this batch updating.</p>

<p>Batch Monte Carlo methods always find the estimates that minimize mean-squared error on the training set, whereas batch TD(0) always finds the estimates that would be exactly correct for the maximum-likelihood model of the Markov process. In this case, the maximum-likelihood estimate is the model of the Markov process formed in the obvious way from the observed episodes: the estimated transition probability from $i$ to $j$ is the fraction of observed transitions from $i$ that went to $j$, and the associated expected reward is the average of the rewards observed on those transitions. Given this model, we can compute the estimate of the value function that would be exactly correct if the model were exactly correct. This is called the certainty-equivalence estimate because it is equivalent to assuming that the estimate of the underlying process was known with certainty rather than being approximated. In general, batch TD(0) converges to the certainty-equivalence estimate.</p>

<h3 id="sarsa-on-policy-td-control">Sarsa: On-Policy TD Control</h3>

<p>As usual, we follow the pattern of generalized policy iteration (GPI), only this time using TD methods for the evaluation or prediction part.</p>

<p>In the previous section we considered transitions from state to state and learned the values of states. Now we consider transitions from state–action pair to state–action pair, and learn the values of state–action pairs. The theorems assuring the convergence of state values under TD(0) also apply to the corresponding algorithm for action values:</p>

<script type="math/tex; mode=display">Q(S_t,A_t)=Q(S_t,A_t)+\alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t+1}) - Q(S_t, A_t)].</script>

<p><img src="/images/rl4.2.png" alt="Alt text" /></p>

<h3 id="q-learning-off-policy-td-control">Q-learning: Off-Policy TD Control</h3>

<p>One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as Q-learning, defined by:</p>

<script type="math/tex; mode=display">Q(S_t, A_t)=Q(S_t, A_t)+\alpha[R_{t+1}+\gamma\max_aQ(S_{t+1},a)-Q(S_t,A_t)]</script>

<p>In this case, the learned action-value function, $Q$, directly approximates $q^*$, the optimal action-value function, independent of the policy being followed.</p>

<p><img src="/images/rl4.3.png" alt="Alt text" /></p>

<h3 id="expected-sarsa">Expected Sarsa</h3>

<p>Consider the algorithm with the update rule:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
Q(S_t,A_t)&=Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma \mathbb{E}[Q(S_{t+1},A_{t+1})|S_{t+1}]-Q(S_t,A_t)\right]\\
&=Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma\sum_a\pi(a|S_{t+1})Q(S_{t+1},a)-Q(S_t,A_t)\right]
\end{split} %]]></script>

<p>but that otherwise follows the schema of Q-learning. Given the next state $S_{t+1}$, this algorithm moves deterministically in the same direction as Sarsa moves in expectation, and accordingly it is called expected Sarsa.</p>

<p><img src="/images/rl4.4.png" alt="Alt text" /></p>

<h3 id="maximization-bias-and-double-learning">Maximization Bias and Double Learning</h3>

<p>All the control algorithms that we have discussed so far involve maximization in the construction of their target policies. In these algorithms, a maximum over estimated values is used implicitly as an estimate of the maximum value, which can lead to a significant positive bias. We call this maximization bias.</p>

<p>One way to view the problem is that it is due to using the same samples (plays) both to determine the maximizing action and to estimate its value. Suppose we divided the plays in two sets and used them to learn two independent estimates, call them $Q_1(a)$ and $Q_2(a)$, each an estimate of the true value $q(a)$, for all $a\in\mathcal{A}$. We could then use one estimate, say $Q_1$, 
to determine the maximizing action $A^*=\mathrm{argmax}_aQ_1(a)$, and the other, $Q_2$, to provide the estimate of its value,
$Q_2(A^*)=Q_2(\mathrm{argmax}_aQ_1(a))$. This estimate will then be unbiased in the sense that 
$\mathbb{E}[Q_2(A^*)]=q(A^*)$. We can also repeat the process with the role of the two estimates reversed to yield a second unbiased estimate 
$Q_1(A^*)=Q_1(\mathrm{argmax}_aQ_2(a))$. This is the idea of doubled learning.</p>

<p><img src="/images/rl4.5.png" alt="Alt text" /></p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Peng (Billy) Xu</span></span>

      




<time class='entry-date' datetime='2016-10-16T19:47:27-04:00'><span class='date'>2016-10-16</span> <span class='time'>7:47 pm</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/reinforcement-learning/'>reinforcement_learning</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/" data-via="billyinn93" data-counturl="http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/10/14/notes-on-reinforcement-learning-3-monte-carlo-methods/" title="Previous Post: Notes on Reinforcement Learning (3): Monte Carlo Methods">&laquo; Notes on Reinforcement Learning (3): Monte Carlo Methods</a>
      
      
        <a class="basic-alignment right" href="/blog/2016/12/14/lose-control-1/" title="Next Post: 【失控: 机器、社会与经济的新生物学】漫谈（一）">【失控: 机器、社会与经济的新生物学】漫谈（一） &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  <!--
    <section>
	<ul id="about" class="nav nav-list">
		<li class="nav-header">About Me</a></li>
			<p>A researcher and engineer passionate about Machine Learning and Natural Language Processing</p>
      <p>A calculated speculator seeking for risk and reward asymmetries, and a value investor sticking to the margin of safety.</p>
			<p>Play piano and Nintendo Switch</p>
      <p>Read history, investment, Sci-Fi and fantasy</p>
      <p>Bodyweight fitness, hiking and skiing</p>
			<p>Fan of Portland Trail Blazers, Former SNH48 member Kiku and Blackpink member Ros&eacute</p>
      <p>Toronto, Edmonton, Beijing and Suzhou</p>
      <p><strong>Opinions posted in this blog are my own!</strong></p>
	</ul>
</section>
<section>
  <h1>Latest Tweets</h1>
  <a class="twitter-timeline" data-chrome="nofooter transparent noheader" data-tweet-limit="5" href="https://twitter.com/billy_nlp">Tweets by billy_nlp</a> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
  <a class="twitter-follow-button" href="https://twitter.com/billy_nlp">Follow @billy_nlp</a>
</section>
<section class="well">
 <ul id="categories_list" class="nav nav-list">
	<li class="nav-header">Categories</li>
	  <li id="categories"><li class='category'><a href='/blog/categories/life/'>life (3)</a></li>
<li class='category'><a href='/blog/categories/machine-learning/'>machine_learning (11)</a></li>
<li class='category'><a href='/blog/categories/nlp/'>nlp (1)</a></li>
<li class='category'><a href='/blog/categories/optimization/'>optimization (5)</a></li>
<li class='category'><a href='/blog/categories/pgm/'>pgm (1)</a></li>
<li class='category'><a href='/blog/categories/project/'>project (1)</a></li>
<li class='category'><a href='/blog/categories/reading/'>reading (2)</a></li>
<li class='category'><a href='/blog/categories/reinforcement-learning/'>reinforcement_learning (4)</a></li>
<li class='category'><a href='/blog/categories/statistics/'>statistics (3)</a></li>
</li>
 </ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models/">Shortcut Learning Hypothesis of Modern Language Models</a>
      </li>
    
      <li class="post">
        <a href="/blog/2022/01/06/navigate-through-the-current-ai-job-market-a-retrospect/">Navigate Through the Current AI Job Market: A Retrospect</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/13/convex-optimization-5/">Notes on Convex Optimization (5): Newton's Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/05/convex-optimization-4/">Notes on Convex Optimization (4): Gradient Descent Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/">Notes on Convex Optimization (3): Unconstrained Minimization Problems</a>
      </li>
    
  </ul>
</section>
<section class="well">
  <ul id="license" class="nav nav-list">
    <li class="nav-header">License</li>
<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.
	</li>
  </ul>
</section>

    -->
  <section>
	<ul id="about" class="nav nav-list">
		<li class="nav-header">About Me</a></li>
			<p>A researcher and engineer passionate about Machine Learning and Natural Language Processing</p>
      <p>A calculated speculator seeking for risk and reward asymmetries, and a value investor sticking to the margin of safety.</p>
			<p>Play piano and Nintendo Switch</p>
      <p>Read history, investment, Sci-Fi and fantasy</p>
      <p>Bodyweight fitness, hiking and skiing</p>
			<p>Fan of Portland Trail Blazers, Former SNH48 member Kiku and Blackpink member Ros&eacute</p>
      <p>Toronto, Edmonton, Beijing and Suzhou</p>
      <p><strong>Opinions posted in this blog are my own!</strong></p>
	</ul>
</section>
<section>
  <h1>Latest Tweets</h1>
  <a class="twitter-timeline" data-chrome="nofooter transparent noheader" data-tweet-limit="5" href="https://twitter.com/billy_nlp">Tweets by billy_nlp</a> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
  <a class="twitter-follow-button" href="https://twitter.com/billy_nlp">Follow @billy_nlp</a>
</section>
<section class="well">
 <ul id="categories_list" class="nav nav-list">
	<li class="nav-header">Categories</li>
	  <li id="categories"><li class='category'><a href='/blog/categories/life/'>life (3)</a></li>
<li class='category'><a href='/blog/categories/machine-learning/'>machine_learning (11)</a></li>
<li class='category'><a href='/blog/categories/nlp/'>nlp (1)</a></li>
<li class='category'><a href='/blog/categories/optimization/'>optimization (5)</a></li>
<li class='category'><a href='/blog/categories/pgm/'>pgm (1)</a></li>
<li class='category'><a href='/blog/categories/project/'>project (1)</a></li>
<li class='category'><a href='/blog/categories/reading/'>reading (2)</a></li>
<li class='category'><a href='/blog/categories/reinforcement-learning/'>reinforcement_learning (4)</a></li>
<li class='category'><a href='/blog/categories/statistics/'>statistics (3)</a></li>
</li>
 </ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models/">Shortcut Learning Hypothesis of Modern Language Models</a>
      </li>
    
      <li class="post">
        <a href="/blog/2022/01/06/navigate-through-the-current-ai-job-market-a-retrospect/">Navigate Through the Current AI Job Market: A Retrospect</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/13/convex-optimization-5/">Notes on Convex Optimization (5): Newton's Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/05/convex-optimization-4/">Notes on Convex Optimization (4): Gradient Descent Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/">Notes on Convex Optimization (3): Unconstrained Minimization Problems</a>
      </li>
    
  </ul>
</section>
<section class="well">
  <ul id="license" class="nav nav-list">
    <li class="nav-header">License</li>
<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.
	</li>
  </ul>
</section>

</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2022 - Peng (Billy) Xu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>


</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'billy-inn';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/';
        var disqus_url = 'http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
