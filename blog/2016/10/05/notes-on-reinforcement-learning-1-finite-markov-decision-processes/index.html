
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Notes on Reinforcement Learning (1): Finite Markov Decision Processes - KRIS</title>
  <meta name="author" content="Peng Xu">

  
  <meta name="description" content="">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://billy-inn.github.io/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/bootstrap/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/bootstrap/responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <style type="text/css">
    body {
      padding-bottom: 40px;
    }
    h1 {
      margin-bottom: 15px;
    }
    img {
      max-width: 100%;
    }
    .sharing, .meta, .pager {
      margin: 20px 0px 20px 0px;
    }
    .page-footer p {
      text-align: center;
    }
  </style>
  <script src="/javascripts/libs/jquery.js"></script>
  <script src="/javascripts/libs/modernizr-2.0.js"></script>
  <script src="/javascripts/libs/bootstrap.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="KRIS" type="application/atom+xml">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
                       tex2jax: {
                       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                       processEscapes: true
                       }
                       });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
                       tex2jax: {
                       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                       }
                       });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
                      var all = MathJax.Hub.getAllJax(), i;
                      for(i=0; i < all.length; i += 1) {
                      all[i].SourceElement().parentNode.className += ' has-jax';
                      }
                      });
</script>
<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-69271262-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <nav role="navigation"><div class="navbar navbar-inverse">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">KRIS</a>

      <div class="nav-collapse">
        <ul class="nav">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>


        <ul class="nav pull-right" data-subscription="rss">
          <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
          
        </ul>

        
          <form class="pull-right navbar-search" action="https://www.google.com/search" method="get">
            <fieldset role="search">
              <input type="hidden" name="q" value="site:billy-inn.github.io" />
              <input class="search-query" type="text" name="q" results="0" placeholder="Search"/>
            </fieldset>
          </form>
        
      </div>
    </div>
  </div>
</div>
</nav>
  <div class="container">
    <div class="row-fluid">
      
<article class="hentry span9" role="article">

  
  <header class="page-header">
    
      <h1 class="entry-title">Notes on Reinforcement Learning (1): Finite Markov Decision Processes</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-10-05T14:55:31-06:00'><span class='date'>2016-10-05</span> <span class='time'>2:55 pm</span></time>
        
         | <a href="#disqus_thread">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><h3 id="the-agent-environment-interface">The Agent-Environment Interface</h3>

<p><img src="/images/rl1.1.png" alt="Alt text" /></p>

<ul>
  <li>The agent and environment interact at each of a sequence of discrete time steps, $t=0,1,2,3,\dots$</li>
  <li>At each time step $t$, the agent receives some representation of the environment’s state, $S_t\in\mathcal{S}$, where $\mathcal{S}$ is the set of possible states.</li>
  <li>On that basis, the agent selects an action, $A_t \in \mathcal{A}(S_t)$, where $\mathcal{A}(S_t)$ is the set of actions available in state $S_t$.</li>
  <li>One time step later, in part as a consequence of its action, the agent receives a numerical reward, $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$, and finds itself in a new state, $S_{t+1}$.</li>
</ul>

<!--more-->

<p>At each time step, the agent implements a mapping from states to probabilities of selecting each possible action. This mapping is called the agent’s policy and is denoted $\pi_t$, where $\pi_t(a \vert s)$ is the probability that $A_t=a$ if $S_t=s$. Reinforcement learning methods specify how the agent changes its policy as a result of its experience. The agent’s goal, roughly speaking, is to maximize the total amount of reward it receives over the long run.</p>

<p>The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.</p>

<h3 id="rewards-and-returns">Rewards and Returns</h3>

<p>At each time step, the reward is a simple number, $R_t \in \mathbb{R}$. Informally, the agent’s goal is to maximize the total amount of reward it receives. If the sequence of rewards received after time step $t$ is denoted $R_{t+1}, R_{t+2}, R_{t+3}, \dots$, we seek to maximize the expected return, where the return $G_t$ is defined as some specific function of the reward sequence.</p>

<p>Here we define the return as:</p>

<script type="math/tex; mode=display">G_t = \sum_{k=0}^{T-t-1}\gamma^kR_{t+k+1}</script>

<p>where $T$ can be $\infty$ and $0&lt;\gamma\le1$ is the discounting rate.</p>

<h3 id="markov-decision-processes">Markov Decision Processes</h3>

<p>A reinforment learning task that satisfies the Markov preperty is called a Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision process (finite MDP).</p>

<p>Given any state and action $s$ and $a$, the probability of each possible pair of next state and reward, $s’$, $r$, is denoted</p>

<script type="math/tex; mode=display">p(s',r|s,a)=\Pr\{S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a\}.</script>

<p>These quantities completely specify the dynamics of a finite MDP.</p>

<h3 id="value-functions">Value Functions</h3>

<p>The value of a state $s$ under a policy $\pi$, denoted $v_\pi(s)$, is the expected return when starting in $s$ and following $\pi$ thereafter. For MDPs, we can define $v_\pi(s)$ formally as</p>

<script type="math/tex; mode=display">v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]=\mathbb{E}_\pi\left[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\middle|S_t=s\right],</script>

<p>where $\mathbb{E}[\centerdot]$ denotes the expected value of a random variable given that the agent follows policy $\pi$, and $t$ is any time step. We call the function $v_\pi$ the state-value function for policy $\pi$.</p>

<p>Similarly, we define the value of taking action $a$ in state $s$ under a policy $\pi$, denoted $q_\pi(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:</p>

<script type="math/tex; mode=display">q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=s]=\mathbb{E}_\pi\left[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\middle|S_t=s,A_t=a\right].</script>

<p>We call $q_\pi$ the action-value function for policy $\pi$.</p>

<p>A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy particular recursive relationships. For any policy $\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states:</p>

<script type="math/tex; mode=display">v_\pi(s)=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')], \forall s \in \mathcal{S}.</script>

<p>This equation is the Bellman equation for $v_\pi$. Likewise, the Bellman equation for action values $q_\pi$ is as follows:</p>

<script type="math/tex; mode=display">q_\pi(s,a)=\sum_{s',r}p(s',r|s,a)[r+\gamma\sum_{a'}\pi(a'|s')q(s',a')]</script>

<p><img src="/images/rl1.2.png" alt="Alt text" /></p>

<p>According to the Bellman equations, we can derive the relatioship between $v_\pi$ and $q_\pi$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
v_\pi(s)&=\mathbb{E}_\pi[q_\pi(S_{t}, a)|S_t=s]\\
&= \sum_a\pi(a|s)q_\pi(s,a)
\end{split} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
q_\pi(s, a)&=\mathbb{E}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s, A_t=a]\\
&=\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]
\end{split} %]]></script>

<h3 id="optimal-value-functions">Optimal Value Functions</h3>

<p>A policy $\pi$ is defined to be better than or equal to a policy $\pi’$ if its expected return is greater than or equal to that of $\pi’$ for all states. In other words, $\pi\ge\pi’$ if and only if $v_\pi(s)\ge v_{\pi’}(s)$ for all $s\in\mathcal{S}$. There is always at least one policy that is better than or equal to all other policies. This is an optimal policy $\pi_*$.</p>

<p>The optimal state-value function, denoted $v_*$, are defined as</p>

<script type="math/tex; mode=display">v_*(s) = \max_\pi v_\pi(s),</script>

<p>for all $s\in\mathcal{S}$.</p>

<p>The optimal action-value function, denoted $q_*$, are defined as</p>

<script type="math/tex; mode=display">q_*(s,a)=\max_\pi q_\pi(s,a),</script>

<p>for all $s\in\mathcal{S}$ and $a\in\mathcal{A}(s)$.</p>

<p>The Bellman optimalality equation for $v_<em>$ and $q_</em>$ is</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
v_*(s)&=\max_a\mathbb{E}_\pi[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s, A_t=a]\\
&= \max_{a\in\mathcal{A}(s)}\sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]
\end{split} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
q_\pi(s, a)&=\mathbb{E}\left[R_{t+1}+\gamma q_*(S_{t+1},a')\middle|S_t=s, A_t=a\right]\\
&=\sum_{s',r}p(s',r|s,a)[r+\gamma \max_{a'}q_*(s',a')]
\end{split} %]]></script>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Peng Xu</span></span>

      




<time class='entry-date' datetime='2016-10-05T14:55:31-06:00'><span class='date'>2016-10-05</span> <span class='time'>2:55 pm</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/reinforcement-learning/'>reinforcement_learning</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://billy-inn.github.io/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes/" data-via="" data-counturl="http://billy-inn.github.io/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes/" >Tweet</a>
  
  
  
</div>

    
    
    <section>
      <h1>Comments</h1>
      <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
    </section>
    
    <ul class="pager">
      
      <li class="previous"><a class="basic-alignment left"
        href="/blog/2016/09/20/ml-with-r-3-logistic-regression/" title="Previous Post:
        ML with R (3): Logistic Regression">&laquo; ML with R (3): Logistic Regression</a></li>
      
      <li><a href="/blog/archives">Blog Archives</a></li>
      
      <li class="next"><a class="basic-alignment right" href="/blog/2016/10/06/notes-on-reinforcement-learning-2-dynamic-programming/"
        title="Next Post: Notes on Reinforcement Learning (2): Dynamic Programming">Notes on Reinforcement Learning (2): Dynamic Programming
        &raquo;</a></li>
      
    </ul>
  </footer>
</article>

<aside class="sidebar-nav span3">
  
    <section>
	<ul id="subtitle" class="nav nav-list">
		<img src="/images/subtitle.png">
		<p></p>
	</ul>
</section>
<section class="well">
 <ul id="about" class="nav nav-list">
	 <li class="nav-header"><a href="/about">About Me</a></li>
	 <p>Machine Learning Lover.</p>
	 <p>Now working hard on related mathematics, e.g. Statistics, Optimization.</p>
	 <p>Also interested in the applied domains like NLP, DM, CV, FE.</p>
	 <p>Welcome to communicate with me among above topics.</p>
	 <p>微博: <a href="http://www.weibo.com/u/2540837283">百里云_bly</a></p>
	 <p>GitHub: <a href="http://github.com/billy-inn">billy-inn</a></p>
	 <p>Email:<code>bly930725 [at] gmail.com</code></p>
 </ul>
</section>
<section class="well">
 <ul id="categories_list" class="nav nav-list">
	<li class="nav-header">Categories</li>
	  <li id="categories"><li class='category'><a href='/blog/categories/life/'>life (2)</a></li>
<li class='category'><a href='/blog/categories/machine-learning/'>machine_learning (5)</a></li>
<li class='category'><a href='/blog/categories/pgm/'>pgm (1)</a></li>
<li class='category'><a href='/blog/categories/project/'>project (1)</a></li>
<li class='category'><a href='/blog/categories/reinforcement-learning/'>reinforcement_learning (4)</a></li>
</li>
 </ul>
</section>
<section class="well">
  <ul id="recent_posts" class="nav nav-list">
    <li class="nav-header">Recent Posts</li>
    
      <li class="post">
        <a href="/blog/2016/12/14/%5B(shi-kong-:ji-qi-,-she-hui-yu-jing-ji-de-xin-sheng-wu-xue-)%5D-man-tan-(1):zhong-yu-gua/">【失控：机器、社会与经济的新生物学】漫谈（1）：众与寡</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/">Notes on Reinforcement Learning (4): Temporal-Difference Learning</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/10/14/notes-on-reinforcement-learning-3-monte-carlo-methods/">Notes on Reinforcement Learning (3): Monte Carlo Methods</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/10/06/notes-on-reinforcement-learning-2-dynamic-programming/">Notes on Reinforcement Learning (2): Dynamic Programming</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes/">Notes on Reinforcement Learning (1): Finite Markov Decision Processes</a>
      </li>
    
  </ul>
</section>
<section class="well">
  <ul id="license" class="nav nav-list">
    <li class="nav-header">License</li>
<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.
	</li>
  </ul>
</section>

  
</aside>


    </div>
  </div>
  <footer role="contentinfo" class="page-footer"><hr>
<p>
  Copyright &copy; 2016 - Peng Xu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'billy-inn';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://billy-inn.github.io/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes/';
        var disqus_url = 'http://billy-inn.github.io/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
