<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Optimization | Billy Ian's Short Leisure-time Wander]]></title>
  <link href="http://billy-inn.github.io/blog/categories/optimization/atom.xml" rel="self"/>
  <link href="http://billy-inn.github.io/"/>
  <updated>2021-09-21T15:16:57-04:00</updated>
  <id>http://billy-inn.github.io/</id>
  <author>
    <name><![CDATA[Peng (Billy) Xu]]></name>
    <email><![CDATA[bly930725@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Notes on Convex Optimization (5): Newton's Method]]></title>
    <link href="http://billy-inn.github.io/blog/2018/11/13/convex-optimization-5/"/>
    <updated>2018-11-13T23:25:12-05:00</updated>
    <id>http://billy-inn.github.io/blog/2018/11/13/convex-optimization-5</id>
    <content type="html"><![CDATA[<p>For $x\in\mathbf{dom}\ f$, the vector</p>

<script type="math/tex; mode=display">\Delta x_{nt}=-\nabla^2 f(x)^{-1}\nabla f(x)</script>

<p>is called the <em>Newton step</em> (for $f$, at $x$).</p>

<h4 id="minimizer-of-second-order-approximation">Minimizer of second-order approximation</h4>

<p>The second-order Taylor approximation $\hat f$ of $f$ at $x$ is</p>

<p>\begin{equation}
\hat f(x+v) = f(x) + \nabla f(x)^T v + \frac12 v^T \nabla^2 f(x) v.
\tag{1} \label{eq:1}
\end{equation}</p>

<p>which is a convex quadratic function of $v$, and is minimized when $v=\Delta x_{nt}$. Thus, the Newton step $\Delta x_{nt}$ is what should be added to the point $x$ to minimize the second-order approximation of $f$ at $x$.</p>

<!--more-->

<h4 id="steepest-descent-direction-in-hessian-norm">Steepest descent direction in Hessian norm</h4>

<p>The Newton step is also the steepest descent direction at $x$, for the quadratic norm defined by the Hessian $\nabla^2 f(x)$, <em>i.e.</em>,</p>

<script type="math/tex; mode=display">\lVert u \rVert_{\nabla^2f(x)}=(u^T\nabla^2f(x)u)^{1/2}.</script>

<h4 id="solution-of-linearized-optimality-condition">Solution of linearized optimality condition</h4>

<p>If we linearize the optimality condition $\nabla f(x^*)=0$ near $x$ we obtain</p>

<script type="math/tex; mode=display">\nabla f(x+v) \approx \nabla f(x) + \nabla^2f(x)v = 0</script>

<p>which is a linear equation in $v$, with solution $v=\Delta x_{nt}$. So the Newton step $\Delta x_{nt}$ is what must be added to $x$ so that the linearized optimality condition holds.</p>

<h4 id="affine-invariance-of-the-newton-step">Affine invariance of the Newton step</h4>

<p>An important feature of the Newton step is that it is independent of linear changes of coordinates. Suppose $T\in \mathbf{R}^{n \times n}$ is nonsingular, and define $\bar f(y)=f(Ty)$. Then we have</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\nabla\bar f(y) & = \nabla f(Ty)\\
&=\frac{\partial f(Ty)}{\partial(Ty)}\frac{\partial(Ty)}{y} \\
&=T^T\nabla f(x),
\end{split} %]]&gt;</script>

<p>where $x=Ty$, likewise we have $\nabla^2 \bar f(y) = T^T\nabla^2f(x)T$. The Newton step for $\bar f$ at $y$ is therefore</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\Delta y_{nt} &= -(T^T\nabla^2f(x)T)^{-1}(T^T\nabla f(x)) \\
&= -T^{-1} \nabla^2 f(x)^{-1} \nabla f(x) \\
&= T^{-1}\Delta x_{nt},
\end{split} %]]&gt;</script>

<p>where $\Delta x_{nt}$ is the Newton step for $f$ at $x$. Hence the Newton steps of $f$ and $\bar f$ are related by the same linear transformation, and</p>

<script type="math/tex; mode=display">x + \Delta x_{nt} = T(y + \Delta y_{nt}).</script>

<h4 id="the-newton-decrement">The Newton decrement</h4>

<p>The quantity</p>

<script type="math/tex; mode=display">\lambda(x)=(\nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x))^{1/2}</script>

<p>is called the <em>Newton decrement</em> at $x$. We can relate the Newton decrement to the quantity $f(x) - \inf_y \hat f(y)$, where $\hat y$ is the second-order approximation of $f$ at $x$:</p>

<script type="math/tex; mode=display">f(x) - \inf_y \hat f(y)=f(x)-\hat f(x + \Delta x_{nt})=\frac12\lambda(x)^2.</script>

<p>We can also express the Newton decrement as</p>

<script type="math/tex; mode=display">\lambda(x)=(\Delta x_{nt}^T \nabla^2 f(x) \Delta x_{nt})^{1/2}.</script>

<p>This shows that $\lambda$ is the norm of the Newton step, in the quadratic norm defined by the Hessian.</p>

<h3 id="newtons-method">Newton’s Method</h3>

<blockquote>
  <p><em>Newton’s method</em>. <br />
<strong>given</strong> a starting point $x \in \mathbf{dom} \enspace f$, tolerance $\epsilon &gt; 0$. <br />
<strong>repeat</strong> <br />
- Compute the Newton step $\Delta x_{nt}$ and decrement $\lambda^2$. <br />
- Stopping criterion. <em>quit** if $\lambda(x)^2/2 \le \epsilon$. <br />
- *Line search</em>. Choose a step size $t &gt; 0$ by backtracking line search. <br />
- Update. $x := x+ t\Delta x_{nt}$.</p>
</blockquote>

<h3 id="summary">Summary</h3>

<p>Newton’s method has several very strong advantages over gradient and steepest descent methods:</p>

<ul>
  <li>Convergence of Newton’s method is rapid in general, and quadratic near $x^\ast$. Once the quadratic convergence phase is reached, at most six or so iterations are required to produce a solution of very high accuracy.</li>
  <li>Newton’s method is affine invariant. It is insensitive to the choice of coordinates, or the condition number of the sublevel sets of the objective.</li>
  <li>The good performance of Newton’s method is not dependent on the choice of algorithm parameters. In contrast, the choice of norm for steepest descent plays a critical role in its performance.</li>
</ul>

<p>The main disadvantage of Newton’s method is the cost of forming and storing the Hessian, and the cost of computing the Newton step, which requires solving a set of linear equations.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Convex Optimization (4): Gradient Descent Method]]></title>
    <link href="http://billy-inn.github.io/blog/2018/11/05/convex-optimization-4/"/>
    <updated>2018-11-05T00:29:05-05:00</updated>
    <id>http://billy-inn.github.io/blog/2018/11/05/convex-optimization-4</id>
    <content type="html"><![CDATA[<h3 id="descent-methods">Descent methods</h3>

<script type="math/tex; mode=display">x^{(k+1)}=x^{(k)} + t^{(k)}\Delta x^{(k)}</script>

<ul>
  <li>$f(x^{(k+1)}) &lt; f(x^{(k)})$</li>
  <li>$\Delta x$ is the <em>step</em> or <em>search direction</em>; $t$ is the <em>step size</em> or <em>step length</em></li>
  <li>from convexity, $\nabla f(x)^T \Delta x &lt;0$</li>
</ul>

<blockquote>
  <p><em>General descent method</em>. <br />
<strong>given</strong> a starting point $x \in \mathbf{dom} \enspace f$. <br />
<strong>repeat</strong> <br />
- Determine a descent direction $\Delta x$. <br />
- <em>Line search</em>. Choose a step size $t &gt; 0$. <br />
- Update. $x := x+ t\Delta x$.</p>

  <p><strong>until</strong> stopping criterion is satisfied</p>
</blockquote>

<!--more-->

<h4 id="exact-line-search">Exact line search</h4>

<script type="math/tex; mode=display">t = \text{argmin}_{t > 0} \enspace f(x + t\Delta x)</script>

<h4 id="backtracking-line-search">Backtracking line search</h4>

<blockquote>
  <p><em>Backtracking line search</em>. <br />
<strong>given</strong> a descent direction $\Delta x$ for $f$ at $x \in \mathbf{dom} f, \alpha \in(0,0.5), \beta\in(0,1)$. <br />
<strong>starting</strong> at $t:=1$. <br />
<strong>while</strong> $f(x+t\Delta x) &gt; f(x) + \alpha t \nabla f(x)^T \Delta x$, $t:=\beta t$</p>
</blockquote>

<h3 id="gradient-descent-method">Gradient descent method</h3>

<p>A natural choice for the search direction is the negative gradient $\Delta x = - \nabla f(x)$.</p>

<h4 id="convergence-analysis-for-exact-line-search">Convergence analysis for exact line search</h4>

<p>We must have $f(x^(k)) - p^\ast \le \epsilon$ after at most</p>

<script type="math/tex; mode=display">\frac{\log((f(x^{(0)})-p^\ast)/\epsilon)}{\log(1/c)}</script>

<p>iterations of the gradient method with exact line search, where $c=1-m/M&lt;1$.</p>

<h4 id="convergence-analysis-for-backtracking-line-search">Convergence analysis for backtracking line search</h4>

<p>Similar to exact line search, except that $c=1 - \min{2m\alpha, 2\beta\alpha m/M} &lt; 1.$</p>

<h4 id="conclusions">Conclusions</h4>

<ul>
  <li>The gradient method often exhibits approximately linear convergence, <em>i.e.</em>, the error $f(x^{(k)}) - p^\ast$ converges to zeros approximately as a geometric series.</li>
  <li>The choice of backtracking parameters $\alpha, \beta$ has a noticeable but not dramatic effect on the convergence. An exact line search sometimes improves the convergence of the gradient method, but the effect is not large.</li>
  <li>The convergence rate depends greatly on the condition number of the Hessian, or the sublevel sets. Convergence can be very slow, even for problems that are moderately well conditioned. When the condition number is larger the gradient method is so slow that it is useless in practice.</li>
</ul>

<h3 id="steepest-descent-method">Steepest descent method</h3>

<p>The first-order Taylor approximation of $f(x+v)$ around $x$ is</p>

<script type="math/tex; mode=display">f(x+v)\approx \hat f(x+v)=f(x) + \nabla f(x)^T v.</script>

<p>The second term on the righthand side, $\nabla f(x)^T v$, is the <em>directional derivative</em> of $f$ at $x$ in the direction $v$. It gives the approximate change in $f$ for a small step $v$. The step $v$ is a descent direction if the directional derivative is negative.</p>

<p>Let $\lVert \cdot \rVert$ be any norm on $\mathbf{R}^n$. We define a <em>normailzied steepest descent direction</em> as</p>

<p>\begin{equation}
\Delta x_{nsd} = \arg\min{\nabla f(x)^T v\ \vert\ \lVert v \rVert = 1}.
\tag{1}\label{eq:1}
\end{equation}</p>

<p>It is also convenient to consider a steepest descent step $\Delta x_{sd}$ that is <em>unnormalized</em>, by scaling the normalized steepest descent direction in a particular way:</p>

<p>\begin{equation}
\Delta x_{sd} = \lVert \nabla f(x) \rVert_\ast \Delta x_{nsd},
\tag{2}\label{eq:2}
\end{equation}</p>

<p>where $\lVert \cdot \rVert_\ast$ denotes the dual norm. Note that for the steepest descent step, we have</p>

<script type="math/tex; mode=display">\nabla f(x)^T \Delta x_{sd} = \lVert \nabla f(x) \rVert_\ast \nabla f(x)^T \nabla x_{nsd}=-\lVert \nabla f(x) \rVert_\ast^2.</script>

<h4 id="steepest-descent-for-euclidean-norm">Steepest descent for Euclidean norm</h4>

<p>To simplify the notation, we can look at the problem of solving $\min_v{u^Tv\ \lvert\ \lVert v \rVert \le 1}$ which ends up being equivalent to find the normalized steepest descent step.</p>

<p>The Cauchy-Schwarz inequality gives $\lvert u^Tv\rvert \le \rVert u \rVert \lVert v \rVert$, hence it is easy to see that the minimum is $\min_v{u^Tv\ \lvert\ \lVert v \rVert \le 1}=-\lVert u \rVert$, and the minimizer is $v=-u/\lVert u \rVert$. As a result, the steepest descent direction is simply the negative gradient, <em>i.e.</em>, $\Delta x_{sd} = - \nabla f(x)$.</p>

<h4 id="steepest-descent-for-quadratic-norm">Steepest descent for quadratic norm</h4>

<p>We consider the quadratic norm</p>

<script type="math/tex; mode=display">\lVert z \rVert_P = (z^TPz)^{1/2}=\lVert P^{1/2}z\rVert_2,</script>

<p>where $P \in \mathbf{S}_{++}^n$. The problem is now $\min_v{u^Tv\ \vert\ \lVert P^{1/2}v\rVert\le1}=\min_v{u^Tv\ \vert\ \lVert\delta\rVert\le1, v=P^{-1/2}\delta}$.
This is equivalent to $\min_\delta{((P^{-1/2})^Tu)^T\delta\ \vert\ \lVert\delta\rVert\le1}$. The problem above shows that the minimum is $-\lVert (P^{-1/2})^Tu\rVert$ while the maximum $\lVert (P^{-1/2})^Tu\rVert$ is the dual norm according to the definition, and the minimizer is $v=P^{-1/2}\delta=-P^{-1}u/\lVert (P^{-1/2})^Tu\rVert$, so the steepest descent desnt is given by</p>

<script type="math/tex; mode=display">\Delta x_{sd} = -P^{-1} \nabla f(x).</script>

<p>In addition, the steepest descent method in the quadratic norm $\lVert \cdot \rVert_P$ can be thought of as the gradient method applied to the problem after the change of coordinates $\bar x=P^{1/2}x$.</p>

<h4 id="steepest-descent-for-l1-norm">Steepest descent for $l_1$-norm</h4>

<p>Let $i$ be any index for which $\lVert \nabla f(x) \rVert_\infty = \lvert (\nabla f(x))_i \rvert$. Then a normalized steepest descent direction $\nabla x_{nsd}$ for the $l_1$-norm is given by</p>

<script type="math/tex; mode=display">\Delta x_{nsd}=-\text{sign}\left(\frac{\partial f(x)}{\partial x_i}\right)e_i,</script>

<p>where $e_i$ is the $i$th standard basis vector. An unnormalized steepest descent step is then</p>

<script type="math/tex; mode=display">\Delta x_{sd} = \Delta x_{nsd}\lVert \nabla f(x) \rVert_\infty = - \frac{\partial f(x)}{\partial x_i}e_i.</script>

<p>The steepest descent algorithm in the $l_1$-norm has a very natural interpertation: At each iteration we select a component of $\nabla f(x)$ with maximum absolute value, and then decrease or increase the corresponding component of $x$, according to the sign of $(\nabla f(x))_i$. The algorithm is sometimes called a <em>corrdinate-descent</em> algorithm, since only one component of the variable $x$ is updated at each iteration.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Convex Optimization (3): Unconstrained Minimization Problems]]></title>
    <link href="http://billy-inn.github.io/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/"/>
    <updated>2018-09-29T15:15:12-04:00</updated>
    <id>http://billy-inn.github.io/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems</id>
    <content type="html"><![CDATA[<p>Unconstrained optimization problems are defined as follows:</p>

<p>\begin{equation}
\text{minimize}\quad f(x)
\tag{1} \label{eq:1}
\end{equation}</p>

<p>where $f: \mathbf{R}^n \rightarrow \mathbf{R}$ is convex and twice continously differentiable (which implies that $\mathbf{dom}\enspace f$ is open). We denote the optimal value $\inf_xf(x)=f(x^\ast)$, as $p^\ast$. Since $f$ is differentiable and convex, a necessary and sufficient condition for a point $x^\ast$ to be optimal is</p>

<p>\begin{equation}
\nabla f(x^\ast)=0
\tag{2} \label{eq:2}
\end{equation}</p>

<p>. Thus, solving the unconstrained minimization problem \eqref{eq:1} is the same as finding a solution of \eqref{eq:2}, which is a set of $n$ equations in the $n$ variables $x_1, \dots, x_n$. Usually, the problem must be solved by an iterative algorithm. By this we mean an algorithm that computes a sequence of points $x^{(0)}, x^{(1)}, \dots \in \mathbf{dom}\enspace f$ with $f(x^{(k)})\rightarrow p^\ast$ as $k\rightarrow\infty$. The algorithm is terminated when $f(x^{k}) - p^\ast \le \epsilon$, where $\epsilon&gt;0$ is some specified tolerance.</p>

<!--more-->

<h4 id="initial-point-and-sublevel-set">Initial point and sublevel set</h4>

<p>The starting point $x^{(0)}$ must lie in $\mathbf{dom}\enspace f$, and in addition the sublevel set</p>

<script type="math/tex; mode=display">S = \{x \in \mathbf{dom}\enspace f \vert f(x) \le f(x^{(0)})\}</script>

<p>must be closed. This condition is satisfied for all $x^{(0)}\in\mathbf{dom}\enspace f$ if the function $f$ is closed.</p>

<p>Note: 1) Continuous functions with $\mathbf{dom}\enspace f=\mathbf{R}^n$ are closed; 2) Another important class of closed functions are continuous functions with open domains.</p>

<h3 id="examples">Examples</h3>

<h4 id="quadratic-minimization-and-least-squares">Quadratic minimization and least-squares</h4>

<p>The general convex quadratic minimization problem has the form</p>

<script type="math/tex; mode=display">\text{minimize}\quad (1/2)x^TPx+q^Tx+r,</script>

<p>where $P\in\mathbf{S}_+^n$, $q\in\mathbf{R}^n$, and $r\in\mathbf{R}$. This problem can be solved via the optimality conditions, $Px+q=0$, which is a set of linear equations.</p>

<p>One special case of the quadratic minimization problem that arises very frequently is the least-squares problem</p>

<script type="math/tex; mode=display">\text{minimize}\quad \lVert Ax-b\rVert_2^2=x^T(A^TA)x-2(A^Tb)^T+b^Tb.</script>

<p>The optimality condition</p>

<script type="math/tex; mode=display">A^TAx^\ast=A^Tb</script>

<p>are called the <em>normal equations</em> of the least-squares problem.</p>

<h4 id="unconstrained-geometric-programming">Unconstrained geometric programming</h4>

<script type="math/tex; mode=display">\text{minimize}\quad f(x)=\log(\sum_{i=1}^m\exp(a_i^Tx+b_i))</script>

<h4 id="analytic-center-of-linear-inequalities">Analytic center of linear inequalities</h4>

<script type="math/tex; mode=display">\text{minimize} f(x)=-\sum_{i=1}^m\log(b_i - a_i^Tx),</script>

<p>where the domain of $f$ is the open set</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\mathbf{dom}\enspace f=\{x\vert a_i^Tx<b_i,i=1,\dots,m\}. %]]&gt;</script>

<h4 id="analytic-center-of-a-linear-matrix-inequality">Analytic center of a linear matrix inequality</h4>

<script type="math/tex; mode=display">\text{minimize}\quad f(x)=\log\det F(x)^{-1}</script>

<p>where $F: \mathbf{R}^n\rightarrow\mathbf{S}^p$ is affine. Here the domain of $f$ is</p>

<script type="math/tex; mode=display">\mathbf{dom}\enspace f=\{x\vert F(x) \succ 0\}.</script>

<h3 id="strong-convexity-and-implications">Strong convexity and implications</h3>

<p>The objective function is <em>strongly convex</em> on $S$, which means that there exists an $m&gt;0$ such that</p>

<p>\begin{equation}
\nabla^2f(x) \succeq mI
\tag{3} \label{eq:3}
\end{equation}</p>

<p>for all $x\in S$.  For $x, y \in S$ we have</p>

<script type="math/tex; mode=display">f(y)=f(x)+\nabla f(x)^T(y-x) + \frac12(y-x)^T\nabla^2f(z)(y-x)</script>

<p>for some $z$ on the line segement $[x, y]$. By the strong convexity assumption \eqref{eq:3}, the last term on the righthand side is at least $(m/2)\lVert y-x\rVert^2_2$, so we have the inequality</p>

<p>\begin{equation}
f(y) \ge f(x) + \nabla f(x)^T(y-x) + \frac{m}2\lVert y-x \rVert_2^2
\tag{4} \label{eq:4}
\end{equation}</p>

<p>for all $x$ and $y$ in $S$.</p>

<h4 id="bound-fx-past-in-terms-of-lvert-nabla-fx-rvert2">Bound $f(x)-p^\ast$ in terms of $\lVert \nabla f(x) \rVert_2$</h4>

<p>Setting the gradient of the righthand side of \eqref{eq:4} with respect to $y$ equal to zero, we find that $\tilde y = x-(1/m)\nabla f(x)$ minimizes the righthand side. Therefore we have</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
f(y) &\ge f(x) + \nabla f(x)^T (y-x) + \frac{m}2\lVert y-x \rVert_2^2 \\
& \ge f(x) + \nabla f(x)^T(\tilde y - x) + \frac{m}2\lVert \tilde y - x \rVert_2^2 \\
& = f(x) - \frac1{2m}\lVert \nabla f(x)\rVert_2^2
\end{split} %]]&gt;</script>

<p>Since this holds for any $y\in S$, we have</p>

<p>\begin{equation}
p^* \ge f(x) - \frac1{2m}\lVert \nabla f(x)\rVert_2^2
\tag{5} \label{eq:5}
\end{equation}</p>

<p>This inequality shows that if the gradient is small at a point, then the point is nearly optimal.</p>

<h4 id="bound-lvert-x-xastrvert22-in-terms-of-lvert-nabla-fx-rvert2">Bound $\lVert x-x^\ast\rVert_2^2$ in terms of $\lVert \nabla f(x) \rVert_2$</h4>

<p>Apply \eqref{eq:4} with $y=x^\ast$ to obtain</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
p^* = f(x^\ast)  & \ge f(x) + \nabla f(x)^T (x^\ast - x) + \frac{m}2\lVert x^\ast-x \rVert_2^2 \\
& \ge f(x) - \lVert \nabla f(x) \rVert_2 \lVert x^\ast - x \rVert_2 + \frac{m}2 \lVert x^\ast - x \rVert_2^2,
\end{split} %]]&gt;</script>

<p>where we use the Cauchy-Schwarz inequality in the second inequality. Since $p^\ast \le f(x)$, we must have</p>

<script type="math/tex; mode=display">-\lVert \nabla f(x) \rVert_2 \lVert x^\ast - x \rVert_2 + \frac{m}2 \lVert x^\ast - x \rVert_2^2 \le 0.</script>

<p>Therefore, we have</p>

<p>\begin{equation}
\lVert x - x^\ast \rVert_2 \le \frac2m\lVert \nabla f(x) \rVert_2.
\tag{6}\label{eq:6}
\end{equation}</p>

<h4 id="uniqueness-of-the-optimal-point-xast">Uniqueness of the optimal point $x^\ast$</h4>

<p>If there are two optimal point $x^\ast_1, x^\ast_2$, according to \eqref{eq:6},</p>

<script type="math/tex; mode=display">\lVert x_1^\ast - x_2^\ast \rVert \le \frac2m \lVert f(x_1^\ast) \rVert_2 = 0.</script>

<p>Hence, $x_1^\ast = x_2^\ast$, the optimal point $x^\ast$ is unique.</p>

<h4 id="upper-bound-on-nabla2fx">Upper bound on $\nabla^2f(x)$</h4>

<p>There exists a  constant $M$ such that</p>

<script type="math/tex; mode=display">\nabla^2 f(x) \preceq MI</script>

<p>for all $x \in S$. This upper bound on the Hessian implies for any $x, y \in S$,</p>

<script type="math/tex; mode=display">f(y) \le f(x) + \nabla f(x)^T (y-x) + \frac{M}2\lVert y-x \rVert_2^2,</script>

<p>minimizing each side over $y$ yields</p>

<script type="math/tex; mode=display">p^\ast \le f(x) - \frac1{2M}\lVert \nabla f(x) \rVert_2^2.</script>

<h4 id="condition-number-of-sublevel-sets">Condition number of sublevel sets</h4>

<p>The ratio $\kappa=M/m$ is an upper bound on the condition number of the matrix $\nabla^2 f(x)$, <em>i.e.</em>, the ratio of its largest eigenvalue to its smallest eigenvalue.</p>

<p>We define the <em>width</em> of a convex set $C \subseteq \mathbf{R}^n$, in the direction $q$, where $\lVert q \rVert_2 = 1$, as</p>

<script type="math/tex; mode=display">W(C, q) = \sup_{z \in C} q^T z - \inf_{z\in C}q^Tz.</script>

<p>The <em>minimum width</em> and <em>maximum width</em> of $C$ are given by</p>

<script type="math/tex; mode=display">W_{min}=\inf_{\lVert q\rVert_2=1}W(C,q), \qquad W_{max}=\sup_{\lVert q\rVert_2=1}W(C,q).</script>

<p>The <em>condition number</em> of the convex set $C$ is defined as</p>

<script type="math/tex; mode=display">\mathbf{cond}(C)=\frac{W^2_{max}}{W^2_{min}}.</script>

<p>Suppose $f$ satisfies $mI \preceq \nabla^2 f(x) \preceq MI$ for all $x\in S$. The condition number of the $\alpha$-sublevel $C_\alpha={x \vert f(x) \le \alpha}$, where $p^\ast &lt; \alpha \le f(x^{(0)})$, is bounded by</p>

<script type="math/tex; mode=display">\mathbf{cond}(C_\alpha) \le \frac Mm.</script>

<h4 id="the-strong-convexity-constants">The strong convexity constants</h4>

<p>It must be kept in mind that the constants $m$ and $M$ are known only in rare cases, so they cannot be used in a practical stopping criterion.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Convex Optimization (2): Convex Functions]]></title>
    <link href="http://billy-inn.github.io/blog/2017/10/21/convex-optimization-2/"/>
    <updated>2017-10-21T13:59:09-04:00</updated>
    <id>http://billy-inn.github.io/blog/2017/10/21/convex-optimization-2</id>
    <content type="html"><![CDATA[<h3 id="basic-properties-and-examples">1. Basic Properties and Examples</h3>

<h4 id="definition">1.1 Definition</h4>

<p>$f:\mathbb{R}^n \rightarrow \mathbb R$ is convex if $\mathbf{dom}\ f$ is a convex set and</p>

<script type="math/tex; mode=display">f(\theta x+(1-\theta)y)\le\theta f(x)+(1-\theta)f(y)</script>

<p>for all $x,y\in \mathbf{dom}\ f, 0\le\theta\le1$</p>

<ul>
  <li>$f$ is concave if $-f$ is convex</li>
  <li>$f$ is strictly convex if $\mathbf{dom}\ f$ is convex and <script type="math/tex">% &lt;![CDATA[
f(\theta x+(1-\theta)y)<\theta f(x)+(1-\theta)f(y) %]]&gt;</script> for $x,y\in\mathbf{dom}\ f,x\ne y, 0&lt;\theta&lt;1$</li>
</ul>

<p>$f:\mathbb{R}^n \rightarrow \mathbb R$ is convex if and only if the function $g: \mathbb{R} \rightarrow \mathbb{R}$,</p>

<script type="math/tex; mode=display">g(t)=f(x+tv),\quad\mathbf{dom}\ g=\{t\mid x+tv\in\mathbf{dom}\ f\}</script>

<p>is convex (in $t$) for any $x \in \mathbf{dom}\ f, v\in\mathbb R^n$</p>

<!--more-->

<h4 id="extended-value-extensions">1.2 Extended-value extensions</h4>

<p>extended-value extension $\tilde f$ of $f$ is</p>

<script type="math/tex; mode=display">\tilde f(x)=f(x),x\in\mathbf{dom}\ f; \tilde f(x)=\infty,x\ne \mathbf{dom}\ f</script>

<h4 id="first-order-conditions">1.3 First-order conditions</h4>

<p><strong>1st-order condition</strong>: differentiable $f$ with convex domain is convex iff</p>

<script type="math/tex; mode=display">f(y)\ge f(x)+\nabla f(x)^T(y-x)\mathrm{\ for\ all\ } x,y\in\mathbf{dom}\ f</script>

<h4 id="second-order-conditions">1.4 Second-order conditions</h4>

<p><strong>2nd-order conditions</strong>: for twice differentiable $f$ with convex domain
- $f$ if convex if and only if
<script type="math/tex">\nabla^2f(x)\succeq0\mathrm{\ for\ all\ } x\in\mathbf{dom}\ f</script>
- if $\nabla^2f(x)\succ0$ for all $x\in\mathbf{dom}\ f$, then $f$ is strictly convex</p>

<h4 id="sublevel-sets-and-epigraph">1.5 Sublevel sets and epigraph</h4>

<p>$\alpha$<strong>-sublevel set</strong> of $f: \mathbb R^n \rightarrow \mathbb R$:</p>

<script type="math/tex; mode=display">C_\alpha=\{x\in\mathbf{dom}\ f \mid f(x)\le\alpha\}</script>

<p>sublevel sets of convex functions are convex (converse if false)</p>

<p>If $f$ is concave, then its $\alpha$<strong>-superlevel set</strong>, given by ${x\in\mathbf{dom}\ f\mid f(x)\le\alpha}$, is a convex set</p>

<p><strong>epigraph</strong> of $f:\mathbb R^n \rightarrow \mathbb R$:</p>

<script type="math/tex; mode=display">\mathbf{epi}\ f=\{(x,t)\in\mathbb R^{n+1}\mid x\in\mathbf{dom}\ f,f(x)\le t\}</script>

<p>$f$ is convex if and only if $\mathbf{epi}\ f$ is a convex set</p>

<p>$f$ is concave if and only if its <strong>hypograph</strong>, defined as</p>

<script type="math/tex; mode=display">\mathbf{hypo}\ f= \{(x,t)\in\mathbb R^{n+1}\mid x\in\mathbf{dom}\ f,f(x)\ge t\}</script>

<p>is a convex set</p>

<h4 id="jensens-inequality-and-extensions">1.6 Jensen’s inequality and extensions</h4>

<p><strong>Jensen’s Inequality</strong>: if $f$ is convex, then for $0\le\theta\le1$,</p>

<script type="math/tex; mode=display">f(\theta x+(1-\theta)y)\le\theta f(x)+(1-\theta)f(y)</script>

<p><strong>extension</strong>: if $f$ is convex, then</p>

<script type="math/tex; mode=display">f(\mathbb E z)\le \mathbb Ef(z)</script>

<p>for any random variable $z$</p>

<h3 id="operations-that-preserve-convexity">2. Operations that Preserve Convexity</h3>

<h4 id="positive-weighted-sum--composition-with-affine-function">2.1 Positive weighted sum &amp; composition with affine function</h4>

<p><strong>nonnegative multiple</strong>: $\alpha f$ is convex if $f$ is convex, $\alpha \ge 0$</p>

<p><strong>sum</strong>: $f_1+f_2$ convex if $f_1,f_2$ convex (extends to infinite sums, integrals)</p>

<p><strong>composition with affine function</strong>: $f(Ax+b)$ is convex if $f$ is convex</p>

<h4 id="pointwise-maximum">2.2 Pointwise maximum</h4>

<p><strong>pointwise maximum</strong>: if $f_1,\dots,f_m$ are convex, then $f(x)=\max{f_1(x),\dots,f_m(x)}$ is convex</p>

<p><strong>pointwise supermum</strong>: if $f(x,y)$ is convex in $x$ for each $y\in\mathcal{A}$, then</p>

<script type="math/tex; mode=display">g(x)=\sup_{y\in\mathcal{A}}f(x,y)</script>

<p>is convex</p>

<p>similarly, the <strong>pointwise infimum</strong> of a set of concave functions is a concave function</p>

<h4 id="composition">2.3 Composition</h4>

<p><strong>composition with scalar functions</strong>: composition of $g: \mathbb R^n \rightarrow \mathbb R$ and $h: \mathbb R\rightarrow \mathbb R$:</p>

<script type="math/tex; mode=display">f(x)=h(g(x))</script>

<p>$f$ is convex if $g$ convex, $h$ convex, $\tilde h$ nondecreasing; $g$ concave, $h$ convex, $\tilde h$ nonincreasing</p>

<p>Note: monotonicity must hold for extended-value extension $\tilde h$</p>

<p><strong>vector composition</strong>: composition of $g:\mathbb R^n \rightarrow \mathbb R^k$ and $h:\mathbb R^k \rightarrow \mathbb R$:</p>

<script type="math/tex; mode=display">f(x)=h(g(x))=h(g_1(x),g_2(x),\dots,g_k(x))</script>

<p>$f$ is convex if $g_i$ convex, $h$ convex, $\tilde h$ nondecreasing in each argument; $g$ concave, $h$ convex, $\tilde h$ nonincreasing in each argument</p>

<h4 id="minimization">2.4 Minimization</h4>

<p><strong>minimization</strong>: if $f(x,y)$ is convex in $(x,y)$ and $C$ is a convex set then</p>

<script type="math/tex; mode=display">g(x)=\inf_{y\in C}f(x,y)</script>

<p>is convex</p>

<h4 id="perspective-of-a-function">2.5 Perspective of a function</h4>

<p><strong>perspective</strong>: the <strong>perspective</strong> of a function $f:\mathbb R^n \rightarrow \mathbb R$ is the function $g:\mathbb R^n \times \mathbb R \rightarrow \mathbb R$,</p>

<script type="math/tex; mode=display">g(x,t)=tf(x/t),\mathbf{dom}\ g=\{(x,t)\mid x/t\in\mathbf{dom}\ f,t>0\}</script>

<p>$g$ is convex if $f$ is convex</p>

<h3 id="the-conjugate-function">3. The Conjugate Function</h3>

<h4 id="definition-1">3.1 Definition</h4>

<p>the <strong>conjugate</strong> of a function $f$ is</p>

<script type="math/tex; mode=display">f^*(y)=\sup_{x\in\mathbf{dom}\ f}(y^Tx-f(x))</script>

<ul>
  <li>$f^*$ is convex whether or not $f$ is convex</li>
</ul>

<h4 id="basic-properties">3.2 Basic properties</h4>

<p><strong>conjugate of the conjugate</strong>: if $f$ is convex and closed, then $f^{**}=f$</p>

<p><strong>differentiable functions</strong>: The conjugate of a differentiable function $f$ is also called the <em>Legendre transform</em> of $f$. Let $z\in\mathbb{R}^n$ be arbitrary and define $y=\nabla f(z)$, then we have</p>

<script type="math/tex; mode=display">f^*(y)=z^T\nabla f(z)-f(z)</script>

<p><strong>scaling and composition with affline transformation</strong>: 
For $a&gt;0$ and $b\in\mathbb{R}$, the conjugate of $g(x)=af(x)+b$ is $g^*(y)=af^*(y/a)-b$.</p>

<p>Suppose $A\in\mathbb{R}^{n\times n}$ is nonsingular and $b\in\mathbb{R}^n$. Then the conjugate of $g(x)=f(Ax+b)$ is</p>

<script type="math/tex; mode=display">g^*(y)=f^*(A^{-T}y)-b^TA^{-T}y</script>

<p>with $\mathbf{dom}\ g^*=A^T\mathbf{dom}\ f^*$</p>

<p><strong>sums of independent functions</strong>: if $f(u,v)=f_1(u)+f_2(v)$, where $f_1$ and $f_2$ are convex functions with conjugates $f_1^*$ and $f_2^*$, respectively, then</p>

<script type="math/tex; mode=display">f^*(w,z)=f_1^*(w)+f_2^*(z)</script>

<h3 id="quasiconvex-functions">4. Quasiconvex Functions</h3>

<p>$f:\mathbb R^n\rightarrow \mathbb R$ is quasiconvex if $\mathbf{dom}\ f$ is convex and the sublevel sets</p>

<script type="math/tex; mode=display">S_{\alpha}=\{x\in\mathbf{dom}\ f\mid f(x)\le\alpha\}</script>

<p>are convex for all $\alpha$</p>

<ul>
  <li>$f$ is quasiconcave if $-f$ is quasiconvex</li>
  <li>$f$ is quasilinear if it is quasiconvex and quasiconcave</li>
  <li>convex functions are quasiconvex, but the converse is not true</li>
</ul>

<p><strong>modified Jensen inequality</strong>: for quasiconvex $f$</p>

<script type="math/tex; mode=display">0\le\theta\le1\Longrightarrow f(\theta x+(1-\theta)y)\le\max\{f(x),f(y)\}</script>

<p><strong>first-order condition</strong>: differentiable $f$ with convex domain is quasiconvex iff</p>

<script type="math/tex; mode=display">f(y)\le f(x)\Longrightarrow\nabla f(x)^T(y-x)\le0</script>

<p><strong>operations that preserve quasiconvexity</strong>:</p>

<ul>
  <li>nonnegative weighted maximum</li>
  <li>composition</li>
  <li>minimization</li>
</ul>

<p><strong>sum</strong> of quasiconvex functions are not necessarily quasiconvex</p>

<h3 id="log-concave-and-log-convex-functions">5. Log-concave and Log-convex Functions</h3>

<p>a positive function $f$ is log-concave if $\log f$ is concave:</p>

<script type="math/tex; mode=display">f(\theta x+(1-\theta)y)\ge f(x)^\theta f(y)^{1-\theta}\mathrm{\ for\ }0\le\theta\le1</script>

<p>$f$ is log-convex if $\log f$ is convex</p>

<p><strong>properties of log-concave functions</strong>:</p>

<ul>
  <li>twice differentiable $f$ with convex domain is log-concave iff</li>
</ul>

<script type="math/tex; mode=display">f(x)\nabla^2f(x)\preceq\nabla f(x)\nabla f(x)^T</script>

<p>for all $x\in\mathbf{dom}\ f$</p>

<ul>
  <li>product of log-concave functions is log-concave</li>
  <li>sum of log-concave functions is not always log-concave; however, log-convexity is preserved under sums</li>
  <li>integration: if $f:\mathbb R^n\times\mathbb R^m \rightarrow \mathbb R$ is log-concave, then</li>
</ul>

<script type="math/tex; mode=display">g(x)=\int f(x,y)dy</script>

<p>is log-concave</p>

<p><strong>consequences of integration property</strong>:</p>

<ul>
  <li>convolution $f*g$ of log-concave functions $f,g$ is log-concave</li>
</ul>

<script type="math/tex; mode=display">(f*g)(x)=\int f(x-y)g(y)dy</script>

<ul>
  <li>if $C\subseteq \mathbb R^n$ convex and $y$ is a random variable with log-concave pdf then</li>
</ul>

<script type="math/tex; mode=display">f(x)=\mathbf{prob}(x+y\in C)</script>

<p>is log-concave</p>

<h3 id="convexity-wrt-generalized-inequalities">6. Convexity w.r.t. Generalized Inequalities</h3>

<p>$f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is $K$-convex if $\mathbf{dom}\ f$ is convex and</p>

<script type="math/tex; mode=display">f(\theta x+(1-\theta)y)\preceq_K\theta f(x)+(1-\theta)f(y)</script>

<p>for $x,y\in\mathbf{dom}\ f,0\le\theta\le1$</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Convex Optimization (1): Convex Sets]]></title>
    <link href="http://billy-inn.github.io/blog/2017/10/17/convex-optimization-1/"/>
    <updated>2017-10-17T02:22:23-04:00</updated>
    <id>http://billy-inn.github.io/blog/2017/10/17/convex-optimization-1</id>
    <content type="html"><![CDATA[<h3 id="affine-and-convex-sets">1. Affine and Convex Sets</h3>

<p>Suppose $x_1\ne x_2$ are two points in $\mathbb{R}^n$.</p>

<h4 id="affine-sets">1.1 Affine sets</h4>

<p><strong>line</strong> through $x_1$, $x_2$: all points</p>

<script type="math/tex; mode=display">x=\theta x_1 + (1-\theta)x_2\quad(\theta \in \mathbb{R})</script>

<p><strong>affine set</strong>: contains the line through any two distinct points in the set</p>

<script type="math/tex; mode=display">x_1,x_2\in C,\ \theta\in\mathbb{R} \Longrightarrow \theta x_1 + (1-\theta)x_2 \in C</script>

<!--more-->

<h4 id="convex-sets">1.2 Convex sets</h4>

<p><strong>line segment</strong> between $x_1$ and $x_2$: all points</p>

<script type="math/tex; mode=display">x=\theta x_1 + (1-\theta)x_2</script>

<p>with $0\leq\theta\leq1$</p>

<p><strong>convex set</strong>: contains line segment between any two points in the set</p>

<script type="math/tex; mode=display">x_1,x_2\in C,\ 0\leq\theta\leq1 \Longrightarrow \theta x_1 + (1-\theta)x_2 \in C</script>

<p><strong>convex combination</strong> of $x_1,\dots,x_k$: any point $x$ of the form</p>

<script type="math/tex; mode=display">x=\theta_1x_1+\theta_2x_2+\dots+\theta_kx_k</script>

<p>with $\theta_1+\dots+\theta_k=1,\theta_i \geq 0$</p>

<p><strong>convex hull</strong> of a set $C$, denoted $\mathbf{conv}\ C$: set of all convex combinations of points in $C$</p>

<h4 id="cones">1.3 Cones</h4>

<p><strong>conic combination</strong> of $x_1$ and $x_2$: any point of the form</p>

<script type="math/tex; mode=display">x=\theta_1x_1+\theta_2x_2</script>

<p>with $\theta_1 \geq 0, \theta_2 \geq 0$</p>

<p><strong>convex cone</strong>: set that contains all conic combinations of points in the set</p>

<script type="math/tex; mode=display">x_1,x_2\in C,\ \theta_1\ge0, \theta_2\ge0 \Longrightarrow \theta_1 x_1 + \theta_2x_2 \in C</script>

<h3 id="some-important-examples">2. Some Important Examples</h3>

<h4 id="hyperplanes-and-halfspaces">2.1 Hyperplanes and halfspaces</h4>

<p><strong>hyperplane</strong>: set of the form {$x\mid a^Tx=b$}$(a\ne0)$</p>

<p><strong>halfspace</strong>: set of the form {$x\mid a^Tx\leq b$}$(a\ne0)$</p>

<ul>
  <li>$a$ is the normal vector</li>
  <li>hyperplanes are affine and convex; halfspaces are convex</li>
</ul>

<h4 id="euclidean-balls-and-ellipsoids">2.2 Euclidean balls and ellipsoids</h4>

<p><strong>(Euclidean) ball</strong> with center $x_c$ and radius $r$:</p>

<script type="math/tex; mode=display">B(x_c,r)=\{x\mid\lVert x-x_c\rVert_2\leq r\}=\{x_c+ru \mid\lVert u \rVert_2 \leq 1 \}</script>

<p><strong>ellipsoid</strong>: set of the form</p>

<script type="math/tex; mode=display">\{x \mid (x-x_c)^TP^{-1}(x-x_c)\leq 1\}</script>

<p>with $P\in \mathbf{S}^n_{++}$ (<em>i.e.</em>, P symmetric positive definite)</p>

<p>another representation: {$x_c+Au\mid \lVert u\rVert_2\le1$} with $A$ square and nonsingular</p>

<ul>
  <li>Euclidean balls and ellipsoids are all convex.</li>
</ul>

<h4 id="norm-balls-and-norm-cones">2.3 Norm balls and norm cones</h4>

<p><strong>norm</strong>: a funtion $\lVert \centerdot \rVert$ that satisfies</p>

<ul>
  <li>$\lVert x \rVert \geq 0$; $\lVert x \rVert=0$ if and only if $x=0$</li>
  <li>$\lVert tx \rVert = \lvert t \rvert \lVert x \rVert$ for $t\in \mathbb{R}$</li>
  <li>$\lVert x+y\rVert \leq \lVert x \rVert+\lVert y \rVert$</li>
</ul>

<p><strong>norm ball</strong> with center $x_c$ and radius <script type="math/tex">r: \{x \mid \lVert x-x_c \rVert \leq r\}</script></p>

<p><strong>norm cone</strong>: <script type="math/tex">\{(x,t) \mid \lVert x \rVert \leq t\}</script></p>

<ul>
  <li>norm balls and cones are convex</li>
  <li>norm cores (as the name suggest) are convex cones</li>
</ul>

<h4 id="polyhedra">2.4 Polyhedra</h4>

<p><strong>polyhedra</strong>: solution set of finitely many linear inequalities and equalities</p>

<script type="math/tex; mode=display">Ax \preceq b, \quad Cx=d</script>

<p>($A\in \mathbb{R}^{m\times n}$, $C\in\mathbb{R}^{p\times n}$, $\preceq$ is componentwise inequality)</p>

<ul>
  <li>polyhedron is intersection of finite number of halfspaces and hyperplances</li>
</ul>

<h4 id="the-positive-semidefinite-cone">2.5 The positive semidefinite cone</h4>

<p><strong>positive semidefinite cone</strong>:</p>

<ul>
  <li>$\mathbf{S}^n$ is set of symmetric $n\times n$ matrices</li>
  <li><script type="math/tex">\mathbf{S}^n_+=\{X\in\mathbf{S}^n\mid X\succeq0\}</script>: positive semidefinite $n\times n$ matrices <script type="math/tex">X\in\mathbf{S}^n_+ \iff z^TXz \geq 0\ \mathrm{for\ all\ }z</script> $\mathbf{S}^n_+$ is a convex cone</li>
  <li><script type="math/tex">\mathbf{S}^n_{++}=\{X\in\mathbf{S}^n\mid X\succ0\}</script>: positive definite $n\times n$ matrices</li>
</ul>

<h3 id="operations-that-preserve-convexity">3. Operations that preserve convexity</h3>

<p><strong>intersection</strong>: the interction of (any number of) convex sets is convex</p>

<p><strong>affine function</strong>: suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is affine ($f(x)=Ax+b$ with $A\in\mathbb{R}^{m\times n}, b\in\mathbb{R}^m$)</p>

<ul>
  <li>the image of a convex set under $f$ is convex</li>
</ul>

<script type="math/tex; mode=display">S \subseteq \mathbb{R}^n\ convex \Longrightarrow f(S)=\{f(x)\mid x\in S\}\ convex</script>

<ul>
  <li>the inverse image $f^{-1}(C)$ of a convex set under $f$ is convex</li>
</ul>

<script type="math/tex; mode=display">C \subseteq \mathbb{R}^m\ convex \Longrightarrow f^{-1}(C)=\{x\in\mathbb{R}^n\mid f(x)\in C\}\ convex</script>

<p><strong>perspective function</strong> $P: \mathbb{R}^{n+1} \rightarrow \mathbb{R}^n$:</p>

<script type="math/tex; mode=display">P(x,t)=x/t, \quad \mathbf{dom}\ P=\{(x,t)\mid t>0\}</script>

<p>images and inverse images of convex sets under perspective are convex</p>

<p><strong>linear-fractional function</strong> $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$:</p>

<script type="math/tex; mode=display">f(x)=\frac{Ax+b}{c^Tx+d}, \quad \mathbf{dom}\ f=\{x\mid c^Tx+d>0\}</script>

<p>images and inverse images of convex sets under linear-fractional functions are convex</p>

<h3 id="generalized-inequalities">4. Generalized Inequalities</h3>

<h4 id="proper-cones-and-generalized-inequalities">4.1 Proper cones and generalized inequalities</h4>

<p>a convex cone $K\subseteq\mathbb{R}^n$ is a <strong>proper cone</strong> if</p>

<ul>
  <li>$K$ is closed (contains its boundary)</li>
  <li>$K$ is solid (has nonempty interior)</li>
  <li>$K$ is pointed (contains no line)</li>
</ul>

<p><strong>generalized inequality</strong> defined by a proper cone $K$:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
x \preceq_K y &\iff y-x \in K\\
x \prec_K y &\iff y-x \in \mathbf{int}\ K
\end{split} %]]&gt;</script>

<h4 id="minimum-and-minimal-elements">4.2 Minimum and minimal elements</h4>

<p>$x\in S$ is <strong>the minimum element</strong> of $S$ with respect to $\preceq_K$ if</p>

<script type="math/tex; mode=display">y \in S \Longrightarrow x \preceq_K y</script>

<p>$x\in S$ is <strong>a minimal element</strong> of $S$ with respect to $\preceq_K$ if</p>

<script type="math/tex; mode=display">y \in S, y \preceq_K x \Longrightarrow y=x</script>

<h3 id="separating-and-supporting-hyperplanes">5. Separating and Supporting Hyperplanes</h3>

<p><strong>separating hyperplane theorem</strong>: if $C$ and $D$ are disjoint convex sets, then there exists $a\ne0$, $b$ such that</p>

<script type="math/tex; mode=display">a^Tx \le b\ \mathrm{for}\ x\in C,\quad a^Tx\ge b\ \mathrm{for}\ x\in D</script>

<p><strong>supporting hyperplane</strong> to set $C$ at boundary point $x_0$:</p>

<script type="math/tex; mode=display">\{x\mid a^Tx=a^Tx_0\}</script>

<p>where $a\ne0$ and $a^Tx\le a^Tx_0$ for all $x\in C$</p>

<p><strong>supporting hyperplance theorem</strong>: if $C$ is convex, then there exists a supporting hyperplane at every boundary point of $C$</p>

<h3 id="dual-cones-and-generalized-inequalities">6. Dual Cones and Generalized Inequalities</h3>

<h4 id="dual-cones">6.1 Dual cones</h4>

<p><strong>dual cone</strong> of a cone $K$:</p>

<script type="math/tex; mode=display">K^*=\{y\mid y^T x \ge 0\mathrm{\ for\ all\ } x\in K\}</script>

<p>Dual cons satisfy several properties, such as:</p>

<ul>
  <li>$K^*$ is closed and convex</li>
  <li>$K_1 \subseteq K_2$ imples $K_2^* \subseteq K_1^*$</li>
  <li>$K^{**}$ is the closure of the convex hull of $K$ (Hence if $K$ is convex and closed, $K^{**}=K$)</li>
</ul>

<p>Thsese properties show that if $K$ is a proper cone, then so is its dual $K^{*}$, and moreover, that $K^{**}=K$</p>

<h4 id="dual-generalized-inequalities">6.2 Dual generalized inequalities</h4>

<p>dual cones of proper cones are proper, hence define generalized inequalities:</p>

<script type="math/tex; mode=display">y\succeq_{K^*}0 \iff y^Tx\ge0\mathrm{\ for\ all\ } x\succeq_K0</script>

<p>Some import properties relating a generalized inequality and its dual are:</p>

<ul>
  <li>$x\preceq_K y$ iff $\lambda^Tx \le \lambda^Ty$ for all $\lambda \succeq_{K^{*}} 0$</li>
  <li>$x\prec_K y$ iff $\lambda^Tx &lt; \lambda^Ty$ for all $\lambda \succ_{K^{*}} 0, \lambda\ne0$</li>
</ul>

<p>Since $K=K^{**}$, the dual generalized inequality associated with $\preceq_{K^{*}}$ is $\preceq_K$, so these properties hold if the generalized inequality and its dual are swapped</p>

<h4 id="minimum-and-minimal-elements-via-dual-inequalities">6.3 Minimum and minimal elements via dual inequalities</h4>

<p><strong>dual characterization of minimum element</strong> w.r.t. $\preceq_K$: $X$ is minimum element of $S$ iff for all $\lambda \succ_{K^*}0$, $x$ is the unique minimizer of $\lambda^Tz$ over $z\in S$</p>

<p><strong>dual characterization of minimal element</strong> w.r.t. $\preceq_K$:</p>

<ul>
  <li>if $x$ minimizes $\lambda^Tz$ over $S$ for some $\lambda \succ_{K^*}0$, then $x$ is minimal</li>
  <li>if $x$ is a minimal element of a convex set $S$, then there exists a nonzero $\lambda \succeq_{K^*}0$ such that $x$ minimizes $\lambda^Tz$ over $z \in S$</li>
</ul>
]]></content>
  </entry>
  
</feed>
