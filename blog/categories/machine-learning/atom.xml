<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine_learning | Billy Inn in Wonderland]]></title>
  <link href="http://billy-inn.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://billy-inn.github.io/"/>
  <updated>2017-12-14T17:10:41-07:00</updated>
  <id>http://billy-inn.github.io/</id>
  <author>
    <name><![CDATA[Peng Xu]]></name>
    <email><![CDATA[bly930725@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[Notes on Mathematics for ESL] Chapter 10: Boosting and Additive Trees]]></title>
    <link href="http://billy-inn.github.io/blog/2017/12/14/esl-chapter10/"/>
    <updated>2017-12-14T17:07:57-07:00</updated>
    <id>http://billy-inn.github.io/blog/2017/12/14/esl-chapter10</id>
    <content type="html"><![CDATA[<h3 id="why-exponential-loss">10.5 Why Exponential Loss?</h3>

<h4 id="derivation-of-equation-1016">Derivation of Equation (10.16)</h4>

<p>Since $Y\in{-1,1}$, we can expand the expectation as follows:</p>

<script type="math/tex; mode=display">\text{E}_{Y\vert x}(e^{-Yf(x)}) = \Pr(Y=1 \vert x)e^{-f(x)} + 
\Pr(Y=-1\vert x)e^{f(x)}</script>

<p>In order to minimize the expectation, we equal derivatives w.r.t. $f(x)$ as zero:</p>

<script type="math/tex; mode=display">-\Pr(Y=1\vert x)e^{-f(x)}+\Pr(Y=-1\vert x)e^{f(x)}=0</script>

<p>which gives:</p>

<script type="math/tex; mode=display">f^*(x)=\frac12\log\frac{\Pr(Y=1\vert x)}{\Pr(Y=-1\vert x)}</script>

<!--more-->

<h4 id="notes-on-equation-1018">Notes on Equation (10.18)</h4>

<p>If $Y=1$, then $Y’=1$, which gives</p>

<script type="math/tex; mode=display">l(Y,f(x))=\log p(x)=\log(1+e^{-2f(x)})</script>

<p>Likewise, if $Y=-1$, then $Y’=0$, which gives</p>

<script type="math/tex; mode=display">l(Y,f(x))=\log (1-p(x))=\log(1+e^{2f(x)})</script>

<p>As a result, the <em>binomial log-likelihood loss</em> is equivalent to the <em>deviance</em>. In the language of neural networks, the <em>cross-entropy</em> is equivalent to the <em>softplus</em>. The only difference is that $0$ is used to indicate negative examples in <em>cross-entropy</em>; while $-1$ is used in <em>softplus</em>.</p>

<h3 id="loss-functions-and-robustness">10.6 Loss Functions and Robustness</h3>

<p>This section explains the choice of loss functions for both classification and regression. It gives a very direct expalanation about why square loss is undesirable for classification. Highly recommended!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Notes on Mathematics for ESL] Chapter 6: Kernel Smoothing Methods]]></title>
    <link href="http://billy-inn.github.io/blog/2017/10/27/esl-chapter-6/"/>
    <updated>2017-10-27T16:57:21-06:00</updated>
    <id>http://billy-inn.github.io/blog/2017/10/27/esl-chapter-6</id>
    <content type="html"><![CDATA[<h3 id="one-dimensional-kernel-smoothers">6.1 One-Dimensional Kernel Smoothers</h3>

<h4 id="notes-on-local-linear-regression">Notes on Local Linear Regression</h4>

<p>Locally weighted regression solves a separate weighted least squares problem at each target point $x_0$:</p>

<script type="math/tex; mode=display">\min_{\alpha(x_0),\beta(x_0)}\sum_{i=1}^NK_\lambda(x_0,x_i)[y-\alpha(x_0)-\beta(x_0)x_i]^2</script>

<p>The estimate is  $\hat f(x_0)=\hat\alpha(x_0)+\hat\beta(x_0)x_0$. Define the vector-value function $b(x)^T=(1,x)$. Let $\mathbf{B}$ be the $N \times 2$ regression matrix with $i$th row row $b(x_i)^T$, $\mathbf{W}(x_0)$ the $N\times N$ diagonal matrix with $i$th diagonal element $K_\lambda (x_0, x_i)$, and $\theta=(\alpha(x_0), \beta(x_0))^T$.</p>

<p>Then the above optimization problem can be rewritten as</p>

<script type="math/tex; mode=display">\min_\theta(y-\mathbf{B}\theta)^T\mathbf{W}(x_0)(y-\mathbf{B}\theta)</script>

<p>Equal the derivative w.r.t $\theta$ as zero, we get</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
&\mathbf{B}^T\mathbf{W}(x_0)(y-\mathbf{B}\hat\theta)=0 \\
&\mathbf{B}^T\mathbf{W}(x_0)\mathbf{B}\hat\theta = \mathbf{B}^T\mathbf{W}(x_0)y \\
&\hat\theta= (\mathbf{B}^T\mathbf{W}(x_0)\mathbf{B})^{-1}\mathbf{B}\mathbf{W}(x_0)y
\end{split} %]]&gt;</script>

<!--more-->

<p>Then</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat f(x_0)&=b(x_0)^T\hat\theta \\
&=b(x_0)^T(\mathbf{B}^T\mathbf{W}(x_0)\mathbf{B})^{-1}\mathbf{B}\mathbf{W}(x_0)y \\
&=\sum_{i=1}^Nl_i(x_0)y_i
\end{split} %]]&gt;</script>

<p>It’s claimed that $\sum_{i=1}^Nl_i(x_0)=1$ and $\sum_{i=1}^N(x-x_0)l_i(x_0)=0$ in the book, so that the bias $\text{E}(\hat f(x_0))-f(x_0)$ depends only on quadratic and higher-order terms in the expansion of $f$. However, the proof is not given. Here I will give the detailed derivations of these two equations.</p>

<p>First, define the following terms:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
S_0 &= \sum_{i=1}^NK_\lambda(x_0, x_i) \\
S_1 &= \sum_{i=1}^NK_\lambda(x_0, x_i)x_i \\
S_2 &= \sum_{i=1}^NK_\lambda(x_0, x_i)x_i^2 \\
m_0 &= \sum_{i=1}^NK_\lambda(x_0, x_i)y_i \\
m_1 &= \sum_{i=1}^NK_\lambda(x_0, x_i)x_iy_i
\end{split} %]]&gt;</script>

<p>Then, we can represent the estimate as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat f(x_0) &= b(x_0)^T\begin{bmatrix}S_0& S_1\\S_1& S_2\end{bmatrix}^{-1}\begin{bmatrix}m_0\\m_1\end{bmatrix} \\
&= \frac1{S_0S_2-S_1^2}b(x_0)^T\begin{bmatrix}S_2 &-S_1\\-S_1& S_0\end{bmatrix}\begin{bmatrix}m_0\\m_1\end{bmatrix} \\
&=\frac{(S_2-S_1x_0)m_0-(S_1-S_0x_0)m_1}{S_0S_2-S_1^2}
\end{split} %]]&gt;</script>

<p>When $y=\mathbf{1}$, $m_0=S_0$ and $m_1=S_1$, we get</p>

<script type="math/tex; mode=display">\hat f(x_0)=\sum_{i=1}^Nl_i(x_0)=\frac{S_0S_2-S_1^2}{S_0S_2-S_1^2}=1</script>

<p>When $y=\mathbf{x}-x_0$,</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat f(x_0) &= \sum_{i=1}^N(x_i-x_0)l_i(x_0) \\
&= \frac{(S_2-S_1x_0)(S_1-S_0x_0)-(S_1-S_0x_0)(S_2-S_1x_0)}{S_0S_2-S_1^2} \\
&= 0
\end{split} %]]&gt;</script>

<p>More generally, it’s easy to show that $\sum_{i=1}^N(x_i-x_0)^pl_i(x_0)=0$ when $p&gt;0$.</p>

<p>We only prove the case when the input $x$ is one-dimensional. Similar strategy can be used to prove the case for high-dimensional input, but it’ll be a little bit complicated if you’re interested. Have fun!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Notes on Mathematics for ESL] Chapter 5: Basis Expansions and Regularization]]></title>
    <link href="http://billy-inn.github.io/blog/2017/10/24/esl-chapter-5/"/>
    <updated>2017-10-24T18:34:25-06:00</updated>
    <id>http://billy-inn.github.io/blog/2017/10/24/esl-chapter-5</id>
    <content type="html"><![CDATA[<h3 id="smoothing-splines">5.4 Smoothing Splines</h3>

<h4 id="derivation-of-equation-512">Derivation of Equation (5.12)</h4>

<p>Equal the derivative of <strong>Equation (5.11)</strong> as zero, we get</p>

<script type="math/tex; mode=display">\frac{\partial\text{RSS}(\theta,\lambda)}{\partial\theta} = -2N^T(y-N\theta)+2\lambda\Omega_N\theta = 0</script>

<p>Put the terms related to $\theta$ on one side and the others on the other side, we get</p>

<script type="math/tex; mode=display">(N^TN+\lambda\Omega_N)\theta = N^Ty</script>

<p>Multiply the inverse of $N^TN+\lambda\Omega_N$ on both sides completes the derivation of <strong>Equation (5.12)</strong></p>

<script type="math/tex; mode=display">\hat\theta = (N^TN+\lambda\Omega_N)^{-1}N^Ty</script>

<!--more-->

<h4 id="explanations-on-equation-517-and-518">Explanations on Equation (5.17) and (5.18)</h4>

<p>It’s a little confusing to get <strong>Equation (5.18)</strong> directly from <strong>Equation (5.17)</strong> and its original form <strong>Equation (5.11)</strong>. In order to give a clear explanation, here we give the proof of the equation,</p>

<script type="math/tex; mode=display">\theta^T\Omega_N\theta=\mathbf{f}^TK\mathbf{f}.</script>

<p>which are the different terms between <strong>Equation (5.11)</strong> and <strong>Equation (5.18)</strong>.</p>

<p>We know that</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\mathbf{f}&=N(N^TN+\lambda\Omega_N)^{-1}N^Ty \\
&=S_{\lambda}y = N\theta
\end{split} %]]&gt;</script>

<p>following <strong>Equation (5.14)</strong> in the book.</p>

<p>From <strong>Equation (5.17)</strong>, we can get</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
K&=\frac1\lambda(S_\lambda^{-1}-I)\\
&=\frac1\lambda\left(N^{-T}(N^TN+\lambda\Omega_N)N^{-1}-I\right)\\
&=N^{-T}\Omega_NN^{-1}
\end{split} %]]&gt;</script>

<p>Plug the above two equation into the right side of the equation remains to be proved, we get</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\mathbf{f}^TK\mathbf{f} &= \theta^TN^TN^{-T}\Omega_NN^{-1}N\theta \\
&=\theta^T\Omega_N\theta
\end{split} %]]&gt;</script>

<p>which completes the proof.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Notes on Mathematics for ESL] Chapter 4: Linear Methods for Classification]]></title>
    <link href="http://billy-inn.github.io/blog/2017/10/15/esl-chapter-4/"/>
    <updated>2017-10-15T14:30:27-06:00</updated>
    <id>http://billy-inn.github.io/blog/2017/10/15/esl-chapter-4</id>
    <content type="html"><![CDATA[<h3 id="linear-discriminant-analysis">4.3 Linear Discriminant Analysis</h3>

<h4 id="derivation-of-equation-49">Derivation of Equation (4.9)</h4>

<p>For that each class’s density follows multivariate Gaussian</p>

<script type="math/tex; mode=display">f_k(x) = \frac{1}{(2\pi)^{p/2}\lvert\Sigma\rvert^{1/2}}e^{-\frac12(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)}</script>

<p>Take the logarithm of $f_k(x)$, we get</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\log f_k(x) &= c - \frac12(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) \\
&= c - \frac12(x^T\Sigma^{-1}x-\mu_k^T\Sigma^{-1}x-x^T\Sigma^{-1}\mu_k+\mu_k\Sigma^{-1}\mu_k) \\
&= c - \frac12(x^T\Sigma^{-1}x+\mu_k\Sigma^{-1}\mu_k) + x^T\Sigma^{-1}\mu_k
\end{split} %]]&gt;</script>

<p>where $c = -\log [(2\pi)^{p/2}\lvert\Sigma\rvert^{1/2}]$ and $\mu_k^T\Sigma^{-1}x=x^T\Sigma^{-1}\mu_k$. Following the above formula, we can derive <strong>Equation (4.9)</strong> easily</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\log\frac{\Pr(G=k|X=x)}{\Pr(G=k|X=x)} &=& \log\frac{f_k(x)}{f_l(x)} + \log\frac{\pi_k}{\pi_l} \\
&=& \log\frac{\pi_k}{\pi_l} - \frac12(\mu_k\Sigma^{-1}\mu_k-\mu_l\Sigma^{-1}\mu_l) \\&&+x^T\Sigma^{-1}(\mu_k-\mu_l) \\
&=& \log\frac{\pi_k}{\pi_l} - \frac12(\mu_k+\mu_l)\Sigma^{-1}(\mu_k-\mu_l) \\&&+x^T\Sigma^{-1}(\mu_k-\mu_l) \\
\end{split} %]]&gt;</script>

<!--more-->

<h4 id="notes-on-computations-for-lda">Notes on Computations for LDA</h4>

<p>It’s stated in the book that the LDA classifier can be implemented by the following pair of steps:</p>

<ul>
  <li>Sphere the data with respect to the common covariance estimate $\hat \Sigma:X^*\leftarrow D^{-1/2}U^TX$, where $\hat \Sigma=UDU^T$. The common covariance estimate of $X^*$  will now be the indentity.</li>
  <li>Classify to the closest class centroid in the transformed space, modulo the effect of the class prior probabilities $\pi_k$.</li>
</ul>

<p>However, detailed explanation is not given in the book. Here, I give some skipped mathematical steps which may help the understanding.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\text{Var}(X^*)&=D^{-1/2}U^T\text{Var}(X)(D^{-1/2}U^T)^T \\
&= D^{-1/2}U^TUDU^TUD^{-1/2} \\
&= D^{-1/2}DD^{-1/2} \\
&= I,
\end{split} %]]&gt;</script>

<p>which shows that the covariance estimate of $X^*$ is the identity.</p>

<p>Note that the classification for LDA is based on the linear discriminat functions</p>

<script type="math/tex; mode=display">\delta_k(x)=x^T\Sigma^{-1}\mu_k-\frac12\mu_k^T\Sigma^{-1}\mu_k+\log\pi_k</script>

<p>which is the <strong>Equation (4.10)</strong> in the book. Since the input $x$ is same for each class, so we can add back a term $\frac12x^T\Sigma^{-1}x$ which is cancelled in the previous derivation. Now the functions are turned into:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\delta_k(x)&=-\frac12(x^T\Sigma^{-1}x-2x^T\Sigma^{-1}\mu_k+\mu_k^T\Sigma^{-1}\mu_k)+\log\pi_k \\
&= -\frac12(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) + \log\pi_k
\end{split} %]]&gt;</script>

<p>We know that $\Sigma=I$ in the transformed space, so $\delta_k(x)=-1/2\lVert x-\mu_k\rVert_2+\log\pi_k$. And $\mu_k$ is the centroid for the $k$th class. The claimed method to classify is proved.</p>

<h3 id="logistic-regression">4.4 Logistic Regression</h3>

<h4 id="derivation-of-equation-421-and-422">Derivation of Equation (4.21) and (4.22)</h4>

<p>In the two-class case, $p_1(x;\beta)=p(x;\beta)$ and  $p_2(x;\beta) = 1-p(x;\beta)$ where</p>

<script type="math/tex; mode=display">p(x;\beta)=\frac{e^{\beta^Tx}}{1+e^{\beta^Tx}}.</script>

<p>The <strong>Equation (4.21)</strong> can be derived easily as follows,</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\frac{\partial l(\beta)}{\partial \beta} &= \frac{\partial}{\partial\beta}\sum_{i=1}^N\left\{y_i\beta^Tx_i-\log(1+e^{\beta^Tx_i})\right\}\\
&= \sum_{i=1}^N(y_ix_i-\frac{x_ie^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}) \\
&= \sum_{i=1}x_i(y_i-p(x_i;\beta))
\end{split} %]]&gt;</script>

<p>Note that</p>

<script type="math/tex; mode=display">\frac{\partial p(x;\beta)}{\partial\beta}=xp(x;\beta)(1-p(x;\beta)).</script>

<p>Plug it into <strong>Equation (4.21)</strong>, we get</p>

<script type="math/tex; mode=display">\frac{\partial^2l(\beta)}{\partial\beta\partial\beta^T} = -\sum_{i=1}^Nx_ix_i^Tp(x_i;\beta)(1-p(x_i;\beta)).</script>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Notes on Mathematics for ESL] Chapter 3: Linear Regression Models and Least Squares]]></title>
    <link href="http://billy-inn.github.io/blog/2017/09/27/esl-chapter-3/"/>
    <updated>2017-09-27T19:30:12-06:00</updated>
    <id>http://billy-inn.github.io/blog/2017/09/27/esl-chapter-3</id>
    <content type="html"><![CDATA[<h3 id="linear-regression-models-and-least-squares">3.2 Linear Regression Models and Least Squares</h3>

<h4 id="derivation-of-equation-38">Derivation of Equation (3.8)</h4>

<p>The least squares estimate of $\beta$ is given by the book’s <strong>Equation (3.6)</strong></p>

<script type="math/tex; mode=display">\hat\beta=(X^TX)^{-1}X^T\mathbf{y}.</script>

<p>From the <a href="/blog/2017/09/01/esl-chapter-2/">previous post</a>, we know that $\mathrm{E}(\mathbf{y})=X\beta$. As a result, we obtain</p>

<script type="math/tex; mode=display">\mathrm{E}(\hat\beta)=(X^TX)^{-1}X^TX\beta=\beta.</script>

<p>Then, we get</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat\beta-\mathrm{E}(\hat\beta)&=(X^TX)^{-1}X^T(\mathbf{y}-X\beta) \\
&=(X^TX)^{-1}X^T\varepsilon.
\end{split} %]]&gt;</script>

<p>The variance of $\hat \beta$ is computed as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\mathrm{Var}(\hat\beta) &= \mathrm{E}[(\hat\beta-\mathrm{E}(\hat\beta)(\hat\beta-\mathrm{E}(\hat\beta))^T] \\
&= (X^TX)^{-1}X^T\mathrm{Var}(\varepsilon)X(X^TX)^{-1}.
\end{split} %]]&gt;</script>

<p>If we assume that the entries of $\mathbf{y}$ are uncorrelated and all have the same variance of $\sigma^2$, then $\mathrm{Var}(\varepsilon)=\sigma^2I_N$ and the above equation becomes</p>

<script type="math/tex; mode=display">\mathrm{Var}(\hat\beta)=(X^TX)^{-1}\sigma^2.</script>

<p>This completes the derivation of <strong>Equation (3.8)</strong>.</p>

<!--more-->

<h4 id="thoughts-on-equation-312-and-313">Thoughts on Equation (3.12) and (3.13)</h4>

<p>There are a lot concepts of statistics in this part. It’s better to go through Chapter 6 and Chapter 10 in <a href="http://www.stat.cmu.edu/~larry/all-of-statistics/"><em>All of Statistics</em></a> to have a taste about hypothesis tests and confidence intervals.</p>

<p>From my own viewpoint, Z-score and F-statistic give a measure about whether the corresponding features are useful or not. They can be used within some feature selection methods. However, they’re not very useful in practice. The perferred feature selection methods are discussed in <strong>Section 3.3</strong> in the book.</p>

<h4 id="interpretations-of-equation-320-and-322">Interpretations of Equation (3.20) and (3.22)</h4>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\text{MSE}(\tilde\theta) &= \text{E}(\tilde\theta-\theta)^2 \\
&= \text{E}(\tilde\theta-\text{E}(\tilde\theta)+\text{E}(\tilde\theta)-\theta)^2 \\
&= \text{Var}(\tilde\theta)+2(\text{E}(\tilde\theta)-\text{E}(\tilde\theta))(\text{E}(\tilde\theta)-\theta)+(\text{E}(\tilde\theta)-\theta)^2 \\
&= \text{Var}(\tilde\theta)+(\text{E}(\tilde\theta)-\theta)^2
\end{split} %]]&gt;</script>

<p>which completes the derivation of <strong>Equation (3.20)</strong>.</p>

<p><strong>Equation (3.22)</strong> shows that the expected quadratic error can be broken down into two parts as</p>

<script type="math/tex; mode=display">\text{E}(Y_0-\tilde f(x_0))^2=\sigma^2+\text{MSE}(\tilde f(x_0))</script>

<p>The first error component $\sigma^2$ is unrelated to what model is used to describe our data. It cannot be reduced for it exists in the true data generation process. The second source of error corresponding to ther term $\text{MSE}(\tilde f(x_0))$ represents the error in the model and is under control of us. By <strong>Equation (3.20)</strong>, the mean square error can be broken down into two terms: a model variance term and a model bias squared term. How to make these two terms as small as possible while considering the trade-offs between them is the central topic in the book.</p>

<h4 id="notes-on-multiple-regression-from-simple-univariate-regression">Notes on Multiple Regression from Simple Univariate Regression</h4>

<p>The first thing that comes to my mind when I read this section is that why we need this when we already have the ordinary least square (OLS) estimate of $\beta$:</p>

<script type="math/tex; mode=display">\hat \beta = (X^TX)^{-1}X^TY.</script>

<p>It’s because we want to study how to obtain orthogonal inputs instead of correlated inputs, since orthogonal inputs have some nice properties.</p>

<p>Following Algorithm 3.1, we can transform the correlated inputs $\mathbf{x}$ to the orthogonal inputs $\mathbf{z}$. Another view is that we form an orthogonal basis by performing the Gram-Schmidt orthogonilization procedure on $X$’s column vectors and obtain an orthogonal basis $\mathbf{z}_{i=1}^p$. With this basis, linear regression can be done simply as in the univariate case as shown in <strong>Equation (3.28)</strong>:</p>

<script type="math/tex; mode=display">\hat \beta_p=\frac{\langle\mathbf{z}_p, \mathbf{y}\rangle}{\langle\mathbf{z}_p, \mathbf{z}_p\rangle}.</script>

<p>Following this equation, we can derive <strong>Equation (3.29)</strong>:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\text{Var}(\hat\beta_p)&=\text{Var}\left(\frac{z_p^Ty}{\langle z_p, z_p \rangle}\right)=\frac{z_p^T\text{Var}(y)z_p}{\langle z_p, z_p\rangle^2}=\frac{z_p^T(\sigma^2I)z_p}{\langle z_p, z_p\rangle^2} \\
&=\frac{\sigma^2}{\langle z_p, z_p \rangle}.
\end{split} %]]&gt;</script>

<p>We can write the Gram-Schmidt result in matrix form using the QR decomposition as</p>

<script type="math/tex; mode=display">X = QR.</script>

<p>In this decomposition $Q$ is a $N\times(p+1)$ matrix with orthonormal columns and $R$ is a $(p+1)\times(p+1)$ upper triangular matrix. In this representation, the OLS estimate for $\beta$ can be written as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat\beta &= (X^TX)^{-1}X^T\mathbf{y} \\
&= (R^TQ^TQR)^{-1}R^TQ^T\mathbf{y} \\
&= (R^TR)^{-1}R^TQ^T\mathbf{y} \\
&= R^{-1}R^{-T}R^TQ^T\mathbf{y} \\
&= R^{-1}Q^T\mathbf{y}
\end{split} %]]&gt;</script>

<p>which is <strong>Equation (3.32)</strong> in the book. Following this equation, the fitted value $\mathbf{\hat y}$ can be written as</p>

<script type="math/tex; mode=display">\mathbf{\hat y}=X\hat\beta=QRR^{-1}Q^T\mathbf{y}=QQ^T\mathbf{y}</script>

<p>which is <strong>Equation (3.33)</strong> in the book.</p>

<h3 id="shrinkage-methods">3.4 Shrinkage Methods</h3>

<h4 id="notes-on-ridge-regression">Notes on Ridge Regression</h4>

<p>If we compute the singular value decomposition (SVD) of the $N\times p$ centered data matrix $X$ as</p>

<script type="math/tex; mode=display">X=UDV^T,</script>

<p>where $U$ is a $N \times p$ matrix with orthonormal columns that span the column space of $X$, $V$ is a $p \times p$ orthogonal matrix, and $D$ is a $p \times p$ diagonal matrix with elements $d_j$ ordered such that $d_1\ge d_2 \ge \dots \ge d_p \ge 0$. From this representation of $X$ we can derive a simple expression for $X^TX$:</p>

<script type="math/tex; mode=display">X^TX=VDU^TUDV^T=VD^2V^T,</script>

<p>which is the <strong>Equation (3.48)</strong> in the book. Using this expression, we can compute the least squares fitted values as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat y^{ls} = X\hat\beta^{ls} &= X(X^TX)^{-1}X^T\mathbf{y}\\
&= UDV^T(VD^2V^T)^{-1}VDU^T\mathbf{y} \\
&= UDV^T(V^{-T}D^{-2}V^{-1})VDU^T\mathbf{y} \\
&= UU^T\mathbf{y} \\
&= \sum_{j=1}^pu_j(u_j^T\mathbf{y})
\end{split} %]]&gt;</script>

<p>which is the <strong>Equation (3.46)</strong> in the book. Similarly, we can find solutions for ridge regression as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat y^{ridge}=X\hat \beta^{ridge}&=X(X^TX+\lambda I)^{-1}X^T\mathbf{y} \\
&= UDV^T(VD^2V^T+\lambda VV^T)^{-1}VDU^T\mathbf{y} \\
&= UD(D^2+\lambda I)^{-1}DU^T\mathbf{y} \\
&= \sum_{j=1}^pu_j\frac{d_j^2}{d_j^2+\lambda}u_j^T\mathbf{y}
\end{split} %]]&gt;</script>

<p>which is the <strong>Equation (3.47)</strong> in the book. Since we can estimate the sample variance by $X^TX/N$, the variance of $\mathbf{z}_1$ can be derived as follows:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\text{Var}(\mathbf{z}_1)=\text{Var}(Xv_1)&= (Xv_1)^T(Xv_1)/N\\
&= v_1^TVD^TU^TUDV^Tv_1/N \\
&= v_1^TVD^2V^Tv_1/N \\
&= \frac{d_1^2}N
\end{split} %]]&gt;</script>

<p>which is the <strong>Equation (3.49)</strong> in the book. Note that $v_1$ is the first column of $V$ and $V$ is orthogonal, so that $V^Tv_1$ is $[1,0, \dots, 0]^T$.</p>

<h4 id="notes-on-degrees-of-freedom-formula-for-lar-and-lasso">Notes on degrees-of-freedom formula for LAR and Lasso</h4>

<p>The degrees-of-freedom of the fitted vector $\mathbf{\hat y}=(\hat y_1, \dots, \hat y_N)$ is defined as</p>

<script type="math/tex; mode=display">\text{df}(\mathbf{\hat y})=\frac1{\sigma^2}\sum_{i=1}^N\text{Cov}(\hat y_i, y_i)</script>

<p>in the book. Also, it’s claimed that $\text{df}(\mathbf{\hat y})$ is $k$ for ordinary least squares regression and $\text{tr}(\mathbf{S}_{\lambda})$ for ridge regresssion without proof in the book. Here, we’ll derive these two expressions. First, we define $e_i$ as a $N$-element vector of all zeros with a one in the $i$th spot. It’s easy to see that $\hat y_i=e_i^T\mathbf{\hat y}$ and $y_i=e_i^T\mathbf{y}$, so that</p>

<script type="math/tex; mode=display">\text{Cov}(\hat y_i, y_i)=\text{Cov}(e_i^T\mathbf{\hat y}, e_i^T\mathbf{y})=e_i^T\text{Cov}(\mathbf{\hat y},\mathbf{y})e_i.</script>

<p>For OLS regression, we have $\mathbf{\hat y}=X(X^TX)^{-1}X^T\mathbf{y}$, so the above expression for $\text{Cov}(\mathbf{\hat y}, \mathbf{y})$ becomes</p>

<script type="math/tex; mode=display">\text{Cov}(\mathbf{\hat y}, \mathbf{y})=X(X^TX)^{-1}X^T\text{Cov}(\mathbf{y}, \mathbf{y})=\sigma^2X(X^TX)^{-1}X^T.</script>

<p>Thus,</p>

<script type="math/tex; mode=display">\text{Cov}(\hat y_i, y_i)=\sigma^2e_i^TX(X^TX)^{-1}X^Te_i=\sigma^2x_i^T(X^TX)^{-1}x_i</script>

<p>where $x_i=X^Te_i$ is the $i$th row of $X$ or $i$th sample’s feature vector. According to the given formula, we get</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
df(\mathbf{\hat y}) &= \sum_{i=1}^N x_i^T(X^TX)^{-1}x_i \\
&= \sum_{i=1}^N\text{tr}(x_i^T(X^TX)^{-1}x_i) \\
&= \sum_{i=1}^N\text{tr}(x_ix_i^T(X^TX)^{-1}) \\
&= \text{tr}\left(\left(\sum_{i=1}^Nx_ix_i^T\right)(X^TX)^{-1}\right).
\end{split} %]]&gt;</script>

<p>If you’re not familar the basic properties of trace, you can refer to <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)#Properties">this page</a>. Note that</p>

<script type="math/tex; mode=display">\sum_{i=1}^Nx_ix_i^T=[x_1\ x_2\ \dots\ x_N]
\begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_N^T\end{bmatrix}=X^TX.</script>

<p>Thus, when there are $k$ predictors we get</p>

<script type="math/tex; mode=display">\text{df}(\mathbf{\hat y})=\text{tr}(I_{k\times k})=k,</script>

<p>the claimed result for OLS in the book. Similarly for ridge regression,</p>

<script type="math/tex; mode=display">\text{Cov}(\mathbf{\hat y}, \mathbf{y})=X(X^TX+\lambda I)^{-1}X^T\text{Cov}(\mathbf{y}, \mathbf{y})=\sigma^2X(X^TX+\lambda I)^{-1}X^T.</script>

<script type="math/tex; mode=display">\text{Cov}(\hat y_i, y_i)=\sigma^2e_i^TX(X^TX+\lambda I)^{-1}X^Te_i=\sigma^2x_i^T(X^TX+\lambda I)^{-1}x_i</script>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
df(\mathbf{\hat y})
&= \sum_{i=1}^N\text{tr}(x_ix_i^T(X^TX+\lambda I)^{-1}) \\
&= \text{tr}(X^TX(X^TX+\lambda I)^{-1}) \\
&= \text{tr}(X(X^TX+\lambda I)^{-1}X^T),
\end{split} %]]&gt;</script>

<p>which is the <strong>Equation (3.50)</strong> in the book.</p>

<h3 id="references">References</h3>

<ul>
  <li><a href="/blog/2017/09/01/esl-chapter-2/">Notes on Mathematics for ESL: Chaper 2</a></li>
  <li><a href="http://waxworksmath.com/Authors/G_M/Hastie/hastie.html">Notes on The Elements of Statistical Learning (by John Weatherwax)</a></li>
  <li><a href="http://www.stat.cmu.edu/~larry/all-of-statistics/">All of Statistics (by Larry Wasserman)</a></li>
</ul>
]]></content>
  </entry>
  
</feed>
