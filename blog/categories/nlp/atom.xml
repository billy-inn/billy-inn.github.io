<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Nlp | Billy Ian's Short Leisure-time Wander]]></title>
  <link href="http://billy-inn.github.io/blog/categories/nlp/atom.xml" rel="self"/>
  <link href="http://billy-inn.github.io/"/>
  <updated>2022-06-22T00:33:25-04:00</updated>
  <id>http://billy-inn.github.io/</id>
  <author>
    <name><![CDATA[Peng (Billy) Xu]]></name>
    <email><![CDATA[bly930725@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Shortcut Learning Hypothesis of Modern Language Models]]></title>
    <link href="http://billy-inn.github.io/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models/"/>
    <updated>2022-03-21T17:33:56-04:00</updated>
    <id>http://billy-inn.github.io/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models</id>
    <content type="html"><![CDATA[<p><strong><em>Disclaimer:</em></strong>
<strong><em>This post was completed in my spare time, with no relevance to my current work. And opinions in this post are my own, not my employers’.</em></strong></p>

<p>As the already gigantic modern language models become ever larger by the day, people seem to ignore many existing works discussing their limitations. In this blog post, I try to connect the results and observations from several such works to form the “shortcut learning hypothesis” of those language models. Such a hypothesis implies that modern language models are fundamentally flawed to effectively capture long-range dependencies or complicated structures of the data, <strong>if</strong> we still stick with the current training objective. Hopefully, this post can help people take a step back and ponder a bit first before going all-in into the current scale competition of language models.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://i.imgur.com/UQJLGPd.png" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">A 137B parameter language model fails to answer a simple question with prompting. (The example comes from <a href="https://arxiv.org/abs/2201.11903">this paper</a>)</td>
    </tr>
  </tbody>
</table>

<!--more-->

<h3 id="preliminaries">Preliminaries</h3>

<p>A language model (LM) is an estimate $Q(x_{1:T})$ of the true underlying probability distribution $P(x_{1:T})$ over sequence of text $x_{1:T}=(x_1,\dots,x_T)$, consisting of tokens $x_t$ from a fixed vocabulary. Prevailing neural language models estimate the joint distribution $Q(x_{1:T})$ autoregressively which is implicitly defined by the conditional distributions $Q(x_t \vert x_{:t})$. Such an autoregressive factorization possesses several benefits:</p>

<ul>
  <li>It's efficiently computable;</li>
  <li>It's well aligned with the human intuition to speak, read and write text sequentially;</li>
  <li>It can be trained in an unsupervised fashion with massive amount of raw text;</li>
  <li>Text can be conveniently generated by sampling from $Q(x_t \vert x_{:t})$ in a sequential fashion</li>
</ul>

<p>It is standard for such a language model to be trained to minimize the cross entropy objective:</p>

<script type="math/tex; mode=display">\mathcal{L}_{MLE}=-\frac1T \mathbb{E}_{x_{1:T}\sim P}\left[\sum_{t=1}^T\log Q(x_t|x_{:t})\right]=-\frac1T\mathbb{E}_{x_{1:T}\sim P}[\log Q(x_{1:T})]</script>

<p>With such a formulation, the problem becomes one of the most basic learning tasks of predicting the next observation $x_t$ given a sequence of past observations $x_1,x_2,\dots,x_{t-1}$. And the objective is basically to minimize the “average error (uncertainty / entropy)” on the token level, which is commonly referred to as “perplexity” in the literature. Masked token prediction, a popular variant of the standard objective, first proposed in <a href="https://arxiv.org/abs/1810.04805">the BERT paper</a> breaks the autoregressive factorization, but its characteristic of “average error” remains unchanged.</p>

<h3 id="long-range-dependencies">Long-Range Dependencies</h3>

<p>For the natural languages, the true distributions $P$ exhibit complex interactions between distant observations, <em>i.e.</em>, long-range dependencies. Modern LMs based on the Transformer architecture have achieved tremendous successes in NLP by pushing the “average error” low enough. However, these large models still struggle to effectively capture long-range dependencies, for example, <a href="https://arxiv.org/abs/1909.10705">generating long and coherent stories</a>, <a href="https://arxiv.org/abs/2112.08608">answering questions depending on long context</a> or <a href="https://arxiv.org/abs/2202.07206">robustly performing numerical reasoning</a>.</p>

<p>One limitation of the transformer-based LMs is the fixed number of tokens they can encode at once, and increasing this number linearly introduces a quadratic computational cost. Recently, <a href="https://arxiv.org/abs/2009.06732">a lot of efforts</a> are dedicated to make the Transformer architecture more efficient to encode longer context, in the hope to help capture long-range dependencies. However, <a href="https://arxiv.org/abs/2109.09115">one recent EMNLP paper</a> showed that whether encoding longer context can help capture long-range dependencies remains unclear. It seems that the LMs still largely rely on the most recent observations to make their predictions even though they have direct access to far distant past.</p>

<h3 id="average-error-is-theoretically-not-good-enough">“Average Error” is Theoretically Not Good Enough</h3>

<p><a href="https://arxiv.org/abs/1612.02526">The STOC paper “Prediction with a Short Memory”</a> presents an interesting theoretical result. The paper is quite technical, but the key result is quite intuitive. Here is the most important proposition (to me) from the paper:</p>

<blockquote>
  <p>Let $\mathcal{M}$ be any distribution over sequences with mutual information $I(\mathcal{M})$ between past observations $\dots,x_{t-2},x_{t-1}$ and future observations $x_t, x_{t+1},\dots$. The best $l$-th order Markov model, which makes predictions based only on the most recent $l$ observations, predicts the distribution of the next observation with average KL error $I(\mathcal{M})/l$, with respect to the actual conditional distribution of $x_t$ given all past observations.</p>
</blockquote>

<p>Essentially, it shows that a Markov model – a model that cannot capture long-range dependencies or structure of the data – can predict accurately on <strong>any</strong> data-generating distribution, provided the order of the Markov model scales with the complexity of the distribution, as parameterized by the mutual information between the past and future. Strikingly, this parameterization is indifferent to whether the dependencies in the sequence are relatively short-range or very long-range. Independent of the nature of these dependencies, provided the mutual information is small, accurate prediction is possible based only on the most recent few observations.</p>

<p>Intuitively, it means that the “average error” can be pushed low enough without capturing long-range dependencies at all, by only doing well on the time steps when prediction relies little on long-range dependencies. There is one condition to make this argument valid, that is, the amount of long-range dependencies is small. To get a sense on whether this condition is met, let’s take a partition of the sequence $x_{1:T}$ into $A = x_{1:t}$ and $B=x_{t+1:T}$. Then, the cross entropy objective is equivalent to:</p>

<script type="math/tex; mode=display">\mathcal{L}_{MLE}=-\mathbb{E}_{x_{1:T}\sim P}\left[\frac1t\log Q(A)+\frac1{T-t}\log Q(B|A)\right].</script>

<p>As $T$ increases, the configuration space of $B$ grows exponentially $\lvert\mathcal{B}\vert \sim d^{T-t}$, where $d$ is the vocabulary size and $\mathcal{B}$ is the set of all possible instances of $B$. However, with one specific instance of $A$ fixed, the amount of possible dependencies with $\mathcal{B}$ remains relatively small. As a result, the dependencies between $A$ and $B$ for large $T$ are very rare. In short, long-range dependencies are likely very sparse <strong>on average</strong> in the data.</p>

<p>The above result and intuition imply that the “average error”, though ubiquitously used in practice, is not a good metric to train and evaluate the LMs, <strong>if</strong> we are interested in capturing long-range dependencies. As long as the number of dependencies is not too large (usually valid), models with no capability to capture long-range dependencies can still perform well under the “average error”.</p>

<h3 id="shortcut-learning-hypothesis">Shortcut Learning Hypothesis</h3>

<p>One may argue that such a result only reveals that models with a short memory can perform well measured by the “average error”. But it is not  direct evidence that modern LMs trained with the “average error” are fundamentally flawed to capture long-range dependencies. Though without rigorous proof, I hypothesize that shortcut learning exists in modern LMs as a direct outcome of optimizing the “average error”. As put in <a href="https://arxiv.org/abs/2004.07780">the paper “Shortcut Learning in Deep Neural Networks”</a>, “<strong>shortcut learning typically reveals itself by a strong discrepancy between intended and actual learning strategy, causing an unexpected failure</strong>”. The figure below shows a toy example of shortcut learning.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"><img src="https://i.imgur.com/GiH05IL.png" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><em>Toy example of shortcut learning. When trained on a simple dataset of stars and moons (top row), a standard neural network can easily categorise novel similar exemplars (middle row). However, tesing it on a slightly different dataset (bottom row) reveals a shortcut strategy: The network has learned to associate object location with a category.</em></td>
    </tr>
  </tbody>
</table>

<p>In the case of modern LMs, we hope that they will capture long-range dependencies naturally by scaling up with more and more parameters and data. However, with the “average error” as the main (usually, the only) objective, they learn unintended shortcuts by only leveraging recent observations but ignore most of the long-range dependencies. And as suggested by the above metioned theoretical result, such shortcuts indeed exist. (Such a phenomenon can also be interpreted as learning of spurious correlations in the data, encouraged by the inductive bias introduced by the “average error” objective.)</p>

<p>Actually, many empirical observations support the “shortcut learning hypothesis”. The most obvious evidence to me is the current large LMs’ inability to effectively capture long-range dependencies and understand complicated structures of the data, manifested in many complicated real-world tasks:</p>

<ul>
  <li>They fail to generate coherent long documents;</li>
  <li>They perform poorly answering questions requiring logic, reasoning or grounding;</li>
  <li>They easily misunderstand the high-level structure of the long documents;</li>
  <li>They tend to hallucinate facts contradicting either the common knowledge or the given long context.</li>
</ul>

<p>From my point of view, the models are learning in an unintended way that differs from what we, as humans, expect. Despite performing well under the “average error”, they try to generalize at places where memorizations are actually needed (e.g., factual information), and memorize at places where generalizations are actually needed (e.g., logic and reasoning). It is the typical characteristic of shortcut learning!</p>

<p>More clues are also observed in different quantitative ways. <a href="https://arxiv.org/abs/1905.11978">Our previous AISTATS paper</a> found that there is a large discrepancy between $I(P)$ and $I(Q)$. Recall that $I(\mathcal{M})$ is the mutual information between past and future observations of any distribution $\mathcal{M}$, $P$ is the true distribution, and $Q$ is the model distribution. Basically, it shows that the learnt model distribution $Q$ exhibits much fewer long-range dependencies than the true distribution $P$. It may be the reason why the current language models are unable to generate long coherent documents. Long-range dependencies are largely lost when conditioning on the models’ own outputs.</p>

<p>In a similar vein, <a href="https://arxiv.org/abs/1906.05664">this ICML paper</a> observed that the entropy rate of the model distribution $Q$</p>

<script type="math/tex; mode=display">EntRate(Q) = -\frac1T\mathbb{E}_{x_{1:T}\sim Q}[\log Q(x_{1:T})]</script>

<p>diverges quickly from the cross entropy objective</p>

<script type="math/tex; mode=display">CE(P || Q) = -\frac1T\mathbb{E}_{x_{1:T}\sim P}[\log Q(x_{1:T})]</script>

<p>as the length of the generated sequences $T$ increases. Ideally, an accurate language model, we expect that $CE(P \Vert Q) \approx EntRate(Q)$. Such a divergence means that the language models become  increasingly uncertain conditioned on their own outputs, even though they are able to push the “average error” to a very low level with respect to the true distribution. It also highlights that the learnt model distribution $Q$ ignores some crucial properties of the true distribution $P$. Why? Likely these properties, albeit important, do not offer much direct help to decrease the “average error”.</p>

<p>Both of these two observations provide additional empirical support that shortcut learning indeed exists for modern LMs, as a direct outcome of only optimizing the “average error”.</p>

<p><img src="https://i.imgur.com/LWhSCCN.jpg" alt="" /></p>

<h3 id="how-to-do-better">How to Do Better?</h3>

<p>The shortcut learning hypothesis clearly suggests that a better metric/objective may be essential for modern LMs to better capture long-range dependencies or complicated structure of the data. One possibility suggested in <a href="https://arxiv.org/abs/1612.02526">the “Prediction with a Short Memory” paper</a> is to only train and evaluate the models at a chosen set of (hard) time steps instead of all time steps. Hence the models can no longer do well with those unintended shortcuts.</p>

<p>Actually, a similar idea is already widely adopted in the practice of natural language processing. When <a href="https://arxiv.org/abs/2005.14165">GPT-3</a> and <a href="(https://arxiv.org/abs/1810.04805)">BERT</a> first comes out, they seem to possess amazing “zero-shot” or “few-shot” transferrabilities. However, people soon realized such capabilities are largely overestimated, especially on harder tasks, <em>e.g.</em>, question answering. Instead, “<strong>fine-tuning</strong>” the model weights with a downstream objective usually achieves much better performance. Just as revealed by the latest <a href="https://openai.com/blog/instruction-following/">InstructGPT from OpenAI</a>, a smart fine-tuning strategy can make small models outperform much larger models. The wide success of fine-tuning again validates the “shortcut learning hypothesis” and implies that only optimizing the “average error” is not enough in the end.</p>

<p>However, such a strategy is imperfect in many ways:</p>

<ul>
  <li>Fine-tuning relies on small annotated downstream datasets which are much more expensive to scale;</li>
  <li>The fine-tuned models on a specific task may perform worse on the other downstream tasks, so a separate copy of the model may be required for each downstream task;</li>
  <li>Fine-tuning is usually conducted with a very small learning rate, which I doubt can really help escape those unintended shortcuts learnt during the pre-training.</li>
</ul>

<p>Many attempts to reslove these issues did not achieve much success, like multi-task pre-training and fine-tuning reported in <a href="https://arxiv.org/abs/1910.10683">the T5 paper from Google</a>. It is unsurprising to me, since the annotated data, even combined across tasks, is tiny in size as compared to the raw data used for the unsupervised pre-training. As a result, fine-tuning is not good enough and we need a better unsupervised objective other than the “average error”.</p>

<p>While the “average error” is still the dominated unsupervised objective to train LMs, the original <a href="https://arxiv.org/abs/1810.04805">BERT</a> actually introduced two promising directions to improve upon the “average error” metric:</p>

<ul>
  <li>Masked token prediction
    <ul>
      <li>predict the masked tokens corrupted with different strategies.</li>
      <li>Pros: it breaks the autoregressive factorization, which makes more complicated interactions among tokens possible; and it works pretty well in practice.</li>
      <li>Cons: it is still “average error”.</li>
      <li>Masked token prediction is now widely used in follow-up works with many variants.</li>
    </ul>
  </li>
  <li>Next sentence prediction
    <ul>
      <li>distinguish between next sentences and randomly selected sentences.</li>
      <li>Pros: it encourages a higher mutual information $\mathcal{I}(Q)$ under the model distribution.</li>
      <li>Cons: it does not work well in practice.</li>
      <li>Next sentence prediction is dropped quickly by the community due to its empirical ineffectiveness. In <a href="https://arxiv.org/abs/1905.11978">our AISTATS paper</a>, some initial explorations are conducted to help improve this objective on RNN-based LMs.</li>
    </ul>
  </li>
</ul>

<p>Generally, I think both new ways to factorize the joint distribution $P(x_{1:T})$, and new unsupervised objectives beyond the “average error” are worth exploring further down the road. Another interesting direction to me is to leverage the massive information available on the web to serve as LM’s external memory, that is, retrieval-based LMs. Interestingly, three prestigious industrial AI labs all released their efforts in this direction recently. However, they all still rely either on the “average error” (<a href="https://arxiv.org/abs/2112.04426">RETRO from DeepMind</a>) or fine-tuning on a human-annotated dataset (<a href="https://arxiv.org/abs/2201.08239">LaMDA from Google</a> and <a href="https://arxiv.org/abs/2112.09332">WebGPT from OpenAI</a>). <a href="https://arxiv.org/abs/2201.11903">This recent paper from Brain</a> points out an intriguing direction to break down long complicated dependencies into short simple dependencies, though it’s completed through prompting. As suggested by <a href="https://arxiv.org/abs/2202.12837">this paper</a>, prompting is unlikely to work on task semantics not close enough to the LM pre-training objective. But these initial efforts are definitely meaningful and provide us guidance towards a better LM paradigm which may circumvent the “shortcut learning hypothesis” to better capture long-range dependencies and understand complicated structures of the data.</p>

<h3 id="conclusion">Conclusion</h3>

<p>Benefitting from the scaling success, modern LMs are becoming a general purpose model able to handle all kinds of tasks (through fine-tuning and prompting). As a result, their impact and implications are also becoming ever larger. In addition, the discussions of modern LMs are also becoming more and more controversial on the social media. As the stake is so high right now, some frank discussions about their limitations appear to be more indispensable, like <a href="https://arxiv.org/abs/2202.07785">this latest paper from Anthropic</a>. Hopefully, this post can also contribute to such a purpose.</p>

<h3 id="acknowledgements"><em>Acknowledgements</em></h3>

<p><em>Thanks for the valuable feedback from Vatsal Sharan (USC/Stanford), Robert Geirhos (University of Tübingen), Yu Hou (USC), Guy Gur-Ari (X, Blueshift), Denny Zhou (Google Brain), Jeff Dean (Google Cloud ML / Google Research) and many other Google colleagues of mine.</em></p>
]]></content>
  </entry>
  
</feed>
