
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Notes on Convex Optimization (4): Gradient Descent Method - Billy Ian's Short Leisure-time Wander</title>
  <meta name="author" content="Peng (Billy) Xu">

  
  <meta name="description" content="Descent methods $f(x^{(k+1)}) &lt; f(x^{(k)})$ $\Delta x$ is the step or search direction; $t$ is the step size or step length from convexity, $\ &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://billy-inn.github.io/blog/2018/11/05/convex-optimization-4/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Billy Ian's Short Leisure-time Wander" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<script src="/mathjax/unpacked/MathJax.js"></script>

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-69271262-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Billy Ian's Short Leisure-time Wander</a></h1>
  
    <h2>into learning, investment, intelligence and beyond</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="billy-inn.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Notes on Convex Optimization (4): Gradient Descent Method</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2018-11-05T00:29:05-05:00'><span class='date'>2018-11-05</span> <span class='time'>12:29 am</span></time>
        
           | <a href="#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><h3 id="descent-methods">Descent methods</h3>

<script type="math/tex; mode=display">x^{(k+1)}=x^{(k)} + t^{(k)}\Delta x^{(k)}</script>

<ul>
  <li>$f(x^{(k+1)}) &lt; f(x^{(k)})$</li>
  <li>$\Delta x$ is the <em>step</em> or <em>search direction</em>; $t$ is the <em>step size</em> or <em>step length</em></li>
  <li>from convexity, $\nabla f(x)^T \Delta x &lt;0$</li>
</ul>

<blockquote>
  <p><em>General descent method</em>. <br />
<strong>given</strong> a starting point $x \in \mathbf{dom} \enspace f$. <br />
<strong>repeat</strong> <br />
- Determine a descent direction $\Delta x$. <br />
- <em>Line search</em>. Choose a step size $t &gt; 0$. <br />
- Update. $x := x+ t\Delta x$.</p>

  <p><strong>until</strong> stopping criterion is satisfied</p>
</blockquote>

<!--more-->

<h4 id="exact-line-search">Exact line search</h4>

<script type="math/tex; mode=display">t = \text{argmin}_{t > 0} \enspace f(x + t\Delta x)</script>

<h4 id="backtracking-line-search">Backtracking line search</h4>

<blockquote>
  <p><em>Backtracking line search</em>. <br />
<strong>given</strong> a descent direction $\Delta x$ for $f$ at $x \in \mathbf{dom} f, \alpha \in(0,0.5), \beta\in(0,1)$. <br />
<strong>starting</strong> at $t:=1$. <br />
<strong>while</strong> $f(x+t\Delta x) &gt; f(x) + \alpha t \nabla f(x)^T \Delta x$, $t:=\beta t$</p>
</blockquote>

<h3 id="gradient-descent-method">Gradient descent method</h3>

<p>A natural choice for the search direction is the negative gradient $\Delta x = - \nabla f(x)$.</p>

<h4 id="convergence-analysis-for-exact-line-search">Convergence analysis for exact line search</h4>

<p>We must have $f(x^(k)) - p^\ast \le \epsilon$ after at most</p>

<script type="math/tex; mode=display">\frac{\log((f(x^{(0)})-p^\ast)/\epsilon)}{\log(1/c)}</script>

<p>iterations of the gradient method with exact line search, where $c=1-m/M&lt;1$.</p>

<h4 id="convergence-analysis-for-backtracking-line-search">Convergence analysis for backtracking line search</h4>

<p>Similar to exact line search, except that $c=1 - \min{2m\alpha, 2\beta\alpha m/M} &lt; 1.$</p>

<h4 id="conclusions">Conclusions</h4>

<ul>
  <li>The gradient method often exhibits approximately linear convergence, <em>i.e.</em>, the error $f(x^{(k)}) - p^\ast$ converges to zeros approximately as a geometric series.</li>
  <li>The choice of backtracking parameters $\alpha, \beta$ has a noticeable but not dramatic effect on the convergence. An exact line search sometimes improves the convergence of the gradient method, but the effect is not large.</li>
  <li>The convergence rate depends greatly on the condition number of the Hessian, or the sublevel sets. Convergence can be very slow, even for problems that are moderately well conditioned. When the condition number is larger the gradient method is so slow that it is useless in practice.</li>
</ul>

<h3 id="steepest-descent-method">Steepest descent method</h3>

<p>The first-order Taylor approximation of $f(x+v)$ around $x$ is</p>

<script type="math/tex; mode=display">f(x+v)\approx \hat f(x+v)=f(x) + \nabla f(x)^T v.</script>

<p>The second term on the righthand side, $\nabla f(x)^T v$, is the <em>directional derivative</em> of $f$ at $x$ in the direction $v$. It gives the approximate change in $f$ for a small step $v$. The step $v$ is a descent direction if the directional derivative is negative.</p>

<p>Let $\lVert \cdot \rVert$ be any norm on $\mathbf{R}^n$. We define a <em>normailzied steepest descent direction</em> as</p>

<p>\begin{equation}
\Delta x_{nsd} = \arg\min{\nabla f(x)^T v\ \vert\ \lVert v \rVert = 1}.
\tag{1}\label{eq:1}
\end{equation}</p>

<p>It is also convenient to consider a steepest descent step $\Delta x_{sd}$ that is <em>unnormalized</em>, by scaling the normalized steepest descent direction in a particular way:</p>

<p>\begin{equation}
\Delta x_{sd} = \lVert \nabla f(x) \rVert_\ast \Delta x_{nsd},
\tag{2}\label{eq:2}
\end{equation}</p>

<p>where $\lVert \cdot \rVert_\ast$ denotes the dual norm. Note that for the steepest descent step, we have</p>

<script type="math/tex; mode=display">\nabla f(x)^T \Delta x_{sd} = \lVert \nabla f(x) \rVert_\ast \nabla f(x)^T \nabla x_{nsd}=-\lVert \nabla f(x) \rVert_\ast^2.</script>

<h4 id="steepest-descent-for-euclidean-norm">Steepest descent for Euclidean norm</h4>

<p>To simplify the notation, we can look at the problem of solving $\min_v{u^Tv\ \lvert\ \lVert v \rVert \le 1}$ which ends up being equivalent to find the normalized steepest descent step.</p>

<p>The Cauchy-Schwarz inequality gives $\lvert u^Tv\rvert \le \rVert u \rVert \lVert v \rVert$, hence it is easy to see that the minimum is $\min_v{u^Tv\ \lvert\ \lVert v \rVert \le 1}=-\lVert u \rVert$, and the minimizer is $v=-u/\lVert u \rVert$. As a result, the steepest descent direction is simply the negative gradient, <em>i.e.</em>, $\Delta x_{sd} = - \nabla f(x)$.</p>

<h4 id="steepest-descent-for-quadratic-norm">Steepest descent for quadratic norm</h4>

<p>We consider the quadratic norm</p>

<script type="math/tex; mode=display">\lVert z \rVert_P = (z^TPz)^{1/2}=\lVert P^{1/2}z\rVert_2,</script>

<p>where $P \in \mathbf{S}_{++}^n$. The problem is now $\min_v{u^Tv\ \vert\ \lVert P^{1/2}v\rVert\le1}=\min_v{u^Tv\ \vert\ \lVert\delta\rVert\le1, v=P^{-1/2}\delta}$.
This is equivalent to $\min_\delta{((P^{-1/2})^Tu)^T\delta\ \vert\ \lVert\delta\rVert\le1}$. The problem above shows that the minimum is $-\lVert (P^{-1/2})^Tu\rVert$ while the maximum $\lVert (P^{-1/2})^Tu\rVert$ is the dual norm according to the definition, and the minimizer is $v=P^{-1/2}\delta=-P^{-1}u/\lVert (P^{-1/2})^Tu\rVert$, so the steepest descent desnt is given by</p>

<script type="math/tex; mode=display">\Delta x_{sd} = -P^{-1} \nabla f(x).</script>

<p>In addition, the steepest descent method in the quadratic norm $\lVert \cdot \rVert_P$ can be thought of as the gradient method applied to the problem after the change of coordinates $\bar x=P^{1/2}x$.</p>

<h4 id="steepest-descent-for-l1-norm">Steepest descent for $l_1$-norm</h4>

<p>Let $i$ be any index for which $\lVert \nabla f(x) \rVert_\infty = \lvert (\nabla f(x))_i \rvert$. Then a normalized steepest descent direction $\nabla x_{nsd}$ for the $l_1$-norm is given by</p>

<script type="math/tex; mode=display">\Delta x_{nsd}=-\text{sign}\left(\frac{\partial f(x)}{\partial x_i}\right)e_i,</script>

<p>where $e_i$ is the $i$th standard basis vector. An unnormalized steepest descent step is then</p>

<script type="math/tex; mode=display">\Delta x_{sd} = \Delta x_{nsd}\lVert \nabla f(x) \rVert_\infty = - \frac{\partial f(x)}{\partial x_i}e_i.</script>

<p>The steepest descent algorithm in the $l_1$-norm has a very natural interpertation: At each iteration we select a component of $\nabla f(x)$ with maximum absolute value, and then decrease or increase the corresponding component of $x$, according to the sign of $(\nabla f(x))_i$. The algorithm is sometimes called a <em>corrdinate-descent</em> algorithm, since only one component of the variable $x$ is updated at each iteration.</p>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Peng (Billy) Xu</span></span>

      




<time class='entry-date' datetime='2018-11-05T00:29:05-05:00'><span class='date'>2018-11-05</span> <span class='time'>12:29 am</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/optimization/'>optimization</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://billy-inn.github.io/blog/2018/11/05/convex-optimization-4/" data-via="billyinn93" data-counturl="http://billy-inn.github.io/blog/2018/11/05/convex-optimization-4/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/" title="Previous Post: Notes on Convex Optimization (3): Unconstrained Minimization Problems">&laquo; Notes on Convex Optimization (3): Unconstrained Minimization Problems</a>
      
      
        <a class="basic-alignment right" href="/blog/2018/11/13/convex-optimization-5/" title="Next Post: Notes on Convex Optimization (5): Newton's Method">Notes on Convex Optimization (5): Newton's Method &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  <!--
    <section>
	<ul id="about" class="nav nav-list">
		<li class="nav-header">About Me</a></li>
			<p>A researcher and engineer passionate about Machine Learning and Natural Language Processing</p>
      <p>A calculated speculator seeking for risk and reward asymmetries, and a value investor sticking to the margin of safety.</p>
			<p>Play piano and Nintendo Switch</p>
      <p>Read history, investment, Sci-Fi and fantasy</p>
      <p>Bodyweight fitness, hiking and skiing</p>
			<p>Fan of Portland Trail Blazers, Former SNH48 member Kiku and Blackpink member Ros&eacute</p>
      <p>Toronto, Edmonton, Beijing and Suzhou</p>
      <p><strong>Opinions posted in this blog are my own!</strong></p>
	</ul>
</section>
<section>
  <h1>Latest Tweets</h1>
  <a class="twitter-timeline" data-chrome="nofooter transparent noheader" data-tweet-limit="5" href="https://twitter.com/billy_nlp">Tweets by billy_nlp</a> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
  <a class="twitter-follow-button" href="https://twitter.com/billy_nlp">Follow @billy_nlp</a>
</section>
<section class="well">
 <ul id="categories_list" class="nav nav-list">
	<li class="nav-header">Categories</li>
	  <li id="categories"><li class='category'><a href='/blog/categories/life/'>life (3)</a></li>
<li class='category'><a href='/blog/categories/machine-learning/'>machine_learning (11)</a></li>
<li class='category'><a href='/blog/categories/nlp/'>nlp (1)</a></li>
<li class='category'><a href='/blog/categories/optimization/'>optimization (5)</a></li>
<li class='category'><a href='/blog/categories/pgm/'>pgm (1)</a></li>
<li class='category'><a href='/blog/categories/project/'>project (1)</a></li>
<li class='category'><a href='/blog/categories/reading/'>reading (2)</a></li>
<li class='category'><a href='/blog/categories/reinforcement-learning/'>reinforcement_learning (4)</a></li>
<li class='category'><a href='/blog/categories/statistics/'>statistics (3)</a></li>
</li>
 </ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models/">Shortcut Learning Hypothesis of Modern Language Models</a>
      </li>
    
      <li class="post">
        <a href="/blog/2022/01/06/navigate-through-the-current-ai-job-market-a-retrospect/">Navigate Through the Current AI Job Market: A Retrospect</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/13/convex-optimization-5/">Notes on Convex Optimization (5): Newton's Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/05/convex-optimization-4/">Notes on Convex Optimization (4): Gradient Descent Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/">Notes on Convex Optimization (3): Unconstrained Minimization Problems</a>
      </li>
    
  </ul>
</section>
<section class="well">
  <ul id="license" class="nav nav-list">
    <li class="nav-header">License</li>
<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.
	</li>
  </ul>
</section>

    -->
  <section>
	<ul id="about" class="nav nav-list">
		<li class="nav-header">About Me</a></li>
			<p>A researcher and engineer passionate about Machine Learning and Natural Language Processing</p>
      <p>A calculated speculator seeking for risk and reward asymmetries, and a value investor sticking to the margin of safety.</p>
			<p>Play piano and Nintendo Switch</p>
      <p>Read history, investment, Sci-Fi and fantasy</p>
      <p>Bodyweight fitness, hiking and skiing</p>
			<p>Fan of Portland Trail Blazers, Former SNH48 member Kiku and Blackpink member Ros&eacute</p>
      <p>Toronto, Edmonton, Beijing and Suzhou</p>
      <p><strong>Opinions posted in this blog are my own!</strong></p>
	</ul>
</section>
<section>
  <h1>Latest Tweets</h1>
  <a class="twitter-timeline" data-chrome="nofooter transparent noheader" data-tweet-limit="5" href="https://twitter.com/billy_nlp">Tweets by billy_nlp</a> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
  <a class="twitter-follow-button" href="https://twitter.com/billy_nlp">Follow @billy_nlp</a>
</section>
<section class="well">
 <ul id="categories_list" class="nav nav-list">
	<li class="nav-header">Categories</li>
	  <li id="categories"><li class='category'><a href='/blog/categories/life/'>life (3)</a></li>
<li class='category'><a href='/blog/categories/machine-learning/'>machine_learning (11)</a></li>
<li class='category'><a href='/blog/categories/nlp/'>nlp (1)</a></li>
<li class='category'><a href='/blog/categories/optimization/'>optimization (5)</a></li>
<li class='category'><a href='/blog/categories/pgm/'>pgm (1)</a></li>
<li class='category'><a href='/blog/categories/project/'>project (1)</a></li>
<li class='category'><a href='/blog/categories/reading/'>reading (2)</a></li>
<li class='category'><a href='/blog/categories/reinforcement-learning/'>reinforcement_learning (4)</a></li>
<li class='category'><a href='/blog/categories/statistics/'>statistics (3)</a></li>
</li>
 </ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models/">Shortcut Learning Hypothesis of Modern Language Models</a>
      </li>
    
      <li class="post">
        <a href="/blog/2022/01/06/navigate-through-the-current-ai-job-market-a-retrospect/">Navigate Through the Current AI Job Market: A Retrospect</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/13/convex-optimization-5/">Notes on Convex Optimization (5): Newton's Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/05/convex-optimization-4/">Notes on Convex Optimization (4): Gradient Descent Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/">Notes on Convex Optimization (3): Unconstrained Minimization Problems</a>
      </li>
    
  </ul>
</section>
<section class="well">
  <ul id="license" class="nav nav-list">
    <li class="nav-header">License</li>
<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.
	</li>
  </ul>
</section>

</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2022 - Peng (Billy) Xu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		  jax: ["input/TeX", "output/HTML-CSS"],
		    tex2jax: {
				    inlineMath: [ ['$', '$'] ],
					    displayMath: [ ['$$', '$$']],
						    processEscapes: true,
							    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
								  },
								    messageStyle: "none",
									  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
								  });
							  </script>
							  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'billy-inn';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://billy-inn.github.io/blog/2018/11/05/convex-optimization-4/';
        var disqus_url = 'http://billy-inn.github.io/blog/2018/11/05/convex-optimization-4/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
