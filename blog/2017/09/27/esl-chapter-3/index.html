
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>[Notes on Mathematics for ESL] Chapter 3: Linear Regression Models and Least Squares - Billy Ian's Short Leisure-time Wander</title>
  <meta name="author" content="Peng (Billy) Xu">

  
  <meta name="description" content="3.2 Linear Regression Models and Least Squares Derivation of Equation (3.8) The least squares estimate of $\beta$ is given by the book’s Equation (3. &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://billy-inn.github.io/blog/2017/09/27/esl-chapter-3/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Billy Ian's Short Leisure-time Wander" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<script src="/mathjax/node_modules/mathjax-full/js/MathJax.js"></script>

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-69271262-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Billy Ian's Short Leisure-time Wander</a></h1>
  
    <h2>into learning, investment, intelligence and beyond</h2>
  
</hgroup>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="billy-inn.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">[Notes on Mathematics for ESL] Chapter 3: Linear Regression Models and Least Squares</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-09-27T21:30:12-04:00'><span class='date'>2017-09-27</span> <span class='time'>9:30 pm</span></time>
        
           | <a href="#disqus_thread"
             data-disqus-identifier="http://billy-inn.github.io">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><h3 id="linear-regression-models-and-least-squares">3.2 Linear Regression Models and Least Squares</h3>

<h4 id="derivation-of-equation-38">Derivation of Equation (3.8)</h4>

<p>The least squares estimate of $\beta$ is given by the book’s <strong>Equation (3.6)</strong></p>

<script type="math/tex; mode=display">\hat\beta=(X^TX)^{-1}X^T\mathbf{y}.</script>

<p>From the <a href="/blog/2017/09/01/esl-chapter-2/">previous post</a>, we know that $\mathrm{E}(\mathbf{y})=X\beta$. As a result, we obtain</p>

<script type="math/tex; mode=display">\mathrm{E}(\hat\beta)=(X^TX)^{-1}X^TX\beta=\beta.</script>

<p>Then, we get</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
\hat\beta-\mathrm{E}(\hat\beta)&=(X^TX)^{-1}X^T(\mathbf{y}-X\beta) \\
&=(X^TX)^{-1}X^T\varepsilon.
\end{split} %]]></script>

<p>The variance of $\hat \beta$ is computed as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
\mathrm{Var}(\hat\beta) &= \mathrm{E}[(\hat\beta-\mathrm{E}(\hat\beta)(\hat\beta-\mathrm{E}(\hat\beta))^T] \\
&= (X^TX)^{-1}X^T\mathrm{Var}(\varepsilon)X(X^TX)^{-1}.
\end{split} %]]></script>

<p>If we assume that the entries of $\mathbf{y}$ are uncorrelated and all have the same variance of $\sigma^2$, then $\mathrm{Var}(\varepsilon)=\sigma^2I_N$ and the above equation becomes</p>

<script type="math/tex; mode=display">\mathrm{Var}(\hat\beta)=(X^TX)^{-1}\sigma^2.</script>

<p>This completes the derivation of <strong>Equation (3.8)</strong>.</p>

<!--more-->

<h4 id="thoughts-on-equation-312-and-313">Thoughts on Equation (3.12) and (3.13)</h4>

<p>There are a lot concepts of statistics in this part. It’s better to go through Chapter 6 and Chapter 10 in <a href="http://www.stat.cmu.edu/~larry/all-of-statistics/"><em>All of Statistics</em></a> to have a taste about hypothesis tests and confidence intervals.</p>

<p>From my own viewpoint, Z-score and F-statistic give a measure about whether the corresponding features are useful or not. They can be used within some feature selection methods. However, they’re not very useful in practice. The perferred feature selection methods are discussed in <strong>Section 3.3</strong> in the book.</p>

<h4 id="interpretations-of-equation-320-and-322">Interpretations of Equation (3.20) and (3.22)</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
\text{MSE}(\tilde\theta) &= \text{E}(\tilde\theta-\theta)^2 \\
&= \text{E}(\tilde\theta-\text{E}(\tilde\theta)+\text{E}(\tilde\theta)-\theta)^2 \\
&= \text{Var}(\tilde\theta)+2(\text{E}(\tilde\theta)-\text{E}(\tilde\theta))(\text{E}(\tilde\theta)-\theta)+(\text{E}(\tilde\theta)-\theta)^2 \\
&= \text{Var}(\tilde\theta)+(\text{E}(\tilde\theta)-\theta)^2
\end{split} %]]></script>

<p>which completes the derivation of <strong>Equation (3.20)</strong>.</p>

<p><strong>Equation (3.22)</strong> shows that the expected quadratic error can be broken down into two parts as</p>

<script type="math/tex; mode=display">\text{E}(Y_0-\tilde f(x_0))^2=\sigma^2+\text{MSE}(\tilde f(x_0))</script>

<p>The first error component $\sigma^2$ is unrelated to what model is used to describe our data. It cannot be reduced for it exists in the true data generation process. The second source of error corresponding to ther term $\text{MSE}(\tilde f(x_0))$ represents the error in the model and is under control of us. By <strong>Equation (3.20)</strong>, the mean square error can be broken down into two terms: a model variance term and a model bias squared term. How to make these two terms as small as possible while considering the trade-offs between them is the central topic in the book.</p>

<h4 id="notes-on-multiple-regression-from-simple-univariate-regression">Notes on Multiple Regression from Simple Univariate Regression</h4>

<p>The first thing that comes to my mind when I read this section is that why we need this when we already have the ordinary least square (OLS) estimate of $\beta$:</p>

<script type="math/tex; mode=display">\hat \beta = (X^TX)^{-1}X^TY.</script>

<p>It’s because we want to study how to obtain orthogonal inputs instead of correlated inputs, since orthogonal inputs have some nice properties.</p>

<p>Following Algorithm 3.1, we can transform the correlated inputs $\mathbf{x}$ to the orthogonal inputs $\mathbf{z}$. Another view is that we form an orthogonal basis by performing the Gram-Schmidt orthogonilization procedure on $X$’s column vectors and obtain an orthogonal basis $\mathbf{z}_{i=1}^p$. With this basis, linear regression can be done simply as in the univariate case as shown in <strong>Equation (3.28)</strong>:</p>

<script type="math/tex; mode=display">\hat \beta_p=\frac{\langle\mathbf{z}_p, \mathbf{y}\rangle}{\langle\mathbf{z}_p, \mathbf{z}_p\rangle}.</script>

<p>Following this equation, we can derive <strong>Equation (3.29)</strong>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
\text{Var}(\hat\beta_p)&=\text{Var}\left(\frac{z_p^Ty}{\langle z_p, z_p \rangle}\right)=\frac{z_p^T\text{Var}(y)z_p}{\langle z_p, z_p\rangle^2}=\frac{z_p^T(\sigma^2I)z_p}{\langle z_p, z_p\rangle^2} \\
&=\frac{\sigma^2}{\langle z_p, z_p \rangle}.
\end{split} %]]></script>

<p>We can write the Gram-Schmidt result in matrix form using the QR decomposition as</p>

<script type="math/tex; mode=display">X = QR.</script>

<p>In this decomposition $Q$ is a $N\times(p+1)$ matrix with orthonormal columns and $R$ is a $(p+1)\times(p+1)$ upper triangular matrix. In this representation, the OLS estimate for $\beta$ can be written as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
\hat\beta &= (X^TX)^{-1}X^T\mathbf{y} \\
&= (R^TQ^TQR)^{-1}R^TQ^T\mathbf{y} \\
&= (R^TR)^{-1}R^TQ^T\mathbf{y} \\
&= R^{-1}R^{-T}R^TQ^T\mathbf{y} \\
&= R^{-1}Q^T\mathbf{y}
\end{split} %]]></script>

<p>which is <strong>Equation (3.32)</strong> in the book. Following this equation, the fitted value $\mathbf{\hat y}$ can be written as</p>

<script type="math/tex; mode=display">\mathbf{\hat y}=X\hat\beta=QRR^{-1}Q^T\mathbf{y}=QQ^T\mathbf{y}</script>

<p>which is <strong>Equation (3.33)</strong> in the book.</p>

<h3 id="shrinkage-methods">3.4 Shrinkage Methods</h3>

<h4 id="notes-on-ridge-regression">Notes on Ridge Regression</h4>

<p>If we compute the singular value decomposition (SVD) of the $N\times p$ centered data matrix $X$ as</p>

<script type="math/tex; mode=display">X=UDV^T,</script>

<p>where $U$ is a $N \times p$ matrix with orthonormal columns that span the column space of $X$, $V$ is a $p \times p$ orthogonal matrix, and $D$ is a $p \times p$ diagonal matrix with elements $d_j$ ordered such that $d_1\ge d_2 \ge \dots \ge d_p \ge 0$. From this representation of $X$ we can derive a simple expression for $X^TX$:</p>

<script type="math/tex; mode=display">X^TX=VDU^TUDV^T=VD^2V^T,</script>

<p>which is the <strong>Equation (3.48)</strong> in the book. Using this expression, we can compute the least squares fitted values as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
\hat y^{ls} = X\hat\beta^{ls} &= X(X^TX)^{-1}X^T\mathbf{y}\\
&= UDV^T(VD^2V^T)^{-1}VDU^T\mathbf{y} \\
&= UDV^T(V^{-T}D^{-2}V^{-1})VDU^T\mathbf{y} \\
&= UU^T\mathbf{y} \\
&= \sum_{j=1}^pu_j(u_j^T\mathbf{y})
\end{split} %]]></script>

<p>which is the <strong>Equation (3.46)</strong> in the book. Similarly, we can find solutions for ridge regression as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
\hat y^{ridge}=X\hat \beta^{ridge}&=X(X^TX+\lambda I)^{-1}X^T\mathbf{y} \\
&= UDV^T(VD^2V^T+\lambda VV^T)^{-1}VDU^T\mathbf{y} \\
&= UD(D^2+\lambda I)^{-1}DU^T\mathbf{y} \\
&= \sum_{j=1}^pu_j\frac{d_j^2}{d_j^2+\lambda}u_j^T\mathbf{y}
\end{split} %]]></script>

<p>which is the <strong>Equation (3.47)</strong> in the book. Since we can estimate the sample variance by $X^TX/N$, the variance of $\mathbf{z}_1$ can be derived as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
\text{Var}(\mathbf{z}_1)=\text{Var}(Xv_1)&= (Xv_1)^T(Xv_1)/N\\
&= v_1^TVD^TU^TUDV^Tv_1/N \\
&= v_1^TVD^2V^Tv_1/N \\
&= \frac{d_1^2}N
\end{split} %]]></script>

<p>which is the <strong>Equation (3.49)</strong> in the book. Note that $v_1$ is the first column of $V$ and $V$ is orthogonal, so that $V^Tv_1$ is $[1,0, \dots, 0]^T$.</p>

<h4 id="notes-on-degrees-of-freedom-formula-for-lar-and-lasso">Notes on degrees-of-freedom formula for LAR and Lasso</h4>

<p>The degrees-of-freedom of the fitted vector $\mathbf{\hat y}=(\hat y_1, \dots, \hat y_N)$ is defined as</p>

<script type="math/tex; mode=display">\text{df}(\mathbf{\hat y})=\frac1{\sigma^2}\sum_{i=1}^N\text{Cov}(\hat y_i, y_i)</script>

<p>in the book. Also, it’s claimed that $\text{df}(\mathbf{\hat y})$ is $k$ for ordinary least squares regression and $\text{tr}(\mathbf{S}_{\lambda})$ for ridge regresssion without proof in the book. Here, we’ll derive these two expressions. First, we define $e_i$ as a $N$-element vector of all zeros with a one in the $i$th spot. It’s easy to see that $\hat y_i=e_i^T\mathbf{\hat y}$ and $y_i=e_i^T\mathbf{y}$, so that</p>

<script type="math/tex; mode=display">\text{Cov}(\hat y_i, y_i)=\text{Cov}(e_i^T\mathbf{\hat y}, e_i^T\mathbf{y})=e_i^T\text{Cov}(\mathbf{\hat y},\mathbf{y})e_i.</script>

<p>For OLS regression, we have $\mathbf{\hat y}=X(X^TX)^{-1}X^T\mathbf{y}$, so the above expression for $\text{Cov}(\mathbf{\hat y}, \mathbf{y})$ becomes</p>

<script type="math/tex; mode=display">\text{Cov}(\mathbf{\hat y}, \mathbf{y})=X(X^TX)^{-1}X^T\text{Cov}(\mathbf{y}, \mathbf{y})=\sigma^2X(X^TX)^{-1}X^T.</script>

<p>Thus,</p>

<script type="math/tex; mode=display">\text{Cov}(\hat y_i, y_i)=\sigma^2e_i^TX(X^TX)^{-1}X^Te_i=\sigma^2x_i^T(X^TX)^{-1}x_i</script>

<p>where $x_i=X^Te_i$ is the $i$th row of $X$ or $i$th sample’s feature vector. According to the given formula, we get</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
df(\mathbf{\hat y}) &= \sum_{i=1}^N x_i^T(X^TX)^{-1}x_i \\
&= \sum_{i=1}^N\text{tr}(x_i^T(X^TX)^{-1}x_i) \\
&= \sum_{i=1}^N\text{tr}(x_ix_i^T(X^TX)^{-1}) \\
&= \text{tr}\left(\left(\sum_{i=1}^Nx_ix_i^T\right)(X^TX)^{-1}\right).
\end{split} %]]></script>

<p>If you’re not familar the basic properties of trace, you can refer to <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)#Properties">this page</a>. Note that</p>

<script type="math/tex; mode=display">\sum_{i=1}^Nx_ix_i^T=[x_1\ x_2\ \dots\ x_N]
\begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_N^T\end{bmatrix}=X^TX.</script>

<p>Thus, when there are $k$ predictors we get</p>

<script type="math/tex; mode=display">\text{df}(\mathbf{\hat y})=\text{tr}(I_{k\times k})=k,</script>

<p>the claimed result for OLS in the book. Similarly for ridge regression,</p>

<script type="math/tex; mode=display">\text{Cov}(\mathbf{\hat y}, \mathbf{y})=X(X^TX+\lambda I)^{-1}X^T\text{Cov}(\mathbf{y}, \mathbf{y})=\sigma^2X(X^TX+\lambda I)^{-1}X^T.</script>

<script type="math/tex; mode=display">\text{Cov}(\hat y_i, y_i)=\sigma^2e_i^TX(X^TX+\lambda I)^{-1}X^Te_i=\sigma^2x_i^T(X^TX+\lambda I)^{-1}x_i</script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
df(\mathbf{\hat y})
&= \sum_{i=1}^N\text{tr}(x_ix_i^T(X^TX+\lambda I)^{-1}) \\
&= \text{tr}(X^TX(X^TX+\lambda I)^{-1}) \\
&= \text{tr}(X(X^TX+\lambda I)^{-1}X^T),
\end{split} %]]></script>

<p>which is the <strong>Equation (3.50)</strong> in the book.</p>

<h3 id="references">References</h3>

<ul>
  <li><a href="/blog/2017/09/01/esl-chapter-2/">Notes on Mathematics for ESL: Chaper 2</a></li>
  <li><a href="http://waxworksmath.com/Authors/G_M/Hastie/hastie.html">Notes on The Elements of Statistical Learning (by John Weatherwax)</a></li>
  <li><a href="http://www.stat.cmu.edu/~larry/all-of-statistics/">All of Statistics (by Larry Wasserman)</a></li>
</ul>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Peng (Billy) Xu</span></span>

      




<time class='entry-date' datetime='2017-09-27T21:30:12-04:00'><span class='date'>2017-09-27</span> <span class='time'>9:30 pm</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/machine-learning/'>machine_learning</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://billy-inn.github.io/blog/2017/09/27/esl-chapter-3/" data-via="billyinn93" data-counturl="http://billy-inn.github.io/blog/2017/09/27/esl-chapter-3/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2017/09/01/esl-chapter-2/" title="Previous Post: [Notes on Mathematics for ESL] Chapter 2: Overview of Supervised Learning">&laquo; [Notes on Mathematics for ESL] Chapter 2: Overview of Supervised Learning</a>
      
      
        <a class="basic-alignment right" href="/blog/2017/10/15/esl-chapter-4/" title="Next Post: [Notes on Mathematics for ESL] Chapter 4: Linear Methods for Classification">[Notes on Mathematics for ESL] Chapter 4: Linear Methods for Classification &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  <!--
    <section>
	<ul id="about" class="nav nav-list">
		<li class="nav-header">About Me</a></li>
			<p>A researcher and engineer passionate about Machine Learning and Natural Language Processing</p>
      <p>A calculated speculator seeking for risk and reward asymmetries, and a value investor sticking to the margin of safety.</p>
			<p>Play piano and Nintendo Switch</p>
      <p>Read history, investment, Sci-Fi and fantasy</p>
      <p>Bodyweight fitness, hiking and skiing</p>
			<p>Fan of Portland Trail Blazers, Former SNH48 member Kiku and Blackpink member Ros&eacute</p>
      <p>Toronto, Edmonton, Beijing and Suzhou</p>
      <p><strong>Opinions posted in this blog are my own!</strong></p>
	</ul>
</section>
<section>
  <h1>Latest Tweets</h1>
  <a class="twitter-timeline" data-chrome="nofooter transparent noheader" data-tweet-limit="5" href="https://twitter.com/billy_nlp">Tweets by billy_nlp</a> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
  <a class="twitter-follow-button" href="https://twitter.com/billy_nlp">Follow @billy_nlp</a>
</section>
<section class="well">
 <ul id="categories_list" class="nav nav-list">
	<li class="nav-header">Categories</li>
	  <li id="categories"><li class='category'><a href='/blog/categories/life/'>life (3)</a></li>
<li class='category'><a href='/blog/categories/machine-learning/'>machine_learning (11)</a></li>
<li class='category'><a href='/blog/categories/nlp/'>nlp (1)</a></li>
<li class='category'><a href='/blog/categories/optimization/'>optimization (5)</a></li>
<li class='category'><a href='/blog/categories/pgm/'>pgm (1)</a></li>
<li class='category'><a href='/blog/categories/project/'>project (1)</a></li>
<li class='category'><a href='/blog/categories/reading/'>reading (2)</a></li>
<li class='category'><a href='/blog/categories/reinforcement-learning/'>reinforcement_learning (4)</a></li>
<li class='category'><a href='/blog/categories/statistics/'>statistics (3)</a></li>
</li>
 </ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models/">Shortcut Learning Hypothesis of Modern Language Models</a>
      </li>
    
      <li class="post">
        <a href="/blog/2022/01/06/navigate-through-the-current-ai-job-market-a-retrospect/">Navigate Through the Current AI Job Market: A Retrospect</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/13/convex-optimization-5/">Notes on Convex Optimization (5): Newton's Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/05/convex-optimization-4/">Notes on Convex Optimization (4): Gradient Descent Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/">Notes on Convex Optimization (3): Unconstrained Minimization Problems</a>
      </li>
    
  </ul>
</section>
<section class="well">
  <ul id="license" class="nav nav-list">
    <li class="nav-header">License</li>
<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.
	</li>
  </ul>
</section>

    -->
  <section>
	<ul id="about" class="nav nav-list">
		<li class="nav-header">About Me</a></li>
			<p>A researcher and engineer passionate about Machine Learning and Natural Language Processing</p>
      <p>A calculated speculator seeking for risk and reward asymmetries, and a value investor sticking to the margin of safety.</p>
			<p>Play piano and Nintendo Switch</p>
      <p>Read history, investment, Sci-Fi and fantasy</p>
      <p>Bodyweight fitness, hiking and skiing</p>
			<p>Fan of Portland Trail Blazers, Former SNH48 member Kiku and Blackpink member Ros&eacute</p>
      <p>Toronto, Edmonton, Beijing and Suzhou</p>
      <p><strong>Opinions posted in this blog are my own!</strong></p>
	</ul>
</section>
<section>
  <h1>Latest Tweets</h1>
  <a class="twitter-timeline" data-chrome="nofooter transparent noheader" data-tweet-limit="5" href="https://twitter.com/billy_nlp">Tweets by billy_nlp</a> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
  <a class="twitter-follow-button" href="https://twitter.com/billy_nlp">Follow @billy_nlp</a>
</section>
<section class="well">
 <ul id="categories_list" class="nav nav-list">
	<li class="nav-header">Categories</li>
	  <li id="categories"><li class='category'><a href='/blog/categories/life/'>life (3)</a></li>
<li class='category'><a href='/blog/categories/machine-learning/'>machine_learning (11)</a></li>
<li class='category'><a href='/blog/categories/nlp/'>nlp (1)</a></li>
<li class='category'><a href='/blog/categories/optimization/'>optimization (5)</a></li>
<li class='category'><a href='/blog/categories/pgm/'>pgm (1)</a></li>
<li class='category'><a href='/blog/categories/project/'>project (1)</a></li>
<li class='category'><a href='/blog/categories/reading/'>reading (2)</a></li>
<li class='category'><a href='/blog/categories/reinforcement-learning/'>reinforcement_learning (4)</a></li>
<li class='category'><a href='/blog/categories/statistics/'>statistics (3)</a></li>
</li>
 </ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2022/03/21/shortcut-learning-hypothesis-of-modern-language-models/">Shortcut Learning Hypothesis of Modern Language Models</a>
      </li>
    
      <li class="post">
        <a href="/blog/2022/01/06/navigate-through-the-current-ai-job-market-a-retrospect/">Navigate Through the Current AI Job Market: A Retrospect</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/13/convex-optimization-5/">Notes on Convex Optimization (5): Newton's Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/11/05/convex-optimization-4/">Notes on Convex Optimization (4): Gradient Descent Method</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/">Notes on Convex Optimization (3): Unconstrained Minimization Problems</a>
      </li>
    
  </ul>
</section>
<section class="well">
  <ul id="license" class="nav nav-list">
    <li class="nav-header">License</li>
<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.
	</li>
  </ul>
</section>

</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2022 - Peng (Billy) Xu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'billy-inn';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://billy-inn.github.io/blog/2017/09/27/esl-chapter-3/';
        var disqus_url = 'http://billy-inn.github.io/blog/2017/09/27/esl-chapter-3/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
