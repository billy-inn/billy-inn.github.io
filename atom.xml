<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[KRIS]]></title>
  <link href="http://billy-inn.github.io/atom.xml" rel="self"/>
  <link href="http://billy-inn.github.io/"/>
  <updated>2016-02-18T16:16:20-07:00</updated>
  <id>http://billy-inn.github.io/</id>
  <author>
    <name><![CDATA[Peng Xu]]></name>
    <email><![CDATA[bly930725@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Trace of My Study on Machine Learning]]></title>
    <link href="http://billy-inn.github.io/blog/2015/12/27/trace-of-my-study-on-machine-learning/"/>
    <updated>2015-12-27T17:38:14-07:00</updated>
    <id>http://billy-inn.github.io/blog/2015/12/27/trace-of-my-study-on-machine-learning</id>
    <content type="html"><![CDATA[<p>This blog will record the timeline, resources, projects along the way of my study on machine learning since 2015. And I will keep updating this blog.</p>

<h3 id="mathematics">Mathematics</h3>

<ul>
  <li>Basics: better to review before real study of ML
    <ul>
      <li>Calculus</li>
      <li>Linear Algebra
        <ul>
          <li><a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a>: a nice textbook in linear algebra.</li>
          <li><a href="https://www.youtube.com/watch?v=ZK3O402wf1c&amp;list=PLE7DDD91010BC51F8">MIT 18.06</a>: Course of MIT in linear algebra (<strong>Completed</strong> in <em>15.12</em>).</li>
        </ul>
      </li>
      <li>Probability Theory</li>
    </ul>
  </li>
  <li>Reference
    <ul>
      <li><a href="http://www.springer.com/us/book/9783662462201">Handbook of Mathematics</a>: an amazing reference book of mathematics.</li>
    </ul>
  </li>
  <li>Statistics
    <ul>
      <li><a href="http://www.stat.cmu.edu/~larry/all-of-statistics/">All of Statistics</a>: Ongoing</li>
      <li><a href="http://www.amazon.com/Statistical-Inference-George-Casella/dp/0534243126">Statistical Inference</a>: Ongoing</li>
      <li><a href="http://www.stat.cmu.edu/~larry/=stat705/">CMU 10-705</a>: Ongoing</li>
    </ul>
  </li>
  <li>Optimization
    <ul>
      <li><a href="http://web.stanford.edu/~boyd/cvxbook/">Convex Optimization</a>: Ongoing</li>
      <li><a href="http://stanford.edu/class/ee364a/index.html">Stanford EE364a</a>: Ongoing</li>
    </ul>
  </li>
</ul>

<!--more-->

<h3 id="statistical-machine-learning">Statistical Machine Learning</h3>

<ul>
  <li><a href="https://www.coursera.org/learn/machine-learning">MOOC: Machine Learning</a>: Introduction to machine learning. (<a href="http://billy-inn.github.io/certificates/ml.pdf">Certificate</a>)</li>
  <li><a href="https://lagunita.stanford.edu/courses/HumanitiesandScience/StatLearning/Winter2015/info">MOOC: Statistical Learning</a>: Introduction to statistical learning and <strong>R</strong>. (<a href="http://billy-inn.github.io/certificates/sl.pdf">Certificate</a>)</li>
  <li><a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning</a>: Ongoing
    <ul>
      <li><a href="http://waxworksmath.com/Authors/G_M/Hastie/hastie.html">Manual</a>: Figure out the mathematical details.</li>
      <li><a href="https://cran.r-project.org/web/packages/ElemStatLearn/index.html">R package “ElemStatLearn”</a>: Contain data sets, functions and examples from the book.</li>
      <li><a href="https://github.com/billy-inn/ElemStatLearn">My code</a>: Exercises and experiment reimplementations in <strong>R</strong>.</li>
    </ul>
  </li>
  <li>Probabilistic Graphical Models:
    <ul>
      <li><a href="https://uofa.ualberta.ca/computing-science/graduate-studies/course-directory/courses/probabilistic-graphical-models">UAlberta CMPUT 659</a>: Graduate Course in University of Alberta based on Coursera Course instructed by <a href="http://ai.stanford.edu/users/koller/">Koller</a></li>
      <li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-15/lecture.html">CMU 10-708</a>: Ongoing</li>
      <li><a href="http://pgm.stanford.edu/">Probabilistic Graphical Models: Principles and Techniques</a>: Ongoing</li>
    </ul>
  </li>
</ul>

<h3 id="deep-learning">Deep Learning</h3>

<ul>
  <li><a href="http://neuralnetworksanddeeplearning.com/index.html">Neural Network and Deep Learning</a>: Nice online tutorial for beginners in deep learning.</li>
  <li><a href="http://cs224d.stanford.edu/">Stanford CS224d</a>: Deep Learning for Natural Language Processing. A nice intro to Word2Vec and its applications.</li>
  <li><a href="http://vision.stanford.edu/teaching/cs231n/index.html">Stanford CS231n</a>: Convolutional Neural Networks for Visual Recognition.</li>
  <li><a href="https://github.com/dmlc/mshadow">MShadow</a></li>
  <li><a href="https://github.com/dmlc/mxnet">Mxnet</a></li>
</ul>

<h3 id="large-scale-machine-learning">Large Scale Machine Learning</h3>

<ul>
  <li><a href="https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/info">Introduction to Big Data with Apache Spark</a>: Introduction to Spark in <strong>Python</strong>. (<a href="http://billy-inn.github.io/certificates/spark.pdf">Certificate</a>)</li>
  <li><a href="https://courses.edx.org/courses/BerkeleyX/CS190.1x/1T2015/info">Scalable Machine Learning</a>: Explore deeper usage of Spark on large scale machine learning. (<a href="http://billy-inn.github.io/certificates/scalableML.pdf">Certificate</a>)</li>
</ul>

<h3 id="data-mining">Data Mining</h3>

<ul>
  <li><a href="https://www.coursera.org/course/mmds">MOOC: Mining of Massive Datasets</a>: Introduction to a lot data mining techniques. (<a href="http://billy-inn.github.io/certificates/mmds.pdf">Certificate</a>)</li>
</ul>

<h3 id="miscellanea">Miscellanea</h3>

<ul>
  <li><strong>Python</strong> &amp; <strong>R</strong>: For experiment and plotting.</li>
  <li><strong>C++</strong> &amp; <strong>Scala</strong>: For development.</li>
  <li><a href="https://www.vagrantup.com/">Vagrant</a>: Create and configure lightweight, reproducible, and portable development environments. Very Useful tool!</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning Basics(1): Linear Regression]]></title>
    <link href="http://billy-inn.github.io/blog/2015/12/23/machine-learning-basics-1-linear-regression/"/>
    <updated>2015-12-23T16:56:20-07:00</updated>
    <id>http://billy-inn.github.io/blog/2015/12/23/machine-learning-basics-1-linear-regression</id>
    <content type="html"><![CDATA[<p>As a beginner in machine learning, I plan to sketch out my learning process. And it will be my first post in this series.</p>

<h3 id="definition">1. Definition</h3>

<p>We have an input vector $X^T=(X_1, X_2, \dots, X_p)$ and want to predict a real-valued output $Y$. The linear regression model has the form</p>

<script type="math/tex; mode=display">f(X)=\beta_0 + \sum_{j=1}^pX_j\beta_j.\quad(1.1)</script>

<p>Here the $\beta_j$’s are unknown parameters or coefficients.</p>

<!--more-->

<h3 id="least-squares">2. Least Squares</h3>

<p>Typically we have a set of training data $(x_1,y_1)\dots(x_N,y_N)$ from which to estimate the parameters $\beta$. The most popular estimation method is least squares, in which we pick the coefficients $\beta=(
\beta_0, \beta_1, \dots, \beta_p)^T$ to minimize the residual sum of squares (RSS)</p>

<script type="math/tex; mode=display">RSS(\beta)=\sum_{i=1}^N(y_i-f(x_i))^2.\quad(2.1)</script>

<p>How do we minimize $(2.1)$? This is an important question with a beautiful answer. The best $\beta$ can be found by geometry or algebra or calculus. Let’s explain the three approaches one by one.</p>

<h4 id="calculus-interpretation">Calculus Interpretation</h4>

<p>Denote by $\mathbf{X}$ the $N\times (p+1)$ matrix with each row an input vector (with a 1 in the first position). We can rewrite $(2.1)$ in the matrix form as</p>

<script type="math/tex; mode=display">RSS(\beta)=(\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta).\quad(2.2)</script>

<p>This is a quadratic function in the $p+1$ parameters. Differentating with respect to $\beta$ we obtain</p>

<script type="math/tex; mode=display">\frac{\partial RSS}{\partial\beta}=-2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta).\quad(2.3)</script>

<p>In order to get the minimum RSS, we set the first derivative to zero</p>

<script type="math/tex; mode=display">\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta)=0\quad(2.4)</script>

<p>to obtain the unique solution</p>

<script type="math/tex; mode=display">\hat\beta = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.\quad(2.5)</script>

<p>The fitted values are</p>

<script type="math/tex; mode=display">\mathbf{\hat y}=\mathbf{X}\hat\beta=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.\quad(2.6)</script>

<p>One thing which should be noticed is that it’s wrong to split $(\mathbf{X}^T\mathbf{X})^{-1}$ into $\mathbf{X}^{-1}$ times $\mathbf{X}$ and cancel the terms before $\mathbf{y}$ in $(2.6)$. The matrix $\mathbf{X}$ is rectangualr and has no inverse matrix. And if $\mathbf{X}$ is square and invertible, $\mathbf{\hat y}$ is exactly $\mathbf{y}$ just as $(2.6)$ shows.</p>

<p>Someone may be confused by how to get $(2.3)$ by $(2.2)$ just as I once experienced. Actually, the easiest way to figure it out is to differentiate with respect to each $\beta_j$ respectively</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}\frac{\partial RSS}{\partial\beta_j}&=\frac{\partial}{\partial\beta_j}\sum_{i=1}^N\left(y_i-\sum_{j=0}^px_{ij}\beta_j\right)^2 \\
&=-2\sum_{i=1}^Nx_{ij}\left(y_i-\sum_{j=0}^px_{ij}\beta_j\right). \\
\end{split} %]]&gt;</script>

<p>Then, we put all the derivatives together and rewrite it in matrix form to get $(2.3)$.</p>

<h4 id="geometry-interpretation">Geometry Interpretation</h4>

<p><img src="http://billy-inn.github.io/images/LinearRegression1.png" alt="Alt text" /></p>

<p>The picture above shows a direct gemotrical representation of the least squares estimate with two predictors. It’s convenient to extend this to the case with more predictors. We denote the column vectors of $X$ by $\mathbf{x}_0,\mathbf{x}_1,\dots,\mathbf{x}_p$ with $\mathbf{x}_0 \equiv 1$. These vectors span a subspace of $\mathbb{R}^N$, also referred to as the column space of $\mathbf{X}$. We minimize $RSS(\beta)=\lVert\mathbf{y}-\mathbf{X}\beta\rVert^2$ by choosing $\hat \beta$ so that the residual vector $\mathbf{y}-\mathbf{\hat y}$ is orthogonal to this subspace. This orthogonality is expressed in $(2.4)$, and the resulting estimate $\mathbf{\hat y}$ is hence the orthogonal projection of $\mathbf y$ onto this subspace.</p>

<h4 id="algebra-interpretation">Algebra Interpretation</h4>

<p>In the world of linear algebra, we will also view our problem from a perspective of algebra. Now we reformulate the problem as to solve the linear equations system $\mathbf{X}\beta=\mathbf{y}$. If you are somewhat familar with linear algebra, you can try to use elimination to solve it. Unfortunately, it has no solution typically due to there are more equations than unknowns ($n$ is larger than $p$).</p>

<p>Instead, we hope that the length of error $\mathbf{e}=\mathbf{y}-\mathbf{X}\hat\beta$ is as small as possible. And the corresponding $\hat\beta$ is a least squares solution.</p>

<p>When $\mathbf{X}\beta=\mathbf{y}$ has no solution, we can multiply by $\mathbf{X}^T$ and solve $\mathbf{X}^T\mathbf{X}\beta=\mathbf{X}^T\mathbf{y}$. And it yields the solution from calculus exactly. In the language of algebra, we can split the $\mathbf{y}$ into two parts as $\mathbf{p}+\mathbf{e}$. The part in the column space is $\mathbf{p}$. The perpendicular part in the nullspace of $\mathbf{X}^T$ is $\mathbf{e}$. There is an equation we cannot solve ($\mathbf{X}\beta=\mathbf{y}$). There is an equation $\mathbf{X}\hat\beta=\mathbf{p}$ we do solve. And the solution to $\mathbf{X}\hat\beta=\mathbf{p}$ leaves the least possible error. How do we choose $\mathbf{p}$ in the column space? Just as the geometry part tells us, the projection of $\mathbf{y}$ onto the column space!</p>

<p>Ok, now the three interpretations come together and form a complete picture for the least squares.</p>

<p>Wait a minute, there is still one more question. It might happen that that the columns of $\mathbf{X}$ are not linearly independent, so that $\mathbf{X}$ is not of full rank. Then $\mathbf{X}^T\mathbf{X}$ is singular and the least squares coefficients $\hat\beta$ are not uniquely defined. However, the fitted values $\mathbf{\hat y}=\mathbf{X}\hat\beta$ are still the projection of $\mathbf{y}$ onto the column space of $\mathbf{X}$; there is just more than one way to express that projections in terms of the column vectors of $\mathbf{X}$.</p>

<h3 id="implementations">3. Implementations</h3>

<p>The implementations is extremely easy in a language like <strong>R</strong>, even without specific packages. The core code is shown below:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="r"><span class="line"><span class="kn">library</span><span class="p">(</span>MASS<span class="p">)</span>
</span><span class="line">betaHat <span class="o">&lt;-</span> ginv<span class="p">(</span><span class="kp">t</span><span class="p">(</span>X<span class="p">)</span> <span class="o">%*%</span> X<span class="p">)</span> <span class="o">%*%</span> <span class="kp">t</span><span class="p">(</span>X<span class="p">)</span> <span class="o">%*%</span> y
</span><span class="line">yhat <span class="o">&lt;-</span> X <span class="o">%*%</span> betaHat
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>You can also use packages like <em>lm</em> in <strong>R</strong>. It provides not only $\hat\beta$ but also a lot sophisticated coefficients.</p>

<h3 id="taste-on-statistics">4. Taste on Statistics</h3>

<p><strong><em>To be continued</em></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[First Post: General Goals]]></title>
    <link href="http://billy-inn.github.io/blog/2015/10/27/first-post/"/>
    <updated>2015-10-27T10:24:51-06:00</updated>
    <id>http://billy-inn.github.io/blog/2015/10/27/first-post</id>
    <content type="html"><![CDATA[<p>签证总算下来了，从毕业到现在，整个人都处于一种很难形容的颓废状态中。昨天又一整夜没有睡，看了很多博客，看到了许多学长的奋斗经历与学术积累，深感不能再如此颓废下去了。</p>

<p>没有压力的生活往往会让人无所事事，现在的我就处于这样一个状态。为了给自己找点事做，也给自己点压力，所以花了些功夫把这个博客搭了起来。希望自己可以把这个博客维护好，坚持将自己在学习生活上的点滴记录下来，同时也可以分享一些有益的东西。</p>

<!--more-->

<p>现在离出发去加拿大还有一个多月的时间，为了一过去就可以在research入门。这段时间的目标大致如下：</p>

<ul>
  <li>Coursera: Introduction to Natural Language Processing</li>
  <li>Mathemathics:
    <ul>
      <li>Revise Linear Algerba</li>
      <li><em>Statistical Inference</em></li>
      <li><em>Convex Optimization</em> (if possible)</li>
    </ul>
  </li>
  <li>Machine Learning:
    <ul>
      <li><em>Elements of Statistical Learning</em></li>
    </ul>
  </li>
  <li>Life:
    <ul>
      <li>Learn basic cooking</li>
      <li>Daily physical exercise</li>
    </ul>
  </li>
  <li>Articles:
    <ul>
      <li>Notes on Linear Algerba</li>
      <li>Notes on <em>Elements of Statistical Learning</em></li>
      <li>Notes on NLP (if necessary)</li>
    </ul>
  </li>
  <li>Programming:
    <ul>
      <li>Re-implement experiments in <em>Elements of Statistical Learning</em> via <strong>R</strong></li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
</feed>
