<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[KRIS]]></title>
  <link href="http://billy-inn.github.io/atom.xml" rel="self"/>
  <link href="http://billy-inn.github.io/"/>
  <updated>2016-06-05T21:58:23-06:00</updated>
  <id>http://billy-inn.github.io/</id>
  <author>
    <name><![CDATA[Peng Xu]]></name>
    <email><![CDATA[bly930725@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Probabilistic Graphical Models (1): Representation]]></title>
    <link href="http://billy-inn.github.io/blog/2016/04/28/probabilistic-graphical-models-1-representation/"/>
    <updated>2016-04-28T16:17:08-06:00</updated>
    <id>http://billy-inn.github.io/blog/2016/04/28/probabilistic-graphical-models-1-representation</id>
    <content type="html"><![CDATA[<p>What is probabilistic graphical model (PGM)? Forget about some high-level and complicated sentences on textbook or wikipedia. From my perspective, it’s all about graphs, distributions and the connections between them.</p>

<p>There are three aspects to a graphical model:</p>

<ul>
  <li>The graph</li>
  <li>A model of the data based on the graph</li>
  <li>The data itself</li>
</ul>

<p>Actually, the data is generated by the underlying distribution and the model is the connection between the two. Here comes three fundamental issues:</p>

<ul>
  <li><strong>Representation</strong>: How to represent the three aspects mentioned above succinctly?</li>
  <li><strong>Inference</strong>: How do I answser questions/queries according to my model and the given data? For example, the probability of a certain variable given some observations $P(X_i \vert \mathcal{D})$.</li>
  <li><strong>Learning</strong>: What model is “right” for my data? We may want to “learn” the parameters of the model, or the model itself or even the topology of the graph from the data.</li>
</ul>

<!--more-->

<p>But, why we need to use PGM? Consider a multivariate distribution on $8$ binary-valued variables $X_1,\dots, X_8$. Such a distribution contains $2^8$ configurations and the specification of the joint distribution would require $2^8-1$ numbers. In pratical, quite a bit of these configurations are unnecessary. In this form, inference is also expensive as it would, in general, require summing over an exponential number of configurations of unobserved variables. Learning is also problematic in this case because huge amounts of data are needed to accurately learn probabilities, especially of rare events. A solution to this problem is to use conditional independencies. Conditional independencies are statements of the form “$X$ is independent of $Y$ given $Z$” or $X \perp Y \vert Z$. If all the variables are independent, we can write:</p>

<script type="math/tex; mode=display">P(X_1,\dots,X_8)=P(X_1)P(X_2)P(X_3)\dots P(X_8)</script>

<p>This representation reduces parameter requirement from exponential to linear in the number of variables. But this representation is not rich enough to capture all possibilities. PGM try to capture middle ground by using conditional independencies known from the domain. Graph representation captures these conditional independencies compactly.</p>

<p><img src="http://billy-inn.github.io/images/PGM1.1.png" alt="Alt text" /></p>

<p>So, what is a PGM after all? It’s a smart way to specify exponentially-large probability distributions without paying an exponential cost, and at the same time endow the distributions with structured semantics. More formally, it refers to a family of distributions on a set of random variables that are compatible with all the probabiliistic independence propositions encoded by a graph that connects these variables.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GSoC 2016: Inferring infobox template class mappings from Wikipedia and WikiData]]></title>
    <link href="http://billy-inn.github.io/blog/2016/04/26/gsoc-2016-inferring-infobox-template-class-mappings-from-wikipedia-and-wikidata/"/>
    <updated>2016-04-26T15:51:27-06:00</updated>
    <id>http://billy-inn.github.io/blog/2016/04/26/gsoc-2016-inferring-infobox-template-class-mappings-from-wikipedia-and-wikidata</id>
    <content type="html"><![CDATA[<p><em>This page is the public project page and will be updated every week.</em></p>

<h3 id="project-description">Project Description</h3>

<p>There are many infoboxes on wikipedia. Here is a example about football box:</p>

<p><img src="http://billy-inn.github.io/images/GSoC1.png" alt="Alt text" /></p>

<p>As seen, every infobox has some properties. Actually, every infobox follows a certain template. In my project, the goal is to find mappings between the classes (eg. <code>dbo:Person</code>, <code>dbo:City</code>) in the DBpedia ontology and infobox templates on pages of Wikipedia resources using techniques of machine learning.</p>

<p>There are lots of infobox mappings available for a few languages, but not as many for other languages. In order to infer mappings for all the languages, cross-lingual knowledge validation should also be considered.</p>

<p>The main output of the project will be a list of new high-quality infobox-class mappings.</p>

<!--more-->

<h3 id="weekly-progess">Weekly Progess</h3>

<p><strong>Week 1 (5.8-5.14)</strong></p>

<ul>
  <li>First meeting with my mentor</li>
  <li>Create the public page for the project</li>
  <li>Create the google doc for the project</li>
</ul>

<p><strong>Week 2 (5.15-5.21)</strong></p>

<ul>
  <li>Get the <a href="http://mappings.dbpedia.org/server/mappings/en/pages/rdf/all">existing mappings</a></li>
  <li>Get the information about all templates: <a href="https://github.com/dbpedia/extraction-framework/tree/master/server/src/main/statistics">https://github.com/dbpedia/extraction-framework/tree/master/server/src/main/statistics</a></li>
  <li>Parse and clean the data</li>
</ul>

<p><strong>Week 3 (5.22-5.28)</strong></p>

<ul>
  <li>Figure out the information we have:
    <ul>
      <li>1) Existing mappings, we have manual information that a template in lang $X$ is mapped to class $Y$</li>
      <li>2) Inter-language links between templates, e.g. template $X_1$ in lang $X$ is mapped to class $Y$ and there is a link from this template to templates in other languages. This gives a high probability that the equivalent templates in other languages should be mapped to the same class $Y$.</li>
      <li>3) Links between articles in different languages and templates each article uses, this way, when (2) is not always true we can find which templates are used for the same concepts.</li>
      <li>4) Most wikidata articles have a type assigned, using this information we have a variation of  metric (3) but with manual types assigned.</li>
    </ul>
  </li>
  <li>Process the information (1) - (3) described above and obtain some prelimenary results.</li>
</ul>

<p><strong>Week 4 (5.29-6.4)</strong></p>

<ul>
  <li>Propose a baseline approach: Given a template classified as an infobox, the approach is instance-based and exploits the already mapped articles and their cross-language links to the articles that contain the template to be mapped. This approach is summaried in the below figure.</li>
</ul>

<p><img src="http://billy-inn.github.io/images/GSoC.png" alt="Alt text" /></p>

<ul>
  <li>Experiments:
    <ul>
      <li>Based on existing mappings in English, to create mappings for Chinese.</li>
      <li>Evaluations: try this approach on some other languages which have some existing mappings. (In progress)</li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Trace of My Study on Machine Learning]]></title>
    <link href="http://billy-inn.github.io/blog/2015/12/27/trace-of-my-study-on-machine-learning/"/>
    <updated>2015-12-27T17:38:14-07:00</updated>
    <id>http://billy-inn.github.io/blog/2015/12/27/trace-of-my-study-on-machine-learning</id>
    <content type="html"><![CDATA[<p>This blog will record the timeline, resources, projects along the way of my study on machine learning since 2015. And I will keep updating this blog.</p>

<h3 id="mathematics">Mathematics</h3>

<ul>
  <li>Basics: better to review before real study of ML
    <ul>
      <li>Calculus</li>
      <li>Linear Algebra
        <ul>
          <li><a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a>: a nice textbook in linear algebra.</li>
          <li><a href="https://www.youtube.com/watch?v=ZK3O402wf1c&amp;list=PLE7DDD91010BC51F8">MIT 18.06</a>: Course of MIT in linear algebra (<strong>Completed</strong> in <em>15.12</em>).</li>
        </ul>
      </li>
      <li>Probability Theory</li>
    </ul>
  </li>
  <li>Reference
    <ul>
      <li><a href="http://www.springer.com/us/book/9783662462201">Handbook of Mathematics</a>: an amazing reference book of mathematics.</li>
    </ul>
  </li>
  <li>Statistics
    <ul>
      <li><a href="http://www.stat.cmu.edu/~larry/all-of-statistics/">All of Statistics</a>: Ongoing</li>
      <li><a href="http://www.amazon.com/Statistical-Inference-George-Casella/dp/0534243126">Statistical Inference</a>: Ongoing</li>
      <li><a href="http://www.stat.cmu.edu/~larry/=stat705/">CMU 10-705</a>: Ongoing</li>
    </ul>
  </li>
  <li>Optimization
    <ul>
      <li><a href="http://web.stanford.edu/~boyd/cvxbook/">Convex Optimization</a>: Ongoing</li>
      <li><a href="http://stanford.edu/class/ee364a/index.html">Stanford EE364a</a>: Ongoing</li>
    </ul>
  </li>
</ul>

<!--more-->

<h3 id="statistical-machine-learning">Statistical Machine Learning</h3>

<ul>
  <li><a href="https://www.coursera.org/learn/machine-learning">MOOC: Machine Learning</a>: Introduction to machine learning. (<a href="http://billy-inn.github.io/certificates/ml.pdf">Certificate</a>)</li>
  <li><a href="https://lagunita.stanford.edu/courses/HumanitiesandScience/StatLearning/Winter2015/info">MOOC: Statistical Learning</a>: Introduction to statistical learning and <strong>R</strong>. (<a href="http://billy-inn.github.io/certificates/sl.pdf">Certificate</a>)</li>
  <li><a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning</a>: Ongoing
    <ul>
      <li><a href="http://waxworksmath.com/Authors/G_M/Hastie/hastie.html">Manual</a>: Figure out the mathematical details.</li>
      <li><a href="https://cran.r-project.org/web/packages/ElemStatLearn/index.html">R package “ElemStatLearn”</a>: Contain data sets, functions and examples from the book.</li>
      <li><a href="https://github.com/billy-inn/ElemStatLearn">My code</a>: Exercises and experiment reimplementations in <strong>R</strong>.</li>
    </ul>
  </li>
  <li>Probabilistic Graphical Models:
    <ul>
      <li><a href="https://uofa.ualberta.ca/computing-science/graduate-studies/course-directory/courses/probabilistic-graphical-models">UAlberta CMPUT 659</a>: Graduate Course in University of Alberta based on Coursera Course instructed by <a href="http://ai.stanford.edu/users/koller/">Koller</a> (<strong>Completed</strong> in <em>16.4</em>).</li>
      <li><a href="http://www.cs.cmu.edu/~epxing/Class/10708-15/lecture.html">CMU 10-708</a>: Ongoing</li>
      <li><a href="http://pgm.stanford.edu/">Probabilistic Graphical Models: Principles and Techniques</a>: Classic Textbook for Probabilistic Graphical Models. I’ve read a few chapters of it and gave up. This book is somewhat too mathematical and I will not recommend this book for beginners in PGM.</li>
      <li><a href="https://github.com/jmschrei/pomegranate">Pomegranate</a>: A Python package for PGM. Nice tutorials for beginners in PGM!</li>
    </ul>
  </li>
</ul>

<h3 id="deep-learning">Deep Learning</h3>

<ul>
  <li><a href="http://neuralnetworksanddeeplearning.com/index.html">Neural Network and Deep Learning</a>: Nice online tutorial for beginners in deep learning.</li>
  <li><a href="http://cs224d.stanford.edu/">Stanford CS224d</a>: Deep Learning for Natural Language Processing. A nice intro to Word2Vec and its applications.</li>
  <li><a href="http://vision.stanford.edu/teaching/cs231n/index.html">Stanford CS231n</a>: Convolutional Neural Networks for Visual Recognition.</li>
  <li><a href="https://github.com/dmlc/mshadow">MShadow</a></li>
  <li><a href="https://github.com/dmlc/mxnet">Mxnet</a></li>
</ul>

<h3 id="large-scale-machine-learning">Large Scale Machine Learning</h3>

<ul>
  <li><a href="https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/info">Introduction to Big Data with Apache Spark</a>: Introduction to Spark in <strong>Python</strong>. (<a href="http://billy-inn.github.io/certificates/spark.pdf">Certificate</a>)</li>
  <li><a href="https://courses.edx.org/courses/BerkeleyX/CS190.1x/1T2015/info">Scalable Machine Learning</a>: Explore deeper usage of Spark on large scale machine learning. (<a href="http://billy-inn.github.io/certificates/scalableML.pdf">Certificate</a>)</li>
</ul>

<h3 id="data-mining">Data Mining</h3>

<ul>
  <li><a href="https://www.coursera.org/course/mmds">MOOC: Mining of Massive Datasets</a>: Introduction to a lot data mining techniques. (<a href="http://billy-inn.github.io/certificates/mmds.pdf">Certificate</a>)</li>
</ul>

<h3 id="ml-competitions">ML Competitions</h3>

<ul>
  <li><a href="https://www.kaggle.com/c/home-depot-product-search-relevance">Kaggle: Home Depot Product Search Relevance</a>: Top 10%.</li>
</ul>

<h3 id="miscellanea">Miscellanea</h3>

<ul>
  <li><strong>Python</strong> &amp; <strong>R</strong>: For experiment and plotting.</li>
  <li><strong>C++</strong> &amp; <strong>Scala</strong>: For development.</li>
  <li><a href="https://www.vagrantup.com/">Vagrant</a>: Create and configure lightweight, reproducible, and portable development environments. Very Useful tool!</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning Basics(1): Linear Regression]]></title>
    <link href="http://billy-inn.github.io/blog/2015/12/23/machine-learning-basics-1-linear-regression/"/>
    <updated>2015-12-23T16:56:20-07:00</updated>
    <id>http://billy-inn.github.io/blog/2015/12/23/machine-learning-basics-1-linear-regression</id>
    <content type="html"><![CDATA[<p>As a beginner in machine learning, I plan to sketch out my learning process. And it will be my first post in this series.</p>

<h3 id="definition">1. Definition</h3>

<p>We have an input vector $X^T=(X_1, X_2, \dots, X_p)$ and want to predict a real-valued output $Y$. The linear regression model has the form</p>

<script type="math/tex; mode=display">f(X)=\beta_0 + \sum_{j=1}^pX_j\beta_j.\quad(1.1)</script>

<p>Here the $\beta_j$’s are unknown parameters or coefficients.</p>

<!--more-->

<h3 id="least-squares">2. Least Squares</h3>

<p>Typically we have a set of training data $(x_1,y_1)\dots(x_N,y_N)$ from which to estimate the parameters $\beta$. The most popular estimation method is least squares, in which we pick the coefficients $\beta=(
\beta_0, \beta_1, \dots, \beta_p)^T$ to minimize the residual sum of squares (RSS)</p>

<script type="math/tex; mode=display">RSS(\beta)=\sum_{i=1}^N(y_i-f(x_i))^2.\quad(2.1)</script>

<p>How do we minimize $(2.1)$? This is an important question with a beautiful answer. The best $\beta$ can be found by geometry or algebra or calculus. Let’s explain the three approaches one by one.</p>

<h4 id="calculus-interpretation">Calculus Interpretation</h4>

<p>Denote by $\mathbf{X}$ the $N\times (p+1)$ matrix with each row an input vector (with a 1 in the first position). We can rewrite $(2.1)$ in the matrix form as</p>

<script type="math/tex; mode=display">RSS(\beta)=(\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta).\quad(2.2)</script>

<p>This is a quadratic function in the $p+1$ parameters. Differentating with respect to $\beta$ we obtain</p>

<script type="math/tex; mode=display">\frac{\partial RSS}{\partial\beta}=-2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta).\quad(2.3)</script>

<p>In order to get the minimum RSS, we set the first derivative to zero</p>

<script type="math/tex; mode=display">\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta)=0\quad(2.4)</script>

<p>to obtain the unique solution</p>

<script type="math/tex; mode=display">\hat\beta = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.\quad(2.5)</script>

<p>The fitted values are</p>

<script type="math/tex; mode=display">\mathbf{\hat y}=\mathbf{X}\hat\beta=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.\quad(2.6)</script>

<p>One thing which should be noticed is that it’s wrong to split $(\mathbf{X}^T\mathbf{X})^{-1}$ into $\mathbf{X}^{-1}$ times $\mathbf{X}$ and cancel the terms before $\mathbf{y}$ in $(2.6)$. The matrix $\mathbf{X}$ is rectangualr and has no inverse matrix. And if $\mathbf{X}$ is square and invertible, $\mathbf{\hat y}$ is exactly $\mathbf{y}$ just as $(2.6)$ shows.</p>

<p>Someone may be confused by how to get $(2.3)$ by $(2.2)$ just as I once experienced. Actually, the easiest way to figure it out is to differentiate with respect to each $\beta_j$ respectively</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}\frac{\partial RSS}{\partial\beta_j}&=\frac{\partial}{\partial\beta_j}\sum_{i=1}^N\left(y_i-\sum_{j=0}^px_{ij}\beta_j\right)^2 \\
&=-2\sum_{i=1}^Nx_{ij}\left(y_i-\sum_{j=0}^px_{ij}\beta_j\right). \\
\end{split} %]]&gt;</script>

<p>Then, we put all the derivatives together and rewrite it in matrix form to get $(2.3)$.</p>

<h4 id="geometry-interpretation">Geometry Interpretation</h4>

<p><img src="http://billy-inn.github.io/images/LinearRegression1.png" alt="Alt text" /></p>

<p>The picture above shows a direct gemotrical representation of the least squares estimate with two predictors. It’s convenient to extend this to the case with more predictors. We denote the column vectors of $X$ by $\mathbf{x}_0,\mathbf{x}_1,\dots,\mathbf{x}_p$ with $\mathbf{x}_0 \equiv 1$. These vectors span a subspace of $\mathbb{R}^N$, also referred to as the column space of $\mathbf{X}$. We minimize $RSS(\beta)=\lVert\mathbf{y}-\mathbf{X}\beta\rVert^2$ by choosing $\hat \beta$ so that the residual vector $\mathbf{y}-\mathbf{\hat y}$ is orthogonal to this subspace. This orthogonality is expressed in $(2.4)$, and the resulting estimate $\mathbf{\hat y}$ is hence the orthogonal projection of $\mathbf y$ onto this subspace.</p>

<h4 id="algebra-interpretation">Algebra Interpretation</h4>

<p>In the world of linear algebra, we will also view our problem from a perspective of algebra. Now we reformulate the problem as to solve the linear equations system $\mathbf{X}\beta=\mathbf{y}$. If you are somewhat familar with linear algebra, you can try to use elimination to solve it. Unfortunately, it has no solution typically due to there are more equations than unknowns ($n$ is larger than $p$).</p>

<p>Instead, we hope that the length of error $\mathbf{e}=\mathbf{y}-\mathbf{X}\hat\beta$ is as small as possible. And the corresponding $\hat\beta$ is a least squares solution.</p>

<p>When $\mathbf{X}\beta=\mathbf{y}$ has no solution, we can multiply by $\mathbf{X}^T$ and solve $\mathbf{X}^T\mathbf{X}\beta=\mathbf{X}^T\mathbf{y}$. And it yields the solution from calculus exactly. In the language of algebra, we can split the $\mathbf{y}$ into two parts as $\mathbf{p}+\mathbf{e}$. The part in the column space is $\mathbf{p}$. The perpendicular part in the nullspace of $\mathbf{X}^T$ is $\mathbf{e}$. There is an equation we cannot solve ($\mathbf{X}\beta=\mathbf{y}$). There is an equation $\mathbf{X}\hat\beta=\mathbf{p}$ we do solve. And the solution to $\mathbf{X}\hat\beta=\mathbf{p}$ leaves the least possible error. How do we choose $\mathbf{p}$ in the column space? Just as the geometry part tells us, the projection of $\mathbf{y}$ onto the column space!</p>

<p>Ok, now the three interpretations come together and form a complete picture for the least squares.</p>

<p>Wait a minute, there is still one more question. It might happen that that the columns of $\mathbf{X}$ are not linearly independent, so that $\mathbf{X}$ is not of full rank. Then $\mathbf{X}^T\mathbf{X}$ is singular and the least squares coefficients $\hat\beta$ are not uniquely defined. However, the fitted values $\mathbf{\hat y}=\mathbf{X}\hat\beta$ are still the projection of $\mathbf{y}$ onto the column space of $\mathbf{X}$; there is just more than one way to express that projections in terms of the column vectors of $\mathbf{X}$.</p>

<h3 id="implementations">3. Implementations</h3>

<p>The implementations is extremely easy in a language like <strong>R</strong>, even without specific packages. The core code is shown below:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="r"><span class="line"><span class="kn">library</span><span class="p">(</span>MASS<span class="p">)</span>
</span><span class="line">betaHat <span class="o">&lt;-</span> ginv<span class="p">(</span><span class="kp">t</span><span class="p">(</span>X<span class="p">)</span> <span class="o">%*%</span> X<span class="p">)</span> <span class="o">%*%</span> <span class="kp">t</span><span class="p">(</span>X<span class="p">)</span> <span class="o">%*%</span> y
</span><span class="line">yhat <span class="o">&lt;-</span> X <span class="o">%*%</span> betaHat
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>You can also use packages like <em>lm</em> in <strong>R</strong>. It provides not only $\hat\beta$ but also a lot sophisticated coefficients.</p>

<h3 id="taste-on-statistics">4. Taste on Statistics</h3>

<p><strong><em>To be continued</em></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[First Post: General Goals]]></title>
    <link href="http://billy-inn.github.io/blog/2015/10/27/first-post/"/>
    <updated>2015-10-27T10:24:51-06:00</updated>
    <id>http://billy-inn.github.io/blog/2015/10/27/first-post</id>
    <content type="html"><![CDATA[<p>签证总算下来了，从毕业到现在，整个人都处于一种很难形容的颓废状态中。昨天又一整夜没有睡，看了很多博客，看到了许多学长的奋斗经历与学术积累，深感不能再如此颓废下去了。</p>

<p>没有压力的生活往往会让人无所事事，现在的我就处于这样一个状态。为了给自己找点事做，也给自己点压力，所以花了些功夫把这个博客搭了起来。希望自己可以把这个博客维护好，坚持将自己在学习生活上的点滴记录下来，同时也可以分享一些有益的东西。</p>

<!--more-->

<p>现在离出发去加拿大还有一个多月的时间，为了一过去就可以在research入门。这段时间的目标大致如下：</p>

<ul>
  <li>Coursera: Introduction to Natural Language Processing</li>
  <li>Mathemathics:
    <ul>
      <li>Revise Linear Algerba</li>
      <li><em>Statistical Inference</em></li>
      <li><em>Convex Optimization</em> (if possible)</li>
    </ul>
  </li>
  <li>Machine Learning:
    <ul>
      <li><em>Elements of Statistical Learning</em></li>
    </ul>
  </li>
  <li>Life:
    <ul>
      <li>Learn basic cooking</li>
      <li>Daily physical exercise</li>
    </ul>
  </li>
  <li>Articles:
    <ul>
      <li>Notes on Linear Algerba</li>
      <li>Notes on <em>Elements of Statistical Learning</em></li>
      <li>Notes on NLP (if necessary)</li>
    </ul>
  </li>
  <li>Programming:
    <ul>
      <li>Re-implement experiments in <em>Elements of Statistical Learning</em> via <strong>R</strong></li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
</feed>
