<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Billy Inn in Wonderland]]></title>
  <link href="http://billy-inn.github.io/atom.xml" rel="self"/>
  <link href="http://billy-inn.github.io/"/>
  <updated>2018-11-13T23:27:25-05:00</updated>
  <id>http://billy-inn.github.io/</id>
  <author>
    <name><![CDATA[Peng Xu]]></name>
    <email><![CDATA[bly930725@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Notes on Convex Optimization (5): Newton's Method]]></title>
    <link href="http://billy-inn.github.io/blog/2018/11/13/convex-optimization-5/"/>
    <updated>2018-11-13T23:25:12-05:00</updated>
    <id>http://billy-inn.github.io/blog/2018/11/13/convex-optimization-5</id>
    <content type="html"><![CDATA[<p>For $x\in\mathbf{dom}\ f$, the vector</p>

<script type="math/tex; mode=display">\Delta x_{nt}=-\nabla^2 f(x)^{-1}\nabla f(x)</script>

<p>is called the <em>Newton step</em> (for $f$, at $x$).</p>

<h4 id="minimizer-of-second-order-approximation">Minimizer of second-order approximation</h4>

<p>The second-order Taylor approximation $\hat f$ of $f$ at $x$ is</p>

<p>\begin{equation}
\hat f(x+v) = f(x) + \nabla f(x)^T v + \frac12 v^T \nabla^2 f(x) v.
\tag{1} \label{eq:1}
\end{equation}</p>

<p>which is a convex quadratic function of $v$, and is minimized when $v=\Delta x_{nt}$. Thus, the Newton step $\Delta x_{nt}$ is what should be added to the point $x$ to minimize the second-order approximation of $f$ at $x$.</p>

<!--more-->

<h4 id="steepest-descent-direction-in-hessian-norm">Steepest descent direction in Hessian norm</h4>

<p>The Newton step is also the steepest descent direction at $x$, for the quadratic norm defined by the Hessian $\nabla^2 f(x)$, <em>i.e.</em>,</p>

<script type="math/tex; mode=display">\lVert u \rVert_{\nabla^2f(x)}=(u^T\nabla^2f(x)u)^{1/2}.</script>

<h4 id="solution-of-linearized-optimality-condition">Solution of linearized optimality condition</h4>

<p>If we linearize the optimality condition $\nabla f(x^*)=0$ near $x$ we obtain</p>

<script type="math/tex; mode=display">\nabla f(x+v) \approx \nabla f(x) + \nabla^2f(x)v = 0</script>

<p>which is a linear equation in $v$, with solution $v=\Delta x_{nt}$. So the Newton step $\Delta x_{nt}$ is what must be added to $x$ so that the linearized optimality condition holds.</p>

<h4 id="affine-invariance-of-the-newton-step">Affine invariance of the Newton step</h4>

<p>An important feature of the Newton step is that it is independent of linear changes of coordinates. Suppose $T\in \mathbf{R}^{n \times n}$ is nonsingular, and define $\bar f(y)=f(Ty)$. Then we have</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\nabla\bar f(y) & = \nabla f(Ty)\\
&=\frac{\partial f(Ty)}{\partial(Ty)}\frac{\partial(Ty)}{y} \\
&=T^T\nabla f(x),
\end{split} %]]&gt;</script>

<p>where $x=Ty$, likewise we have $\nabla^2 \bar f(y) = T^T\nabla^2f(x)T$. The Newton step for $\bar f$ at $y$ is therefore</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\Delta y_{nt} &= -(T^T\nabla^2f(x)T)^{-1}(T^T\nabla f(x)) \\
&= -T^{-1} \nabla^2 f(x)^{-1} \nabla f(x) \\
&= T^{-1}\Delta x_{nt},
\end{split} %]]&gt;</script>

<p>where $\Delta x_{nt}$ is the Newton step for $f$ at $x$. Hence the Newton steps of $f$ and $\bar f$ are related by the same linear transformation, and</p>

<script type="math/tex; mode=display">x + \Delta x_{nt} = T(y + \Delta y_{nt}).</script>

<h4 id="the-newton-decrement">The Newton decrement</h4>

<p>The quantity</p>

<script type="math/tex; mode=display">\lambda(x)=(\nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x))^{1/2}</script>

<p>is called the <em>Newton decrement</em> at $x$. We can relate the Newton decrement to the quantity $f(x) - \inf_y \hat f(y)$, where $\hat y$ is the second-order approximation of $f$ at $x$:</p>

<script type="math/tex; mode=display">f(x) - \inf_y \hat f(y)=f(x)-\hat f(x + \Delta x_{nt})=\frac12\lambda(x)^2.</script>

<p>We can also express the Newton decrement as</p>

<script type="math/tex; mode=display">\lambda(x)=(\Delta x_{nt}^T \nabla^2 f(x) \Delta x_{nt})^{1/2}.</script>

<p>This shows that $\lambda$ is the norm of the Newton step, in the quadratic norm defined by the Hessian.</p>

<h3 id="newtons-method">Newton’s Method</h3>

<blockquote>
  <p><em>Newton’s method</em>. <br />
<strong>given</strong> a starting point $x \in \mathbf{dom} \enspace f$, tolerance $\epsilon &gt; 0$. <br />
<strong>repeat</strong> <br />
- Compute the Newton step $\Delta x_{nt}$ and decrement $\lambda^2$. <br />
- Stopping criterion. <em>quit** if $\lambda(x)^2/2 \le \epsilon$. <br />
- *Line search</em>. Choose a step size $t &gt; 0$ by backtracking line search. <br />
- Update. $x := x+ t\Delta x_{nt}$.</p>
</blockquote>

<h3 id="summary">Summary</h3>

<p>Newton’s method has several very strong advantages over gradient and steepest descent methods:</p>

<ul>
  <li>Convergence of Newton’s method is rapid in general, and quadratic near $x^\ast$. Once the quadratic convergence phase is reached, at most six or so iterations are required to produce a solution of very high accuracy.</li>
  <li>Newton’s method is affine invariant. It is insensitive to the choice of coordinates, or the condition number of the sublevel sets of the objective.</li>
  <li>The good performance of Newton’s method is not dependent on the choice of algorithm parameters. In contrast, the choice of norm for steepest descent plays a critical role in its performance.</li>
</ul>

<p>The main disadvantage of Newton’s method is the cost of forming and storing the Hessian, and the cost of computing the Newton step, which requires solving a set of linear equations.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Convex Optimization (4): Gradient Descent Method]]></title>
    <link href="http://billy-inn.github.io/blog/2018/11/05/convex-optimization-4/"/>
    <updated>2018-11-05T00:29:05-05:00</updated>
    <id>http://billy-inn.github.io/blog/2018/11/05/convex-optimization-4</id>
    <content type="html"><![CDATA[<h3 id="descent-methods">Descent methods</h3>

<script type="math/tex; mode=display">x^{(k+1)}=x^{(k)} + t^{(k)}\Delta x^{(k)}</script>

<ul>
  <li>$f(x^{(k+1)}) &lt; f(x^{(k)})$</li>
  <li>$\Delta x$ is the <em>step</em> or <em>search direction</em>; $t$ is the <em>step size</em> or <em>step length</em></li>
  <li>from convexity, $\nabla f(x)^T \Delta x &lt;0$</li>
</ul>

<blockquote>
  <p><em>General descent method</em>. <br />
<strong>given</strong> a starting point $x \in \mathbf{dom} \enspace f$. <br />
<strong>repeat</strong> <br />
- Determine a descent direction $\Delta x$. <br />
- <em>Line search</em>. Choose a step size $t &gt; 0$. <br />
- Update. $x := x+ t\Delta x$.</p>

  <p><strong>until</strong> stopping criterion is satisfied</p>
</blockquote>

<!--more-->

<h4 id="exact-line-search">Exact line search</h4>

<script type="math/tex; mode=display">t = \text{argmin}_{t > 0} \enspace f(x + t\Delta x)</script>

<h4 id="backtracking-line-search">Backtracking line search</h4>

<blockquote>
  <p><em>Backtracking line search</em>. <br />
<strong>given</strong> a descent direction $\Delta x$ for $f$ at $x \in \mathbf{dom} f, \alpha \in(0,0.5), \beta\in(0,1)$. <br />
<strong>starting</strong> at $t:=1$. <br />
<strong>while</strong> $f(x+t\Delta x) &gt; f(x) + \alpha t \nabla f(x)^T \Delta x$, $t:=\beta t$</p>
</blockquote>

<h3 id="gradient-descent-method">Gradient descent method</h3>

<p>A natural choice for the search direction is the negative gradient $\Delta x = - \nabla f(x)$.</p>

<h4 id="convergence-analysis-for-exact-line-search">Convergence analysis for exact line search</h4>

<p>We must have $f(x^(k)) - p^\ast \le \epsilon$ after at most</p>

<script type="math/tex; mode=display">\frac{\log((f(x^{(0)})-p^\ast)/\epsilon)}{\log(1/c)}</script>

<p>iterations of the gradient method with exact line search, where $c=1-m/M&lt;1$.</p>

<h4 id="convergence-analysis-for-backtracking-line-search">Convergence analysis for backtracking line search</h4>

<p>Similar to exact line search, except that $c=1 - \min{2m\alpha, 2\beta\alpha m/M} &lt; 1.$</p>

<h4 id="conclusions">Conclusions</h4>

<ul>
  <li>The gradient method often exhibits approximately linear convergence, <em>i.e.</em>, the error $f(x^{(k)}) - p^\ast$ converges to zeros approximately as a geometric series.</li>
  <li>The choice of backtracking parameters $\alpha, \beta$ has a noticeable but not dramatic effect on the convergence. An exact line search sometimes improves the convergence of the gradient method, but the effect is not large.</li>
  <li>The convergence rate depends greatly on the condition number of the Hessian, or the sublevel sets. Convergence can be very slow, even for problems that are moderately well conditioned. When the condition number is larger the gradient method is so slow that it is useless in practice.</li>
</ul>

<h3 id="steepest-descent-method">Steepest descent method</h3>

<p>The first-order Taylor approximation of $f(x+v)$ around $x$ is</p>

<script type="math/tex; mode=display">f(x+v)\approx \hat f(x+v)=f(x) + \nabla f(x)^T v.</script>

<p>The second term on the righthand side, $\nabla f(x)^T v$, is the <em>directional derivative</em> of $f$ at $x$ in the direction $v$. It gives the approximate change in $f$ for a small step $v$. The step $v$ is a descent direction if the directional derivative is negative.</p>

<p>Let $\lVert \cdot \rVert$ be any norm on $\mathbf{R}^n$. We define a <em>normailzied steepest descent direction</em> as</p>

<p>\begin{equation}
\Delta x_{nsd} = \arg\min{\nabla f(x)^T v\ \vert\ \lVert v \rVert = 1}.
\tag{1}\label{eq:1}
\end{equation}</p>

<p>It is also convenient to consider a steepest descent step $\Delta x_{sd}$ that is <em>unnormalized</em>, by scaling the normalized steepest descent direction in a particular way:</p>

<p>\begin{equation}
\Delta x_{sd} = \lVert \nabla f(x) \rVert_\ast \Delta x_{nsd},
\tag{2}\label{eq:2}
\end{equation}</p>

<p>where $\lVert \cdot \rVert_\ast$ denotes the dual norm. Note that for the steepest descent step, we have</p>

<script type="math/tex; mode=display">\nabla f(x)^T \Delta x_{sd} = \lVert \nabla f(x) \rVert_\ast \nabla f(x)^T \nabla x_{nsd}=-\lVert \nabla f(x) \rVert_\ast^2.</script>

<h4 id="steepest-descent-for-euclidean-norm">Steepest descent for Euclidean norm</h4>

<p>To simplify the notation, we can look at the problem of solving $\min_v{u^Tv\ \lvert\ \lVert v \rVert \le 1}$ which ends up being equivalent to find the normalized steepest descent step.</p>

<p>The Cauchy-Schwarz inequality gives $\lvert u^Tv\rvert \le \rVert u \rVert \lVert v \rVert$, hence it is easy to see that the minimum is $\min_v{u^Tv\ \lvert\ \lVert v \rVert \le 1}=-\lVert u \rVert$, and the minimizer is $v=-u/\lVert u \rVert$. As a result, the steepest descent direction is simply the negative gradient, <em>i.e.</em>, $\Delta x_{sd} = - \nabla f(x)$.</p>

<h4 id="steepest-descent-for-quadratic-norm">Steepest descent for quadratic norm</h4>

<p>We consider the quadratic norm</p>

<script type="math/tex; mode=display">\lVert z \rVert_P = (z^TPz)^{1/2}=\lVert P^{1/2}z\rVert_2,</script>

<p>where $P \in \mathbf{S}_{++}^n$. The problem is now $\min_v{u^Tv\ \vert\ \lVert P^{1/2}v\rVert\le1}=\min_v{u^Tv\ \vert\ \lVert\delta\rVert\le1, v=P^{-1/2}\delta}$.
This is equivalent to $\min_\delta{((P^{-1/2})^Tu)^T\delta\ \vert\ \lVert\delta\rVert\le1}$. The problem above shows that the minimum is $-\lVert (P^{-1/2})^Tu\rVert$ while the maximum $\lVert (P^{-1/2})^Tu\rVert$ is the dual norm according to the definition, and the minimizer is $v=P^{-1/2}\delta=-P^{-1}u/\lVert (P^{-1/2})^Tu\rVert$, so the steepest descent desnt is given by</p>

<script type="math/tex; mode=display">\Delta x_{sd} = -P^{-1} \nabla f(x).</script>

<p>In addition, the steepest descent method in the quadratic norm $\lVert \cdot \rVert_P$ can be thought of as the gradient method applied to the problem after the change of coordinates $\bar x=P^{1/2}x$.</p>

<h4 id="steepest-descent-for-l1-norm">Steepest descent for $l_1$-norm</h4>

<p>Let $i$ be any index for which $\lVert \nabla f(x) \rVert_\infty = \lvert (\nabla f(x))_i \rvert$. Then a normalized steepest descent direction $\nabla x_{nsd}$ for the $l_1$-norm is given by</p>

<script type="math/tex; mode=display">\Delta x_{nsd}=-\text{sign}\left(\frac{\partial f(x)}{\partial x_i}\right)e_i,</script>

<p>where $e_i$ is the $i$th standard basis vector. An unnormalized steepest descent step is then</p>

<script type="math/tex; mode=display">\Delta x_{sd} = \Delta x_{nsd}\lVert \nabla f(x) \rVert_\infty = - \frac{\partial f(x)}{\partial x_i}e_i.</script>

<p>The steepest descent algorithm in the $l_1$-norm has a very natural interpertation: At each iteration we select a component of $\nabla f(x)$ with maximum absolute value, and then decrease or increase the corresponding component of $x$, according to the sign of $(\nabla f(x))_i$. The algorithm is sometimes called a <em>corrdinate-descent</em> algorithm, since only one component of the variable $x$ is updated at each iteration.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Convex Optimization (3): Unconstrained Minimization Problems]]></title>
    <link href="http://billy-inn.github.io/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems/"/>
    <updated>2018-09-29T15:15:12-04:00</updated>
    <id>http://billy-inn.github.io/blog/2018/09/29/notes-on-convex-optimization-3-unconstrained-minimization-problems</id>
    <content type="html"><![CDATA[<p>Unconstrained optimization problems are defined as follows:</p>

<p>\begin{equation}
\text{minimize}\quad f(x)
\tag{1} \label{eq:1}
\end{equation}</p>

<p>where $f: \mathbf{R}^n \rightarrow \mathbf{R}$ is convex and twice continously differentiable (which implies that $\mathbf{dom}\enspace f$ is open). We denote the optimal value $\inf_xf(x)=f(x^\ast)$, as $p^\ast$. Since $f$ is differentiable and convex, a necessary and sufficient condition for a point $x^\ast$ to be optimal is</p>

<p>\begin{equation}
\nabla f(x^\ast)=0
\tag{2} \label{eq:2}
\end{equation}</p>

<p>. Thus, solving the unconstrained minimization problem \eqref{eq:1} is the same as finding a solution of \eqref{eq:2}, which is a set of $n$ equations in the $n$ variables $x_1, \dots, x_n$. Usually, the problem must be solved by an iterative algorithm. By this we mean an algorithm that computes a sequence of points $x^{(0)}, x^{(1)}, \dots \in \mathbf{dom}\enspace f$ with $f(x^{(k)})\rightarrow p^\ast$ as $k\rightarrow\infty$. The algorithm is terminated when $f(x^{k}) - p^\ast \le \epsilon$, where $\epsilon&gt;0$ is some specified tolerance.</p>

<!--more-->

<h4 id="initial-point-and-sublevel-set">Initial point and sublevel set</h4>

<p>The starting point $x^{(0)}$ must lie in $\mathbf{dom}\enspace f$, and in addition the sublevel set</p>

<script type="math/tex; mode=display">S = \{x \in \mathbf{dom}\enspace f \vert f(x) \le f(x^{(0)})\}</script>

<p>must be closed. This condition is satisfied for all $x^{(0)}\in\mathbf{dom}\enspace f$ if the function $f$ is closed.</p>

<p>Note: 1) Continuous functions with $\mathbf{dom}\enspace f=\mathbf{R}^n$ are closed; 2) Another important class of closed functions are continuous functions with open domains.</p>

<h3 id="examples">Examples</h3>

<h4 id="quadratic-minimization-and-least-squares">Quadratic minimization and least-squares</h4>

<p>The general convex quadratic minimization problem has the form</p>

<script type="math/tex; mode=display">\text{minimize}\quad (1/2)x^TPx+q^Tx+r,</script>

<p>where $P\in\mathbf{S}_+^n$, $q\in\mathbf{R}^n$, and $r\in\mathbf{R}$. This problem can be solved via the optimality conditions, $Px+q=0$, which is a set of linear equations.</p>

<p>One special case of the quadratic minimization problem that arises very frequently is the least-squares problem</p>

<script type="math/tex; mode=display">\text{minimize}\quad \lVert Ax-b\rVert_2^2=x^T(A^TA)x-2(A^Tb)^T+b^Tb.</script>

<p>The optimality condition</p>

<script type="math/tex; mode=display">A^TAx^\ast=A^Tb</script>

<p>are called the <em>normal equations</em> of the least-squares problem.</p>

<h4 id="unconstrained-geometric-programming">Unconstrained geometric programming</h4>

<script type="math/tex; mode=display">\text{minimize}\quad f(x)=\log(\sum_{i=1}^m\exp(a_i^Tx+b_i))</script>

<h4 id="analytic-center-of-linear-inequalities">Analytic center of linear inequalities</h4>

<script type="math/tex; mode=display">\text{minimize} f(x)=-\sum_{i=1}^m\log(b_i - a_i^Tx),</script>

<p>where the domain of $f$ is the open set</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\mathbf{dom}\enspace f=\{x\vert a_i^Tx<b_i,i=1,\dots,m\}. %]]&gt;</script>

<h4 id="analytic-center-of-a-linear-matrix-inequality">Analytic center of a linear matrix inequality</h4>

<script type="math/tex; mode=display">\text{minimize}\quad f(x)=\log\det F(x)^{-1}</script>

<p>where $F: \mathbf{R}^n\rightarrow\mathbf{S}^p$ is affine. Here the domain of $f$ is</p>

<script type="math/tex; mode=display">\mathbf{dom}\enspace f=\{x\vert F(x) \succ 0\}.</script>

<h3 id="strong-convexity-and-implications">Strong convexity and implications</h3>

<p>The objective function is <em>strongly convex</em> on $S$, which means that there exists an $m&gt;0$ such that</p>

<p>\begin{equation}
\nabla^2f(x) \succeq mI
\tag{3} \label{eq:3}
\end{equation}</p>

<p>for all $x\in S$.  For $x, y \in S$ we have</p>

<script type="math/tex; mode=display">f(y)=f(x)+\nabla f(x)^T(y-x) + \frac12(y-x)^T\nabla^2f(z)(y-x)</script>

<p>for some $z$ on the line segement $[x, y]$. By the strong convexity assumption \eqref{eq:3}, the last term on the righthand side is at least $(m/2)\lVert y-x\rVert^2_2$, so we have the inequality</p>

<p>\begin{equation}
f(y) \ge f(x) + \nabla f(x)^T(y-x) + \frac{m}2\lVert y-x \rVert_2^2
\tag{4} \label{eq:4}
\end{equation}</p>

<p>for all $x$ and $y$ in $S$.</p>

<h4 id="bound-fx-past-in-terms-of-lvert-nabla-fx-rvert2">Bound $f(x)-p^\ast$ in terms of $\lVert \nabla f(x) \rVert_2$</h4>

<p>Setting the gradient of the righthand side of \eqref{eq:4} with respect to $y$ equal to zero, we find that $\tilde y = x-(1/m)\nabla f(x)$ minimizes the righthand side. Therefore we have</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
f(y) &\ge f(x) + \nabla f(x)^T (y-x) + \frac{m}2\lVert y-x \rVert_2^2 \\
& \ge f(x) + \nabla f(x)^T(\tilde y - x) + \frac{m}2\lVert \tilde y - x \rVert_2^2 \\
& = f(x) - \frac1{2m}\lVert \nabla f(x)\rVert_2^2
\end{split} %]]&gt;</script>

<p>Since this holds for any $y\in S$, we have</p>

<p>\begin{equation}
p^* \ge f(x) - \frac1{2m}\lVert \nabla f(x)\rVert_2^2
\tag{5} \label{eq:5}
\end{equation}</p>

<p>This inequality shows that if the gradient is small at a point, then the point is nearly optimal.</p>

<h4 id="bound-lvert-x-xastrvert22-in-terms-of-lvert-nabla-fx-rvert2">Bound $\lVert x-x^\ast\rVert_2^2$ in terms of $\lVert \nabla f(x) \rVert_2$</h4>

<p>Apply \eqref{eq:4} with $y=x^\ast$ to obtain</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
p^* = f(x^\ast)  & \ge f(x) + \nabla f(x)^T (x^\ast - x) + \frac{m}2\lVert x^\ast-x \rVert_2^2 \\
& \ge f(x) - \lVert \nabla f(x) \rVert_2 \lVert x^\ast - x \rVert_2 + \frac{m}2 \lVert x^\ast - x \rVert_2^2,
\end{split} %]]&gt;</script>

<p>where we use the Cauchy-Schwarz inequality in the second inequality. Since $p^\ast \le f(x)$, we must have</p>

<script type="math/tex; mode=display">-\lVert \nabla f(x) \rVert_2 \lVert x^\ast - x \rVert_2 + \frac{m}2 \lVert x^\ast - x \rVert_2^2 \le 0.</script>

<p>Therefore, we have</p>

<p>\begin{equation}
\lVert x - x^\ast \rVert_2 \le \frac2m\lVert \nabla f(x) \rVert_2.
\tag{6}\label{eq:6}
\end{equation}</p>

<h4 id="uniqueness-of-the-optimal-point-xast">Uniqueness of the optimal point $x^\ast$</h4>

<p>If there are two optimal point $x^\ast_1, x^\ast_2$, according to \eqref{eq:6},</p>

<script type="math/tex; mode=display">\lVert x_1^\ast - x_2^\ast \rVert \le \frac2m \lVert f(x_1^\ast) \rVert_2 = 0.</script>

<p>Hence, $x_1^\ast = x_2^\ast$, the optimal point $x^\ast$ is unique.</p>

<h4 id="upper-bound-on-nabla2fx">Upper bound on $\nabla^2f(x)$</h4>

<p>There exists a  constant $M$ such that</p>

<script type="math/tex; mode=display">\nabla^2 f(x) \preceq MI</script>

<p>for all $x \in S$. This upper bound on the Hessian implies for any $x, y \in S$,</p>

<script type="math/tex; mode=display">f(y) \le f(x) + \nabla f(x)^T (y-x) + \frac{M}2\lVert y-x \rVert_2^2,</script>

<p>minimizing each side over $y$ yields</p>

<script type="math/tex; mode=display">p^\ast \le f(x) - \frac1{2M}\lVert \nabla f(x) \rVert_2^2.</script>

<h4 id="condition-number-of-sublevel-sets">Condition number of sublevel sets</h4>

<p>The ratio $\kappa=M/m$ is an upper bound on the condition number of the matrix $\nabla^2 f(x)$, <em>i.e.</em>, the ratio of its largest eigenvalue to its smallest eigenvalue.</p>

<p>We define the <em>width</em> of a convex set $C \subseteq \mathbf{R}^n$, in the direction $q$, where $\lVert q \rVert_2 = 1$, as</p>

<script type="math/tex; mode=display">W(C, q) = \sup_{z \in C} q^T z - \inf_{z\in C}q^Tz.</script>

<p>The <em>minimum width</em> and <em>maximum width</em> of $C$ are given by</p>

<script type="math/tex; mode=display">W_{min}=\inf_{\lVert q\rVert_2=1}W(C,q), \qquad W_{max}=\sup_{\lVert q\rVert_2=1}W(C,q).</script>

<p>The <em>condition number</em> of the convex set $C$ is defined as</p>

<script type="math/tex; mode=display">\mathbf{cond}(C)=\frac{W^2_{max}}{W^2_{min}}.</script>

<p>Suppose $f$ satisfies $mI \preceq \nabla^2 f(x) \preceq MI$ for all $x\in S$. The condition number of the $\alpha$-sublevel $C_\alpha={x \vert f(x) \le \alpha}$, where $p^\ast &lt; \alpha \le f(x^{(0)})$, is bounded by</p>

<script type="math/tex; mode=display">\mathbf{cond}(C_\alpha) \le \frac Mm.</script>

<h4 id="the-strong-convexity-constants">The strong convexity constants</h4>

<p>It must be kept in mind that the constants $m$ and $M$ are known only in rare cases, so they cannot be used in a practical stopping criterion.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Notes on Mathematics for ESL] Chapter 10: Boosting and Additive Trees]]></title>
    <link href="http://billy-inn.github.io/blog/2017/12/14/esl-chapter10/"/>
    <updated>2017-12-14T19:07:57-05:00</updated>
    <id>http://billy-inn.github.io/blog/2017/12/14/esl-chapter10</id>
    <content type="html"><![CDATA[<h3 id="why-exponential-loss">10.5 Why Exponential Loss?</h3>

<h4 id="derivation-of-equation-1016">Derivation of Equation (10.16)</h4>

<p>Since $Y\in{-1,1}$, we can expand the expectation as follows:</p>

<script type="math/tex; mode=display">\text{E}_{Y\vert x}(e^{-Yf(x)}) = \Pr(Y=1 \vert x)e^{-f(x)} + 
\Pr(Y=-1\vert x)e^{f(x)}</script>

<p>In order to minimize the expectation, we equal derivatives w.r.t. $f(x)$ as zero:</p>

<script type="math/tex; mode=display">-\Pr(Y=1\vert x)e^{-f(x)}+\Pr(Y=-1\vert x)e^{f(x)}=0</script>

<p>which gives:</p>

<script type="math/tex; mode=display">f^*(x)=\frac12\log\frac{\Pr(Y=1\vert x)}{\Pr(Y=-1\vert x)}</script>

<!--more-->

<h4 id="notes-on-equation-1018">Notes on Equation (10.18)</h4>

<p>If $Y=1$, then $Y’=1$, which gives</p>

<script type="math/tex; mode=display">l(Y,f(x))=\log p(x)=\log(1+e^{-2f(x)})</script>

<p>Likewise, if $Y=-1$, then $Y’=0$, which gives</p>

<script type="math/tex; mode=display">l(Y,f(x))=\log (1-p(x))=\log(1+e^{2f(x)})</script>

<p>As a result, the <em>binomial log-likelihood loss</em> is equivalent to the <em>deviance</em>. In the language of neural networks, the <em>cross-entropy</em> is equivalent to the <em>softplus</em>. The only difference is that $0$ is used to indicate negative examples in <em>cross-entropy</em>; while $-1$ is used in <em>softplus</em>.</p>

<h3 id="loss-functions-and-robustness">10.6 Loss Functions and Robustness</h3>

<p>This section explains the choice of loss functions for both classification and regression. It gives a very direct expalanation about why square loss is undesirable for classification. Highly recommended!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Notes on Mathematics for ESL] Chapter 6: Kernel Smoothing Methods]]></title>
    <link href="http://billy-inn.github.io/blog/2017/10/27/esl-chapter-6/"/>
    <updated>2017-10-27T18:57:21-04:00</updated>
    <id>http://billy-inn.github.io/blog/2017/10/27/esl-chapter-6</id>
    <content type="html"><![CDATA[<h3 id="one-dimensional-kernel-smoothers">6.1 One-Dimensional Kernel Smoothers</h3>

<h4 id="notes-on-local-linear-regression">Notes on Local Linear Regression</h4>

<p>Locally weighted regression solves a separate weighted least squares problem at each target point $x_0$:</p>

<script type="math/tex; mode=display">\min_{\alpha(x_0),\beta(x_0)}\sum_{i=1}^NK_\lambda(x_0,x_i)[y-\alpha(x_0)-\beta(x_0)x_i]^2</script>

<p>The estimate is  $\hat f(x_0)=\hat\alpha(x_0)+\hat\beta(x_0)x_0$. Define the vector-value function $b(x)^T=(1,x)$. Let $\mathbf{B}$ be the $N \times 2$ regression matrix with $i$th row row $b(x_i)^T$, $\mathbf{W}(x_0)$ the $N\times N$ diagonal matrix with $i$th diagonal element $K_\lambda (x_0, x_i)$, and $\theta=(\alpha(x_0), \beta(x_0))^T$.</p>

<p>Then the above optimization problem can be rewritten as</p>

<script type="math/tex; mode=display">\min_\theta(y-\mathbf{B}\theta)^T\mathbf{W}(x_0)(y-\mathbf{B}\theta)</script>

<p>Equal the derivative w.r.t $\theta$ as zero, we get</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
&\mathbf{B}^T\mathbf{W}(x_0)(y-\mathbf{B}\hat\theta)=0 \\
&\mathbf{B}^T\mathbf{W}(x_0)\mathbf{B}\hat\theta = \mathbf{B}^T\mathbf{W}(x_0)y \\
&\hat\theta= (\mathbf{B}^T\mathbf{W}(x_0)\mathbf{B})^{-1}\mathbf{B}\mathbf{W}(x_0)y
\end{split} %]]&gt;</script>

<!--more-->

<p>Then</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat f(x_0)&=b(x_0)^T\hat\theta \\
&=b(x_0)^T(\mathbf{B}^T\mathbf{W}(x_0)\mathbf{B})^{-1}\mathbf{B}\mathbf{W}(x_0)y \\
&=\sum_{i=1}^Nl_i(x_0)y_i
\end{split} %]]&gt;</script>

<p>It’s claimed that $\sum_{i=1}^Nl_i(x_0)=1$ and $\sum_{i=1}^N(x-x_0)l_i(x_0)=0$ in the book, so that the bias $\text{E}(\hat f(x_0))-f(x_0)$ depends only on quadratic and higher-order terms in the expansion of $f$. However, the proof is not given. Here I will give the detailed derivations of these two equations.</p>

<p>First, define the following terms:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
S_0 &= \sum_{i=1}^NK_\lambda(x_0, x_i) \\
S_1 &= \sum_{i=1}^NK_\lambda(x_0, x_i)x_i \\
S_2 &= \sum_{i=1}^NK_\lambda(x_0, x_i)x_i^2 \\
m_0 &= \sum_{i=1}^NK_\lambda(x_0, x_i)y_i \\
m_1 &= \sum_{i=1}^NK_\lambda(x_0, x_i)x_iy_i
\end{split} %]]&gt;</script>

<p>Then, we can represent the estimate as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat f(x_0) &= b(x_0)^T\begin{bmatrix}S_0& S_1\\S_1& S_2\end{bmatrix}^{-1}\begin{bmatrix}m_0\\m_1\end{bmatrix} \\
&= \frac1{S_0S_2-S_1^2}b(x_0)^T\begin{bmatrix}S_2 &-S_1\\-S_1& S_0\end{bmatrix}\begin{bmatrix}m_0\\m_1\end{bmatrix} \\
&=\frac{(S_2-S_1x_0)m_0-(S_1-S_0x_0)m_1}{S_0S_2-S_1^2}
\end{split} %]]&gt;</script>

<p>When $y=\mathbf{1}$, $m_0=S_0$ and $m_1=S_1$, we get</p>

<script type="math/tex; mode=display">\hat f(x_0)=\sum_{i=1}^Nl_i(x_0)=\frac{S_0S_2-S_1^2}{S_0S_2-S_1^2}=1</script>

<p>When $y=\mathbf{x}-x_0$,</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat f(x_0) &= \sum_{i=1}^N(x_i-x_0)l_i(x_0) \\
&= \frac{(S_2-S_1x_0)(S_1-S_0x_0)-(S_1-S_0x_0)(S_2-S_1x_0)}{S_0S_2-S_1^2} \\
&= 0
\end{split} %]]&gt;</script>

<p>More generally, it’s easy to show that $\sum_{i=1}^N(x_i-x_0)^pl_i(x_0)=0$ when $p&gt;0$.</p>

<p>We only prove the case when the input $x$ is one-dimensional. Similar strategy can be used to prove the case for high-dimensional input, but it’ll be a little bit complicated if you’re interested. Have fun!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Notes on Mathematics for ESL] Chapter 5: Basis Expansions and Regularization]]></title>
    <link href="http://billy-inn.github.io/blog/2017/10/24/esl-chapter-5/"/>
    <updated>2017-10-24T20:34:25-04:00</updated>
    <id>http://billy-inn.github.io/blog/2017/10/24/esl-chapter-5</id>
    <content type="html"><![CDATA[<h3 id="smoothing-splines">5.4 Smoothing Splines</h3>

<h4 id="derivation-of-equation-512">Derivation of Equation (5.12)</h4>

<p>Equal the derivative of <strong>Equation (5.11)</strong> as zero, we get</p>

<script type="math/tex; mode=display">\frac{\partial\text{RSS}(\theta,\lambda)}{\partial\theta} = -2N^T(y-N\theta)+2\lambda\Omega_N\theta = 0</script>

<p>Put the terms related to $\theta$ on one side and the others on the other side, we get</p>

<script type="math/tex; mode=display">(N^TN+\lambda\Omega_N)\theta = N^Ty</script>

<p>Multiply the inverse of $N^TN+\lambda\Omega_N$ on both sides completes the derivation of <strong>Equation (5.12)</strong></p>

<script type="math/tex; mode=display">\hat\theta = (N^TN+\lambda\Omega_N)^{-1}N^Ty</script>

<!--more-->

<h4 id="explanations-on-equation-517-and-518">Explanations on Equation (5.17) and (5.18)</h4>

<p>It’s a little confusing to get <strong>Equation (5.18)</strong> directly from <strong>Equation (5.17)</strong> and its original form <strong>Equation (5.11)</strong>. In order to give a clear explanation, here we give the proof of the equation,</p>

<script type="math/tex; mode=display">\theta^T\Omega_N\theta=\mathbf{f}^TK\mathbf{f}.</script>

<p>which are the different terms between <strong>Equation (5.11)</strong> and <strong>Equation (5.18)</strong>.</p>

<p>We know that</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\mathbf{f}&=N(N^TN+\lambda\Omega_N)^{-1}N^Ty \\
&=S_{\lambda}y = N\theta
\end{split} %]]&gt;</script>

<p>following <strong>Equation (5.14)</strong> in the book.</p>

<p>From <strong>Equation (5.17)</strong>, we can get</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
K&=\frac1\lambda(S_\lambda^{-1}-I)\\
&=\frac1\lambda\left(N^{-T}(N^TN+\lambda\Omega_N)N^{-1}-I\right)\\
&=N^{-T}\Omega_NN^{-1}
\end{split} %]]&gt;</script>

<p>Plug the above two equation into the right side of the equation remains to be proved, we get</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\mathbf{f}^TK\mathbf{f} &= \theta^TN^TN^{-T}\Omega_NN^{-1}N\theta \\
&=\theta^T\Omega_N\theta
\end{split} %]]&gt;</script>

<p>which completes the proof.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Convex Optimization (2): Convex Functions]]></title>
    <link href="http://billy-inn.github.io/blog/2017/10/21/convex-optimization-2/"/>
    <updated>2017-10-21T13:59:09-04:00</updated>
    <id>http://billy-inn.github.io/blog/2017/10/21/convex-optimization-2</id>
    <content type="html"><![CDATA[<h3 id="basic-properties-and-examples">1. Basic Properties and Examples</h3>

<h4 id="definition">1.1 Definition</h4>

<p>$f:\mathbb{R}^n \rightarrow \mathbb R$ is convex if $\mathbf{dom}\ f$ is a convex set and</p>

<script type="math/tex; mode=display">f(\theta x+(1-\theta)y)\le\theta f(x)+(1-\theta)f(y)</script>

<p>for all $x,y\in \mathbf{dom}\ f, 0\le\theta\le1$</p>

<ul>
  <li>$f$ is concave if $-f$ is convex</li>
  <li>$f$ is strictly convex if $\mathbf{dom}\ f$ is convex and <script type="math/tex">% &lt;![CDATA[
f(\theta x+(1-\theta)y)<\theta f(x)+(1-\theta)f(y) %]]&gt;</script> for $x,y\in\mathbf{dom}\ f,x\ne y, 0&lt;\theta&lt;1$</li>
</ul>

<p>$f:\mathbb{R}^n \rightarrow \mathbb R$ is convex if and only if the function $g: \mathbb{R} \rightarrow \mathbb{R}$,</p>

<script type="math/tex; mode=display">g(t)=f(x+tv),\quad\mathbf{dom}\ g=\{t\mid x+tv\in\mathbf{dom}\ f\}</script>

<p>is convex (in $t$) for any $x \in \mathbf{dom}\ f, v\in\mathbb R^n$</p>

<!--more-->

<h4 id="extended-value-extensions">1.2 Extended-value extensions</h4>

<p>extended-value extension $\tilde f$ of $f$ is</p>

<script type="math/tex; mode=display">\tilde f(x)=f(x),x\in\mathbf{dom}\ f; \tilde f(x)=\infty,x\ne \mathbf{dom}\ f</script>

<h4 id="first-order-conditions">1.3 First-order conditions</h4>

<p><strong>1st-order condition</strong>: differentiable $f$ with convex domain is convex iff</p>

<script type="math/tex; mode=display">f(y)\ge f(x)+\nabla f(x)^T(y-x)\mathrm{\ for\ all\ } x,y\in\mathbf{dom}\ f</script>

<h4 id="second-order-conditions">1.4 Second-order conditions</h4>

<p><strong>2nd-order conditions</strong>: for twice differentiable $f$ with convex domain
- $f$ if convex if and only if
<script type="math/tex">\nabla^2f(x)\succeq0\mathrm{\ for\ all\ } x\in\mathbf{dom}\ f</script>
- if $\nabla^2f(x)\succ0$ for all $x\in\mathbf{dom}\ f$, then $f$ is strictly convex</p>

<h4 id="sublevel-sets-and-epigraph">1.5 Sublevel sets and epigraph</h4>

<p>$\alpha$<strong>-sublevel set</strong> of $f: \mathbb R^n \rightarrow \mathbb R$:</p>

<script type="math/tex; mode=display">C_\alpha=\{x\in\mathbf{dom}\ f \mid f(x)\le\alpha\}</script>

<p>sublevel sets of convex functions are convex (converse if false)</p>

<p>If $f$ is concave, then its $\alpha$<strong>-superlevel set</strong>, given by ${x\in\mathbf{dom}\ f\mid f(x)\le\alpha}$, is a convex set</p>

<p><strong>epigraph</strong> of $f:\mathbb R^n \rightarrow \mathbb R$:</p>

<script type="math/tex; mode=display">\mathbf{epi}\ f=\{(x,t)\in\mathbb R^{n+1}\mid x\in\mathbf{dom}\ f,f(x)\le t\}</script>

<p>$f$ is convex if and only if $\mathbf{epi}\ f$ is a convex set</p>

<p>$f$ is concave if and only if its <strong>hypograph</strong>, defined as</p>

<script type="math/tex; mode=display">\mathbf{hypo}\ f= \{(x,t)\in\mathbb R^{n+1}\mid x\in\mathbf{dom}\ f,f(x)\ge t\}</script>

<p>is a convex set</p>

<h4 id="jensens-inequality-and-extensions">1.6 Jensen’s inequality and extensions</h4>

<p><strong>Jensen’s Inequality</strong>: if $f$ is convex, then for $0\le\theta\le1$,</p>

<script type="math/tex; mode=display">f(\theta x+(1-\theta)y)\le\theta f(x)+(1-\theta)f(y)</script>

<p><strong>extension</strong>: if $f$ is convex, then</p>

<script type="math/tex; mode=display">f(\mathbb E z)\le \mathbb Ef(z)</script>

<p>for any random variable $z$</p>

<h3 id="operations-that-preserve-convexity">2. Operations that Preserve Convexity</h3>

<h4 id="positive-weighted-sum--composition-with-affine-function">2.1 Positive weighted sum &amp; composition with affine function</h4>

<p><strong>nonnegative multiple</strong>: $\alpha f$ is convex if $f$ is convex, $\alpha \ge 0$</p>

<p><strong>sum</strong>: $f_1+f_2$ convex if $f_1,f_2$ convex (extends to infinite sums, integrals)</p>

<p><strong>composition with affine function</strong>: $f(Ax+b)$ is convex if $f$ is convex</p>

<h4 id="pointwise-maximum">2.2 Pointwise maximum</h4>

<p><strong>pointwise maximum</strong>: if $f_1,\dots,f_m$ are convex, then $f(x)=\max{f_1(x),\dots,f_m(x)}$ is convex</p>

<p><strong>pointwise supermum</strong>: if $f(x,y)$ is convex in $x$ for each $y\in\mathcal{A}$, then</p>

<script type="math/tex; mode=display">g(x)=\sup_{y\in\mathcal{A}}f(x,y)</script>

<p>is convex</p>

<p>similarly, the <strong>pointwise infimum</strong> of a set of concave functions is a concave function</p>

<h4 id="composition">2.3 Composition</h4>

<p><strong>composition with scalar functions</strong>: composition of $g: \mathbb R^n \rightarrow \mathbb R$ and $h: \mathbb R\rightarrow \mathbb R$:</p>

<script type="math/tex; mode=display">f(x)=h(g(x))</script>

<p>$f$ is convex if $g$ convex, $h$ convex, $\tilde h$ nondecreasing; $g$ concave, $h$ convex, $\tilde h$ nonincreasing</p>

<p>Note: monotonicity must hold for extended-value extension $\tilde h$</p>

<p><strong>vector composition</strong>: composition of $g:\mathbb R^n \rightarrow \mathbb R^k$ and $h:\mathbb R^k \rightarrow \mathbb R$:</p>

<script type="math/tex; mode=display">f(x)=h(g(x))=h(g_1(x),g_2(x),\dots,g_k(x))</script>

<p>$f$ is convex if $g_i$ convex, $h$ convex, $\tilde h$ nondecreasing in each argument; $g$ concave, $h$ convex, $\tilde h$ nonincreasing in each argument</p>

<h4 id="minimization">2.4 Minimization</h4>

<p><strong>minimization</strong>: if $f(x,y)$ is convex in $(x,y)$ and $C$ is a convex set then</p>

<script type="math/tex; mode=display">g(x)=\inf_{y\in C}f(x,y)</script>

<p>is convex</p>

<h4 id="perspective-of-a-function">2.5 Perspective of a function</h4>

<p><strong>perspective</strong>: the <strong>perspective</strong> of a function $f:\mathbb R^n \rightarrow \mathbb R$ is the function $g:\mathbb R^n \times \mathbb R \rightarrow \mathbb R$,</p>

<script type="math/tex; mode=display">g(x,t)=tf(x/t),\mathbf{dom}\ g=\{(x,t)\mid x/t\in\mathbf{dom}\ f,t>0\}</script>

<p>$g$ is convex if $f$ is convex</p>

<h3 id="the-conjugate-function">3. The Conjugate Function</h3>

<h4 id="definition-1">3.1 Definition</h4>

<p>the <strong>conjugate</strong> of a function $f$ is</p>

<script type="math/tex; mode=display">f^*(y)=\sup_{x\in\mathbf{dom}\ f}(y^Tx-f(x))</script>

<ul>
  <li>$f^*$ is convex whether or not $f$ is convex</li>
</ul>

<h4 id="basic-properties">3.2 Basic properties</h4>

<p><strong>conjugate of the conjugate</strong>: if $f$ is convex and closed, then $f^{**}=f$</p>

<p><strong>differentiable functions</strong>: The conjugate of a differentiable function $f$ is also called the <em>Legendre transform</em> of $f$. Let $z\in\mathbb{R}^n$ be arbitrary and define $y=\nabla f(z)$, then we have</p>

<script type="math/tex; mode=display">f^*(y)=z^T\nabla f(z)-f(z)</script>

<p><strong>scaling and composition with affline transformation</strong>: 
For $a&gt;0$ and $b\in\mathbb{R}$, the conjugate of $g(x)=af(x)+b$ is $g^*(y)=af^*(y/a)-b$.</p>

<p>Suppose $A\in\mathbb{R}^{n\times n}$ is nonsingular and $b\in\mathbb{R}^n$. Then the conjugate of $g(x)=f(Ax+b)$ is</p>

<script type="math/tex; mode=display">g^*(y)=f^*(A^{-T}y)-b^TA^{-T}y</script>

<p>with $\mathbf{dom}\ g^*=A^T\mathbf{dom}\ f^*$</p>

<p><strong>sums of independent functions</strong>: if $f(u,v)=f_1(u)+f_2(v)$, where $f_1$ and $f_2$ are convex functions with conjugates $f_1^*$ and $f_2^*$, respectively, then</p>

<script type="math/tex; mode=display">f^*(w,z)=f_1^*(w)+f_2^*(z)</script>

<h3 id="quasiconvex-functions">4. Quasiconvex Functions</h3>

<p>$f:\mathbb R^n\rightarrow \mathbb R$ is quasiconvex if $\mathbf{dom}\ f$ is convex and the sublevel sets</p>

<script type="math/tex; mode=display">S_{\alpha}=\{x\in\mathbf{dom}\ f\mid f(x)\le\alpha\}</script>

<p>are convex for all $\alpha$</p>

<ul>
  <li>$f$ is quasiconcave if $-f$ is quasiconvex</li>
  <li>$f$ is quasilinear if it is quasiconvex and quasiconcave</li>
  <li>convex functions are quasiconvex, but the converse is not true</li>
</ul>

<p><strong>modified Jensen inequality</strong>: for quasiconvex $f$</p>

<script type="math/tex; mode=display">0\le\theta\le1\Longrightarrow f(\theta x+(1-\theta)y)\le\max\{f(x),f(y)\}</script>

<p><strong>first-order condition</strong>: differentiable $f$ with convex domain is quasiconvex iff</p>

<script type="math/tex; mode=display">f(y)\le f(x)\Longrightarrow\nabla f(x)^T(y-x)\le0</script>

<p><strong>operations that preserve quasiconvexity</strong>:</p>

<ul>
  <li>nonnegative weighted maximum</li>
  <li>composition</li>
  <li>minimization</li>
</ul>

<p><strong>sum</strong> of quasiconvex functions are not necessarily quasiconvex</p>

<h3 id="log-concave-and-log-convex-functions">5. Log-concave and Log-convex Functions</h3>

<p>a positive function $f$ is log-concave if $\log f$ is concave:</p>

<script type="math/tex; mode=display">f(\theta x+(1-\theta)y)\ge f(x)^\theta f(y)^{1-\theta}\mathrm{\ for\ }0\le\theta\le1</script>

<p>$f$ is log-convex if $\log f$ is convex</p>

<p><strong>properties of log-concave functions</strong>:</p>

<ul>
  <li>twice differentiable $f$ with convex domain is log-concave iff</li>
</ul>

<script type="math/tex; mode=display">f(x)\nabla^2f(x)\preceq\nabla f(x)\nabla f(x)^T</script>

<p>for all $x\in\mathbf{dom}\ f$</p>

<ul>
  <li>product of log-concave functions is log-concave</li>
  <li>sum of log-concave functions is not always log-concave; however, log-convexity is preserved under sums</li>
  <li>integration: if $f:\mathbb R^n\times\mathbb R^m \rightarrow \mathbb R$ is log-concave, then</li>
</ul>

<script type="math/tex; mode=display">g(x)=\int f(x,y)dy</script>

<p>is log-concave</p>

<p><strong>consequences of integration property</strong>:</p>

<ul>
  <li>convolution $f*g$ of log-concave functions $f,g$ is log-concave</li>
</ul>

<script type="math/tex; mode=display">(f*g)(x)=\int f(x-y)g(y)dy</script>

<ul>
  <li>if $C\subseteq \mathbb R^n$ convex and $y$ is a random variable with log-concave pdf then</li>
</ul>

<script type="math/tex; mode=display">f(x)=\mathbf{prob}(x+y\in C)</script>

<p>is log-concave</p>

<h3 id="convexity-wrt-generalized-inequalities">6. Convexity w.r.t. Generalized Inequalities</h3>

<p>$f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is $K$-convex if $\mathbf{dom}\ f$ is convex and</p>

<script type="math/tex; mode=display">f(\theta x+(1-\theta)y)\preceq_K\theta f(x)+(1-\theta)f(y)</script>

<p>for $x,y\in\mathbf{dom}\ f,0\le\theta\le1$</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Convex Optimization (1): Convex Sets]]></title>
    <link href="http://billy-inn.github.io/blog/2017/10/17/convex-optimization-1/"/>
    <updated>2017-10-17T02:22:23-04:00</updated>
    <id>http://billy-inn.github.io/blog/2017/10/17/convex-optimization-1</id>
    <content type="html"><![CDATA[<h3 id="affine-and-convex-sets">1. Affine and Convex Sets</h3>

<p>Suppose $x_1\ne x_2$ are two points in $\mathbb{R}^n$.</p>

<h4 id="affine-sets">1.1 Affine sets</h4>

<p><strong>line</strong> through $x_1$, $x_2$: all points</p>

<script type="math/tex; mode=display">x=\theta x_1 + (1-\theta)x_2\quad(\theta \in \mathbb{R})</script>

<p><strong>affine set</strong>: contains the line through any two distinct points in the set</p>

<script type="math/tex; mode=display">x_1,x_2\in C,\ \theta\in\mathbb{R} \Longrightarrow \theta x_1 + (1-\theta)x_2 \in C</script>

<!--more-->

<h4 id="convex-sets">1.2 Convex sets</h4>

<p><strong>line segment</strong> between $x_1$ and $x_2$: all points</p>

<script type="math/tex; mode=display">x=\theta x_1 + (1-\theta)x_2</script>

<p>with $0\leq\theta\leq1$</p>

<p><strong>convex set</strong>: contains line segment between any two points in the set</p>

<script type="math/tex; mode=display">x_1,x_2\in C,\ 0\leq\theta\leq1 \Longrightarrow \theta x_1 + (1-\theta)x_2 \in C</script>

<p><strong>convex combination</strong> of $x_1,\dots,x_k$: any point $x$ of the form</p>

<script type="math/tex; mode=display">x=\theta_1x_1+\theta_2x_2+\dots+\theta_kx_k</script>

<p>with $\theta_1+\dots+\theta_k=1,\theta_i \geq 0$</p>

<p><strong>convex hull</strong> of a set $C$, denoted $\mathbf{conv}\ C$: set of all convex combinations of points in $C$</p>

<h4 id="cones">1.3 Cones</h4>

<p><strong>conic combination</strong> of $x_1$ and $x_2$: any point of the form</p>

<script type="math/tex; mode=display">x=\theta_1x_1+\theta_2x_2</script>

<p>with $\theta_1 \geq 0, \theta_2 \geq 0$</p>

<p><strong>convex cone</strong>: set that contains all conic combinations of points in the set</p>

<script type="math/tex; mode=display">x_1,x_2\in C,\ \theta_1\ge0, \theta_2\ge0 \Longrightarrow \theta_1 x_1 + \theta_2x_2 \in C</script>

<h3 id="some-important-examples">2. Some Important Examples</h3>

<h4 id="hyperplanes-and-halfspaces">2.1 Hyperplanes and halfspaces</h4>

<p><strong>hyperplane</strong>: set of the form {$x\mid a^Tx=b$}$(a\ne0)$</p>

<p><strong>halfspace</strong>: set of the form {$x\mid a^Tx\leq b$}$(a\ne0)$</p>

<ul>
  <li>$a$ is the normal vector</li>
  <li>hyperplanes are affine and convex; halfspaces are convex</li>
</ul>

<h4 id="euclidean-balls-and-ellipsoids">2.2 Euclidean balls and ellipsoids</h4>

<p><strong>(Euclidean) ball</strong> with center $x_c$ and radius $r$:</p>

<script type="math/tex; mode=display">B(x_c,r)=\{x\mid\lVert x-x_c\rVert_2\leq r\}=\{x_c+ru \mid\lVert u \rVert_2 \leq 1 \}</script>

<p><strong>ellipsoid</strong>: set of the form</p>

<script type="math/tex; mode=display">\{x \mid (x-x_c)^TP^{-1}(x-x_c)\leq 1\}</script>

<p>with $P\in \mathbf{S}^n_{++}$ (<em>i.e.</em>, P symmetric positive definite)</p>

<p>another representation: {$x_c+Au\mid \lVert u\rVert_2\le1$} with $A$ square and nonsingular</p>

<ul>
  <li>Euclidean balls and ellipsoids are all convex.</li>
</ul>

<h4 id="norm-balls-and-norm-cones">2.3 Norm balls and norm cones</h4>

<p><strong>norm</strong>: a funtion $\lVert \centerdot \rVert$ that satisfies</p>

<ul>
  <li>$\lVert x \rVert \geq 0$; $\lVert x \rVert=0$ if and only if $x=0$</li>
  <li>$\lVert tx \rVert = \lvert t \rvert \lVert x \rVert$ for $t\in \mathbb{R}$</li>
  <li>$\lVert x+y\rVert \leq \lVert x \rVert+\lVert y \rVert$</li>
</ul>

<p><strong>norm ball</strong> with center $x_c$ and radius <script type="math/tex">r: \{x \mid \lVert x-x_c \rVert \leq r\}</script></p>

<p><strong>norm cone</strong>: <script type="math/tex">\{(x,t) \mid \lVert x \rVert \leq t\}</script></p>

<ul>
  <li>norm balls and cones are convex</li>
  <li>norm cores (as the name suggest) are convex cones</li>
</ul>

<h4 id="polyhedra">2.4 Polyhedra</h4>

<p><strong>polyhedra</strong>: solution set of finitely many linear inequalities and equalities</p>

<script type="math/tex; mode=display">Ax \preceq b, \quad Cx=d</script>

<p>($A\in \mathbb{R}^{m\times n}$, $C\in\mathbb{R}^{p\times n}$, $\preceq$ is componentwise inequality)</p>

<ul>
  <li>polyhedron is intersection of finite number of halfspaces and hyperplances</li>
</ul>

<h4 id="the-positive-semidefinite-cone">2.5 The positive semidefinite cone</h4>

<p><strong>positive semidefinite cone</strong>:</p>

<ul>
  <li>$\mathbf{S}^n$ is set of symmetric $n\times n$ matrices</li>
  <li><script type="math/tex">\mathbf{S}^n_+=\{X\in\mathbf{S}^n\mid X\succeq0\}</script>: positive semidefinite $n\times n$ matrices <script type="math/tex">X\in\mathbf{S}^n_+ \iff z^TXz \geq 0\ \mathrm{for\ all\ }z</script> $\mathbf{S}^n_+$ is a convex cone</li>
  <li><script type="math/tex">\mathbf{S}^n_{++}=\{X\in\mathbf{S}^n\mid X\succ0\}</script>: positive definite $n\times n$ matrices</li>
</ul>

<h3 id="operations-that-preserve-convexity">3. Operations that preserve convexity</h3>

<p><strong>intersection</strong>: the interction of (any number of) convex sets is convex</p>

<p><strong>affine function</strong>: suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is affine ($f(x)=Ax+b$ with $A\in\mathbb{R}^{m\times n}, b\in\mathbb{R}^m$)</p>

<ul>
  <li>the image of a convex set under $f$ is convex</li>
</ul>

<script type="math/tex; mode=display">S \subseteq \mathbb{R}^n\ convex \Longrightarrow f(S)=\{f(x)\mid x\in S\}\ convex</script>

<ul>
  <li>the inverse image $f^{-1}(C)$ of a convex set under $f$ is convex</li>
</ul>

<script type="math/tex; mode=display">C \subseteq \mathbb{R}^m\ convex \Longrightarrow f^{-1}(C)=\{x\in\mathbb{R}^n\mid f(x)\in C\}\ convex</script>

<p><strong>perspective function</strong> $P: \mathbb{R}^{n+1} \rightarrow \mathbb{R}^n$:</p>

<script type="math/tex; mode=display">P(x,t)=x/t, \quad \mathbf{dom}\ P=\{(x,t)\mid t>0\}</script>

<p>images and inverse images of convex sets under perspective are convex</p>

<p><strong>linear-fractional function</strong> $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$:</p>

<script type="math/tex; mode=display">f(x)=\frac{Ax+b}{c^Tx+d}, \quad \mathbf{dom}\ f=\{x\mid c^Tx+d>0\}</script>

<p>images and inverse images of convex sets under linear-fractional functions are convex</p>

<h3 id="generalized-inequalities">4. Generalized Inequalities</h3>

<h4 id="proper-cones-and-generalized-inequalities">4.1 Proper cones and generalized inequalities</h4>

<p>a convex cone $K\subseteq\mathbb{R}^n$ is a <strong>proper cone</strong> if</p>

<ul>
  <li>$K$ is closed (contains its boundary)</li>
  <li>$K$ is solid (has nonempty interior)</li>
  <li>$K$ is pointed (contains no line)</li>
</ul>

<p><strong>generalized inequality</strong> defined by a proper cone $K$:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
x \preceq_K y &\iff y-x \in K\\
x \prec_K y &\iff y-x \in \mathbf{int}\ K
\end{split} %]]&gt;</script>

<h4 id="minimum-and-minimal-elements">4.2 Minimum and minimal elements</h4>

<p>$x\in S$ is <strong>the minimum element</strong> of $S$ with respect to $\preceq_K$ if</p>

<script type="math/tex; mode=display">y \in S \Longrightarrow x \preceq_K y</script>

<p>$x\in S$ is <strong>a minimal element</strong> of $S$ with respect to $\preceq_K$ if</p>

<script type="math/tex; mode=display">y \in S, y \preceq_K x \Longrightarrow y=x</script>

<h3 id="separating-and-supporting-hyperplanes">5. Separating and Supporting Hyperplanes</h3>

<p><strong>separating hyperplane theorem</strong>: if $C$ and $D$ are disjoint convex sets, then there exists $a\ne0$, $b$ such that</p>

<script type="math/tex; mode=display">a^Tx \le b\ \mathrm{for}\ x\in C,\quad a^Tx\ge b\ \mathrm{for}\ x\in D</script>

<p><strong>supporting hyperplane</strong> to set $C$ at boundary point $x_0$:</p>

<script type="math/tex; mode=display">\{x\mid a^Tx=a^Tx_0\}</script>

<p>where $a\ne0$ and $a^Tx\le a^Tx_0$ for all $x\in C$</p>

<p><strong>supporting hyperplance theorem</strong>: if $C$ is convex, then there exists a supporting hyperplane at every boundary point of $C$</p>

<h3 id="dual-cones-and-generalized-inequalities">6. Dual Cones and Generalized Inequalities</h3>

<h4 id="dual-cones">6.1 Dual cones</h4>

<p><strong>dual cone</strong> of a cone $K$:</p>

<script type="math/tex; mode=display">K^*=\{y\mid y^T x \ge 0\mathrm{\ for\ all\ } x\in K\}</script>

<p>Dual cons satisfy several properties, such as:</p>

<ul>
  <li>$K^*$ is closed and convex</li>
  <li>$K_1 \subseteq K_2$ imples $K_2^* \subseteq K_1^*$</li>
  <li>$K^{**}$ is the closure of the convex hull of $K$ (Hence if $K$ is convex and closed, $K^{**}=K$)</li>
</ul>

<p>Thsese properties show that if $K$ is a proper cone, then so is its dual $K^{*}$, and moreover, that $K^{**}=K$</p>

<h4 id="dual-generalized-inequalities">6.2 Dual generalized inequalities</h4>

<p>dual cones of proper cones are proper, hence define generalized inequalities:</p>

<script type="math/tex; mode=display">y\succeq_{K^*}0 \iff y^Tx\ge0\mathrm{\ for\ all\ } x\succeq_K0</script>

<p>Some import properties relating a generalized inequality and its dual are:</p>

<ul>
  <li>$x\preceq_K y$ iff $\lambda^Tx \le \lambda^Ty$ for all $\lambda \succeq_{K^{*}} 0$</li>
  <li>$x\prec_K y$ iff $\lambda^Tx &lt; \lambda^Ty$ for all $\lambda \succ_{K^{*}} 0, \lambda\ne0$</li>
</ul>

<p>Since $K=K^{**}$, the dual generalized inequality associated with $\preceq_{K^{*}}$ is $\preceq_K$, so these properties hold if the generalized inequality and its dual are swapped</p>

<h4 id="minimum-and-minimal-elements-via-dual-inequalities">6.3 Minimum and minimal elements via dual inequalities</h4>

<p><strong>dual characterization of minimum element</strong> w.r.t. $\preceq_K$: $X$ is minimum element of $S$ iff for all $\lambda \succ_{K^*}0$, $x$ is the unique minimizer of $\lambda^Tz$ over $z\in S$</p>

<p><strong>dual characterization of minimal element</strong> w.r.t. $\preceq_K$:</p>

<ul>
  <li>if $x$ minimizes $\lambda^Tz$ over $S$ for some $\lambda \succ_{K^*}0$, then $x$ is minimal</li>
  <li>if $x$ is a minimal element of a convex set $S$, then there exists a nonzero $\lambda \succeq_{K^*}0$ such that $x$ minimizes $\lambda^Tz$ over $z \in S$</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Notes on Mathematics for ESL] Chapter 4: Linear Methods for Classification]]></title>
    <link href="http://billy-inn.github.io/blog/2017/10/15/esl-chapter-4/"/>
    <updated>2017-10-15T16:30:27-04:00</updated>
    <id>http://billy-inn.github.io/blog/2017/10/15/esl-chapter-4</id>
    <content type="html"><![CDATA[<h3 id="linear-discriminant-analysis">4.3 Linear Discriminant Analysis</h3>

<h4 id="derivation-of-equation-49">Derivation of Equation (4.9)</h4>

<p>For that each class’s density follows multivariate Gaussian</p>

<script type="math/tex; mode=display">f_k(x) = \frac{1}{(2\pi)^{p/2}\lvert\Sigma\rvert^{1/2}}e^{-\frac12(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)}</script>

<p>Take the logarithm of $f_k(x)$, we get</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\log f_k(x) &= c - \frac12(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) \\
&= c - \frac12(x^T\Sigma^{-1}x-\mu_k^T\Sigma^{-1}x-x^T\Sigma^{-1}\mu_k+\mu_k\Sigma^{-1}\mu_k) \\
&= c - \frac12(x^T\Sigma^{-1}x+\mu_k\Sigma^{-1}\mu_k) + x^T\Sigma^{-1}\mu_k
\end{split} %]]&gt;</script>

<p>where $c = -\log [(2\pi)^{p/2}\lvert\Sigma\rvert^{1/2}]$ and $\mu_k^T\Sigma^{-1}x=x^T\Sigma^{-1}\mu_k$. Following the above formula, we can derive <strong>Equation (4.9)</strong> easily</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\log\frac{\Pr(G=k|X=x)}{\Pr(G=k|X=x)} &=& \log\frac{f_k(x)}{f_l(x)} + \log\frac{\pi_k}{\pi_l} \\
&=& \log\frac{\pi_k}{\pi_l} - \frac12(\mu_k\Sigma^{-1}\mu_k-\mu_l\Sigma^{-1}\mu_l) \\&&+x^T\Sigma^{-1}(\mu_k-\mu_l) \\
&=& \log\frac{\pi_k}{\pi_l} - \frac12(\mu_k+\mu_l)\Sigma^{-1}(\mu_k-\mu_l) \\&&+x^T\Sigma^{-1}(\mu_k-\mu_l) \\
\end{split} %]]&gt;</script>

<!--more-->

<h4 id="notes-on-computations-for-lda">Notes on Computations for LDA</h4>

<p>It’s stated in the book that the LDA classifier can be implemented by the following pair of steps:</p>

<ul>
  <li>Sphere the data with respect to the common covariance estimate $\hat \Sigma:X^*\leftarrow D^{-1/2}U^TX$, where $\hat \Sigma=UDU^T$. The common covariance estimate of $X^*$  will now be the indentity.</li>
  <li>Classify to the closest class centroid in the transformed space, modulo the effect of the class prior probabilities $\pi_k$.</li>
</ul>

<p>However, detailed explanation is not given in the book. Here, I give some skipped mathematical steps which may help the understanding.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\text{Var}(X^*)&=D^{-1/2}U^T\text{Var}(X)(D^{-1/2}U^T)^T \\
&= D^{-1/2}U^TUDU^TUD^{-1/2} \\
&= D^{-1/2}DD^{-1/2} \\
&= I,
\end{split} %]]&gt;</script>

<p>which shows that the covariance estimate of $X^*$ is the identity.</p>

<p>Note that the classification for LDA is based on the linear discriminat functions</p>

<script type="math/tex; mode=display">\delta_k(x)=x^T\Sigma^{-1}\mu_k-\frac12\mu_k^T\Sigma^{-1}\mu_k+\log\pi_k</script>

<p>which is the <strong>Equation (4.10)</strong> in the book. Since the input $x$ is same for each class, so we can add back a term $\frac12x^T\Sigma^{-1}x$ which is cancelled in the previous derivation. Now the functions are turned into:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\delta_k(x)&=-\frac12(x^T\Sigma^{-1}x-2x^T\Sigma^{-1}\mu_k+\mu_k^T\Sigma^{-1}\mu_k)+\log\pi_k \\
&= -\frac12(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) + \log\pi_k
\end{split} %]]&gt;</script>

<p>We know that $\Sigma=I$ in the transformed space, so $\delta_k(x)=-1/2\lVert x-\mu_k\rVert_2+\log\pi_k$. And $\mu_k$ is the centroid for the $k$th class. The claimed method to classify is proved.</p>

<h3 id="logistic-regression">4.4 Logistic Regression</h3>

<h4 id="derivation-of-equation-421-and-422">Derivation of Equation (4.21) and (4.22)</h4>

<p>In the two-class case, $p_1(x;\beta)=p(x;\beta)$ and  $p_2(x;\beta) = 1-p(x;\beta)$ where</p>

<script type="math/tex; mode=display">p(x;\beta)=\frac{e^{\beta^Tx}}{1+e^{\beta^Tx}}.</script>

<p>The <strong>Equation (4.21)</strong> can be derived easily as follows,</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\frac{\partial l(\beta)}{\partial \beta} &= \frac{\partial}{\partial\beta}\sum_{i=1}^N\left\{y_i\beta^Tx_i-\log(1+e^{\beta^Tx_i})\right\}\\
&= \sum_{i=1}^N(y_ix_i-\frac{x_ie^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}) \\
&= \sum_{i=1}x_i(y_i-p(x_i;\beta))
\end{split} %]]&gt;</script>

<p>Note that</p>

<script type="math/tex; mode=display">\frac{\partial p(x;\beta)}{\partial\beta}=xp(x;\beta)(1-p(x;\beta)).</script>

<p>Plug it into <strong>Equation (4.21)</strong>, we get</p>

<script type="math/tex; mode=display">\frac{\partial^2l(\beta)}{\partial\beta\partial\beta^T} = -\sum_{i=1}^Nx_ix_i^Tp(x_i;\beta)(1-p(x_i;\beta)).</script>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Notes on Mathematics for ESL] Chapter 3: Linear Regression Models and Least Squares]]></title>
    <link href="http://billy-inn.github.io/blog/2017/09/27/esl-chapter-3/"/>
    <updated>2017-09-27T21:30:12-04:00</updated>
    <id>http://billy-inn.github.io/blog/2017/09/27/esl-chapter-3</id>
    <content type="html"><![CDATA[<h3 id="linear-regression-models-and-least-squares">3.2 Linear Regression Models and Least Squares</h3>

<h4 id="derivation-of-equation-38">Derivation of Equation (3.8)</h4>

<p>The least squares estimate of $\beta$ is given by the book’s <strong>Equation (3.6)</strong></p>

<script type="math/tex; mode=display">\hat\beta=(X^TX)^{-1}X^T\mathbf{y}.</script>

<p>From the <a href="http://billy-inn.github.io/blog/2017/09/01/esl-chapter-2/">previous post</a>, we know that $\mathrm{E}(\mathbf{y})=X\beta$. As a result, we obtain</p>

<script type="math/tex; mode=display">\mathrm{E}(\hat\beta)=(X^TX)^{-1}X^TX\beta=\beta.</script>

<p>Then, we get</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat\beta-\mathrm{E}(\hat\beta)&=(X^TX)^{-1}X^T(\mathbf{y}-X\beta) \\
&=(X^TX)^{-1}X^T\varepsilon.
\end{split} %]]&gt;</script>

<p>The variance of $\hat \beta$ is computed as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\mathrm{Var}(\hat\beta) &= \mathrm{E}[(\hat\beta-\mathrm{E}(\hat\beta)(\hat\beta-\mathrm{E}(\hat\beta))^T] \\
&= (X^TX)^{-1}X^T\mathrm{Var}(\varepsilon)X(X^TX)^{-1}.
\end{split} %]]&gt;</script>

<p>If we assume that the entries of $\mathbf{y}$ are uncorrelated and all have the same variance of $\sigma^2$, then $\mathrm{Var}(\varepsilon)=\sigma^2I_N$ and the above equation becomes</p>

<script type="math/tex; mode=display">\mathrm{Var}(\hat\beta)=(X^TX)^{-1}\sigma^2.</script>

<p>This completes the derivation of <strong>Equation (3.8)</strong>.</p>

<!--more-->

<h4 id="thoughts-on-equation-312-and-313">Thoughts on Equation (3.12) and (3.13)</h4>

<p>There are a lot concepts of statistics in this part. It’s better to go through Chapter 6 and Chapter 10 in <a href="http://www.stat.cmu.edu/~larry/all-of-statistics/"><em>All of Statistics</em></a> to have a taste about hypothesis tests and confidence intervals.</p>

<p>From my own viewpoint, Z-score and F-statistic give a measure about whether the corresponding features are useful or not. They can be used within some feature selection methods. However, they’re not very useful in practice. The perferred feature selection methods are discussed in <strong>Section 3.3</strong> in the book.</p>

<h4 id="interpretations-of-equation-320-and-322">Interpretations of Equation (3.20) and (3.22)</h4>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\text{MSE}(\tilde\theta) &= \text{E}(\tilde\theta-\theta)^2 \\
&= \text{E}(\tilde\theta-\text{E}(\tilde\theta)+\text{E}(\tilde\theta)-\theta)^2 \\
&= \text{Var}(\tilde\theta)+2(\text{E}(\tilde\theta)-\text{E}(\tilde\theta))(\text{E}(\tilde\theta)-\theta)+(\text{E}(\tilde\theta)-\theta)^2 \\
&= \text{Var}(\tilde\theta)+(\text{E}(\tilde\theta)-\theta)^2
\end{split} %]]&gt;</script>

<p>which completes the derivation of <strong>Equation (3.20)</strong>.</p>

<p><strong>Equation (3.22)</strong> shows that the expected quadratic error can be broken down into two parts as</p>

<script type="math/tex; mode=display">\text{E}(Y_0-\tilde f(x_0))^2=\sigma^2+\text{MSE}(\tilde f(x_0))</script>

<p>The first error component $\sigma^2$ is unrelated to what model is used to describe our data. It cannot be reduced for it exists in the true data generation process. The second source of error corresponding to ther term $\text{MSE}(\tilde f(x_0))$ represents the error in the model and is under control of us. By <strong>Equation (3.20)</strong>, the mean square error can be broken down into two terms: a model variance term and a model bias squared term. How to make these two terms as small as possible while considering the trade-offs between them is the central topic in the book.</p>

<h4 id="notes-on-multiple-regression-from-simple-univariate-regression">Notes on Multiple Regression from Simple Univariate Regression</h4>

<p>The first thing that comes to my mind when I read this section is that why we need this when we already have the ordinary least square (OLS) estimate of $\beta$:</p>

<script type="math/tex; mode=display">\hat \beta = (X^TX)^{-1}X^TY.</script>

<p>It’s because we want to study how to obtain orthogonal inputs instead of correlated inputs, since orthogonal inputs have some nice properties.</p>

<p>Following Algorithm 3.1, we can transform the correlated inputs $\mathbf{x}$ to the orthogonal inputs $\mathbf{z}$. Another view is that we form an orthogonal basis by performing the Gram-Schmidt orthogonilization procedure on $X$’s column vectors and obtain an orthogonal basis $\mathbf{z}_{i=1}^p$. With this basis, linear regression can be done simply as in the univariate case as shown in <strong>Equation (3.28)</strong>:</p>

<script type="math/tex; mode=display">\hat \beta_p=\frac{\langle\mathbf{z}_p, \mathbf{y}\rangle}{\langle\mathbf{z}_p, \mathbf{z}_p\rangle}.</script>

<p>Following this equation, we can derive <strong>Equation (3.29)</strong>:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\text{Var}(\hat\beta_p)&=\text{Var}\left(\frac{z_p^Ty}{\langle z_p, z_p \rangle}\right)=\frac{z_p^T\text{Var}(y)z_p}{\langle z_p, z_p\rangle^2}=\frac{z_p^T(\sigma^2I)z_p}{\langle z_p, z_p\rangle^2} \\
&=\frac{\sigma^2}{\langle z_p, z_p \rangle}.
\end{split} %]]&gt;</script>

<p>We can write the Gram-Schmidt result in matrix form using the QR decomposition as</p>

<script type="math/tex; mode=display">X = QR.</script>

<p>In this decomposition $Q$ is a $N\times(p+1)$ matrix with orthonormal columns and $R$ is a $(p+1)\times(p+1)$ upper triangular matrix. In this representation, the OLS estimate for $\beta$ can be written as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat\beta &= (X^TX)^{-1}X^T\mathbf{y} \\
&= (R^TQ^TQR)^{-1}R^TQ^T\mathbf{y} \\
&= (R^TR)^{-1}R^TQ^T\mathbf{y} \\
&= R^{-1}R^{-T}R^TQ^T\mathbf{y} \\
&= R^{-1}Q^T\mathbf{y}
\end{split} %]]&gt;</script>

<p>which is <strong>Equation (3.32)</strong> in the book. Following this equation, the fitted value $\mathbf{\hat y}$ can be written as</p>

<script type="math/tex; mode=display">\mathbf{\hat y}=X\hat\beta=QRR^{-1}Q^T\mathbf{y}=QQ^T\mathbf{y}</script>

<p>which is <strong>Equation (3.33)</strong> in the book.</p>

<h3 id="shrinkage-methods">3.4 Shrinkage Methods</h3>

<h4 id="notes-on-ridge-regression">Notes on Ridge Regression</h4>

<p>If we compute the singular value decomposition (SVD) of the $N\times p$ centered data matrix $X$ as</p>

<script type="math/tex; mode=display">X=UDV^T,</script>

<p>where $U$ is a $N \times p$ matrix with orthonormal columns that span the column space of $X$, $V$ is a $p \times p$ orthogonal matrix, and $D$ is a $p \times p$ diagonal matrix with elements $d_j$ ordered such that $d_1\ge d_2 \ge \dots \ge d_p \ge 0$. From this representation of $X$ we can derive a simple expression for $X^TX$:</p>

<script type="math/tex; mode=display">X^TX=VDU^TUDV^T=VD^2V^T,</script>

<p>which is the <strong>Equation (3.48)</strong> in the book. Using this expression, we can compute the least squares fitted values as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat y^{ls} = X\hat\beta^{ls} &= X(X^TX)^{-1}X^T\mathbf{y}\\
&= UDV^T(VD^2V^T)^{-1}VDU^T\mathbf{y} \\
&= UDV^T(V^{-T}D^{-2}V^{-1})VDU^T\mathbf{y} \\
&= UU^T\mathbf{y} \\
&= \sum_{j=1}^pu_j(u_j^T\mathbf{y})
\end{split} %]]&gt;</script>

<p>which is the <strong>Equation (3.46)</strong> in the book. Similarly, we can find solutions for ridge regression as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\hat y^{ridge}=X\hat \beta^{ridge}&=X(X^TX+\lambda I)^{-1}X^T\mathbf{y} \\
&= UDV^T(VD^2V^T+\lambda VV^T)^{-1}VDU^T\mathbf{y} \\
&= UD(D^2+\lambda I)^{-1}DU^T\mathbf{y} \\
&= \sum_{j=1}^pu_j\frac{d_j^2}{d_j^2+\lambda}u_j^T\mathbf{y}
\end{split} %]]&gt;</script>

<p>which is the <strong>Equation (3.47)</strong> in the book. Since we can estimate the sample variance by $X^TX/N$, the variance of $\mathbf{z}_1$ can be derived as follows:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\text{Var}(\mathbf{z}_1)=\text{Var}(Xv_1)&= (Xv_1)^T(Xv_1)/N\\
&= v_1^TVD^TU^TUDV^Tv_1/N \\
&= v_1^TVD^2V^Tv_1/N \\
&= \frac{d_1^2}N
\end{split} %]]&gt;</script>

<p>which is the <strong>Equation (3.49)</strong> in the book. Note that $v_1$ is the first column of $V$ and $V$ is orthogonal, so that $V^Tv_1$ is $[1,0, \dots, 0]^T$.</p>

<h4 id="notes-on-degrees-of-freedom-formula-for-lar-and-lasso">Notes on degrees-of-freedom formula for LAR and Lasso</h4>

<p>The degrees-of-freedom of the fitted vector $\mathbf{\hat y}=(\hat y_1, \dots, \hat y_N)$ is defined as</p>

<script type="math/tex; mode=display">\text{df}(\mathbf{\hat y})=\frac1{\sigma^2}\sum_{i=1}^N\text{Cov}(\hat y_i, y_i)</script>

<p>in the book. Also, it’s claimed that $\text{df}(\mathbf{\hat y})$ is $k$ for ordinary least squares regression and $\text{tr}(\mathbf{S}_{\lambda})$ for ridge regresssion without proof in the book. Here, we’ll derive these two expressions. First, we define $e_i$ as a $N$-element vector of all zeros with a one in the $i$th spot. It’s easy to see that $\hat y_i=e_i^T\mathbf{\hat y}$ and $y_i=e_i^T\mathbf{y}$, so that</p>

<script type="math/tex; mode=display">\text{Cov}(\hat y_i, y_i)=\text{Cov}(e_i^T\mathbf{\hat y}, e_i^T\mathbf{y})=e_i^T\text{Cov}(\mathbf{\hat y},\mathbf{y})e_i.</script>

<p>For OLS regression, we have $\mathbf{\hat y}=X(X^TX)^{-1}X^T\mathbf{y}$, so the above expression for $\text{Cov}(\mathbf{\hat y}, \mathbf{y})$ becomes</p>

<script type="math/tex; mode=display">\text{Cov}(\mathbf{\hat y}, \mathbf{y})=X(X^TX)^{-1}X^T\text{Cov}(\mathbf{y}, \mathbf{y})=\sigma^2X(X^TX)^{-1}X^T.</script>

<p>Thus,</p>

<script type="math/tex; mode=display">\text{Cov}(\hat y_i, y_i)=\sigma^2e_i^TX(X^TX)^{-1}X^Te_i=\sigma^2x_i^T(X^TX)^{-1}x_i</script>

<p>where $x_i=X^Te_i$ is the $i$th row of $X$ or $i$th sample’s feature vector. According to the given formula, we get</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
df(\mathbf{\hat y}) &= \sum_{i=1}^N x_i^T(X^TX)^{-1}x_i \\
&= \sum_{i=1}^N\text{tr}(x_i^T(X^TX)^{-1}x_i) \\
&= \sum_{i=1}^N\text{tr}(x_ix_i^T(X^TX)^{-1}) \\
&= \text{tr}\left(\left(\sum_{i=1}^Nx_ix_i^T\right)(X^TX)^{-1}\right).
\end{split} %]]&gt;</script>

<p>If you’re not familar the basic properties of trace, you can refer to <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)#Properties">this page</a>. Note that</p>

<script type="math/tex; mode=display">\sum_{i=1}^Nx_ix_i^T=[x_1\ x_2\ \dots\ x_N]
\begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_N^T\end{bmatrix}=X^TX.</script>

<p>Thus, when there are $k$ predictors we get</p>

<script type="math/tex; mode=display">\text{df}(\mathbf{\hat y})=\text{tr}(I_{k\times k})=k,</script>

<p>the claimed result for OLS in the book. Similarly for ridge regression,</p>

<script type="math/tex; mode=display">\text{Cov}(\mathbf{\hat y}, \mathbf{y})=X(X^TX+\lambda I)^{-1}X^T\text{Cov}(\mathbf{y}, \mathbf{y})=\sigma^2X(X^TX+\lambda I)^{-1}X^T.</script>

<script type="math/tex; mode=display">\text{Cov}(\hat y_i, y_i)=\sigma^2e_i^TX(X^TX+\lambda I)^{-1}X^Te_i=\sigma^2x_i^T(X^TX+\lambda I)^{-1}x_i</script>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
df(\mathbf{\hat y})
&= \sum_{i=1}^N\text{tr}(x_ix_i^T(X^TX+\lambda I)^{-1}) \\
&= \text{tr}(X^TX(X^TX+\lambda I)^{-1}) \\
&= \text{tr}(X(X^TX+\lambda I)^{-1}X^T),
\end{split} %]]&gt;</script>

<p>which is the <strong>Equation (3.50)</strong> in the book.</p>

<h3 id="references">References</h3>

<ul>
  <li><a href="http://billy-inn.github.io/blog/2017/09/01/esl-chapter-2/">Notes on Mathematics for ESL: Chaper 2</a></li>
  <li><a href="http://waxworksmath.com/Authors/G_M/Hastie/hastie.html">Notes on The Elements of Statistical Learning (by John Weatherwax)</a></li>
  <li><a href="http://www.stat.cmu.edu/~larry/all-of-statistics/">All of Statistics (by Larry Wasserman)</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Notes on Mathematics for ESL] Chapter 2: Overview of Supervised Learning]]></title>
    <link href="http://billy-inn.github.io/blog/2017/09/01/esl-chapter-2/"/>
    <updated>2017-09-01T03:12:36-04:00</updated>
    <id>http://billy-inn.github.io/blog/2017/09/01/esl-chapter-2</id>
    <content type="html"><![CDATA[<h3 id="statistical-decision-theory">2.4 Statistical Decision Theory</h3>

<h4 id="derivation-of-equation-216">Derivation of Equation (2.16)</h4>

<p>The expected predicted error (EPE) under the squared error loss:</p>

<script type="math/tex; mode=display">\mathrm{EPE}(\beta) = \int (y-x^T\beta)^2\Pr(dx, dy).</script>

<p>Taking derivatives with respect to $\beta$:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\frac{\partial\mathrm{EFE}}{\partial\beta}&=-2\int(y-x^T\beta)x\Pr(dx, dy). \\
&= -2(E[yx]-E[xx^T\beta])
\end{split} %]]&gt;</script>

<p>In order to minimize the EFE, we make derivatives equal zero which gives <strong>Equation (2.16)</strong>:</p>

<script type="math/tex; mode=display">\beta=E[xx^T]^{-1}E[yx].</script>

<p><em>Note: $x^T\beta$ is a scalar, and $\beta$ is a constant.</em></p>

<!--more-->

<h3 id="local-methods-in-high-dimensions">2.5 Local Methods in High Dimensions</h3>

<h4 id="intuition-on-equation-224">Intuition on Equation (2.24)</h4>

<p>There are $N$ $p$-dimensional data point $x_1,\dots, x_N$, that is, $N\times p$ dimensions in total. Let $r_i=\Vert x_i \Vert$. Without loss of generality, we assume that $A &lt; r_1 &lt; \dots &lt; r_n &lt; 1$. Let $U(A)$ be the region of all possible sampled data which meet the assumptation:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
U(A) = \int_{A<r_1<\dots<r_n<1}dx_1\dots dx_N. %]]&gt;</script>

<p>The goal is to find $A$ such that $U(A)=\frac12U(0)$. It turns out to be a integration problem on a $N \times p$ dimensional space.</p>

<p>With some mathematical techniques (which make me overwhelmed), we can get $U(A)=(1-A^p)^N$. Then $U(0)=1$. Solving $(1-A^p)^N=1/2$, we obtain <strong>Equation (2.24)</strong>:</p>

<script type="math/tex; mode=display">A=\left(1-2^{-\frac1N}\right)^{\frac1p}.</script>

<h4 id="derivation-of-equation-227-and-228">Derivation of Equation (2.27) and (2.28)</h4>

<p>The variation is over all training sets $\mathcal{T}$, and over all values of $y_0$, while keeping $x_0$ fixed. Note that $x_0$ and $y_0$ are chosen independently of $\mathcal{T}$ and so the expectations commute: 
$\mathrm{E}_{y_0\vert x_0}\mathrm{E}_{\mathcal{T}}=\mathrm{E}_{\mathcal{T}}\mathrm{E}_{y_0 \vert x_0}$.
Also $\mathrm{E}_\mathcal{T}=\mathrm{E}_\mathcal{X}\mathrm{E}_{\mathcal{Y \vert X}}$.</p>

<p>In order to make the derivation more comprehensible, here lists some definitions:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
y_0 &= x_0^T\beta + \varepsilon \\
\hat y_0 &= x_0^T\beta + \sum_{i=1}^Nl_i(x_0)\varepsilon_i \\
\mathrm{E}_\mathcal{T}(\hat y_0) &= x_0^T\beta + \mathrm{E}_{\mathcal{T}}((X^TX)^{-1}Xx_0\varepsilon) \\
&= x_0^T\beta+x_0^T\mathrm{E}_\mathcal{X}((X^TX)^{-1}X^T\mathrm{E}_{\mathcal{Y|X}}\varepsilon) \\
&=x_0^T\beta
\end{split} %]]&gt;</script>

<p>$y_0-\hat y_0$ can be written as the sum of three terms:</p>

<script type="math/tex; mode=display">(y_0-x_0^T\beta)-(\hat y_0-\mathrm{E}_\mathcal{T}(\hat y_0))-(\mathrm{E}_\mathcal{T}(\hat y_0)-x_0^T\beta)=U_1-U_2-U_3</script>

<p>Following above definitions, we have $U_1=\varepsilon$, $U_3=0$. In addition, clearly we have $\mathrm{E}_\mathcal{T}U_2=0$. When squaring $U_1-U_2-U_3$, we can eliminate all three cross terms and one squared terms $U_3^2$.</p>

<p>Following the definition of variance, we have: $\mathrm{E}_{y_0\vert x_0}\mathrm{E}_\mathcal{T}U_1^2=\mathrm{Var}(\varepsilon)=\sigma^2$ and $\mathrm{E}_\mathcal{T}(\hat y_0 - \mathrm{E}_\mathcal{T}\hat y_0)^2=\mathrm{Var}_\mathcal{T}(\hat y_0)$.</p>

<p>Since $U_2=\sum_{i=1}^Nl_i(x_0)\varepsilon_i$, we have $\mathrm{Var}_\mathcal{T}(\hat y_0)=\mathrm{E}_\mathcal{T}U_2^2$ as</p>

<script type="math/tex; mode=display">\mathrm{E}_\mathcal{T}(x_0^T(X_TX)^{-1}X^T\varepsilon\varepsilon^TX(X^TX)^{-1}x_0).</script>

<p>Since $\mathrm{E}_\mathcal{T}\varepsilon\varepsilon^T=\sigma^2I_N$, this is equal to $\mathrm{E}_\mathcal{T}x_0(X^TX)^{-1}x_0\sigma^2$. This completes the derivation of <strong>Equation (2.27)</strong>.</p>

<p>Under the conditions stated by the authors, $X^TX/N$ is then approximately equal to $\mathrm{Cov}(X)=\mathrm{Cov}(x_0)$. Applying $\mathrm{E}_{x_0}$ to $\mathrm{E}_\mathcal{T}x_0(X^TX)^{-1}x_0\sigma^2$, we obtain (approximately)</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\sigma^2\mathrm{E}_{x_0}(x_0^T\mathrm{Cov}(X)^{-1}x_0)/N &= \sigma^2\mathrm{E}_{x_0}(\mathrm{trace}(x_0^T\mathrm{Cov}(X)^{-1}x_0))/N \\
&= \sigma^2\mathrm{E}_{x_0}(\mathrm{trace}(\mathrm{Cov}(X)^{-1}x_0x_0^T))/N \\
&= \sigma^2\mathrm{trace}(\mathrm{Cov}(X)^{-1}\mathrm{C ov}(x_0))/N \\
&= \sigma^2\mathrm{trace}(I_p)/N \\
&= \sigma^2p/N.
\end{split} %]]&gt;</script>

<p>This completes the derivation of <strong>Equation (2.28)</strong>.</p>

<h4 id="references">References</h4>

<ul>
  <li><a href="http://waxworksmath.com/Authors/G_M/Hastie/hastie.html">Notes on The Elements of Statistical Learning (by John Weatherwax)</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[统计释疑(3)：大数定理和中心极限定理]]></title>
    <link href="http://billy-inn.github.io/blog/2017/08/18/lln-and-clt/"/>
    <updated>2017-08-18T01:49:29-04:00</updated>
    <id>http://billy-inn.github.io/blog/2017/08/18/lln-and-clt</id>
    <content type="html"><![CDATA[<p>两个必须得记住并理解的统计学定理：大数定理和中心极限定理。有相当多的统计学理论是以这两个定理为基础。另外从我个人理解，这两个定理在一定程度上解释了为什么数据越多越好（为什么我们需要大数据）。</p>

<h3 id="section">收敛的类型</h3>

<p>为了更加准确的理解上述两个定理，我们需要理解概率层面的收敛，而非微积分里的收敛（如果对与任意$\epsilon&gt;0$和足够大的$n$，$\vert x_n -x \rvert &lt; \epsilon$，那么我们称这一实数列$x_n$收敛于极限$x$）。</p>

<p>在统计中，主要有两种类型的收敛：</p>

<!--more-->

<blockquote>
  <p>令$X_1,X_2,\dots$为一系列随机变量并令$X$为另一个随机变量。令$F_n$表示$X_n$的概率密度函数 (CDF)，$F$表示$X$的概率密度函数。
1) $X_n$在概率上收敛于$X$ (converges in probability)，写作$X_n\xrightarrow[]{P} X$，如果对于任意$\epsilon&gt;0$</p>
</blockquote>

<script type="math/tex; mode=display">\mathbb{P}(\vert X_n - X\vert>\epsilon) \rightarrow 0</script>

<blockquote>
  <p>当$n\rightarrow\infty$。
2) $X_n$在分布上收敛于$X$ (converges in distribution), 写作$X_n\rightsquigarrow X$，如果对于任意在$F$中连续的点$t$，</p>
</blockquote>

<script type="math/tex; mode=display">\lim_{n\rightarrow\infty}F_n(t)=F(t)</script>

<p>另外由$X_n\xrightarrow[]{P}X$可以推出$X_n\rightsquigarrow X$。</p>

<p>P.S. 其实还有另外两种类型的收敛，他们之间的关系也更加复杂，这里的重点是介绍两个定理，所以这部分从简，想深入了解可参考相关教材。</p>

<h3 id="the-law-of-large-numbers">大数定理 (The Law of Large Numbers)</h3>

<p>令$X_1,X_2\dots$为独立同分布 (IID) 的样本，令$\mu=\mathbb{E}(X_1)$，$\sigma^2=\mathbb{V}(X_1)$。另外$\bar X_n = n^{-1}\sum_{i=1}^nX_i$为样本均值并且$\mathbb{E}(\bar X_n)=\mu$，$\mathbb{V}(\bar X_n)=\sigma^2/n$。</p>

<blockquote>
  <p><strong>弱大数定理(WLLN)</strong>：如果$X_1,\dots, X_n$为独立同分布，那么$\bar X_n \xrightarrow[]{P} \mu$。</p>
</blockquote>

<p>理解：当$n$越来越大时，$\bar X_n$的分布变得越来越聚集于$\mu$附近。</p>

<h3 id="the-central-limit-theorem">中心极限定理 (The Central Limit Theorem)</h3>

<blockquote>
  <p><strong>中心极限定理(CLT)</strong>：如果$X_1,\dots,X_n$为独立同分布，那么</p>
</blockquote>

<script type="math/tex; mode=display">Z_n=\frac{\sqrt{n}(\bar X_n - \mu)}{\sigma} \rightsquigarrow Z</script>

<blockquote>
  <p>其中$Z \sim N(0,1)$，即正态分布。</p>
</blockquote>

<p>理解：关于$\bar X_n$的概率表达式可以近似于正态分布。注意并不是随机变量本身近似于正态分布。</p>

<p>然而大多数时候我们并不知道$\sigma$，实际中我们可以用标准差$S_n^2=\frac1{n-1}\sum_{i=1}^n(X_i-\bar X_n)^2$来代替$\sigma$。</p>

<blockquote>
  <p><strong>定理</strong>：在与CLT相同的条件下，</p>
</blockquote>

<script type="math/tex; mode=display">\frac{\sqrt{n}(\bar X_n - \mu)}{S_n} \rightsquigarrow N(0,1)</script>

<h3 id="delta-the-delta-method">Delta方法 (The Delta Method)</h3>

<p>为了能更加广泛有效的利用中心极限定理，掌握Delta方法是相当有必要的。</p>

<blockquote>
  <p><strong>Delta方法</strong>：</p>
</blockquote>

<script type="math/tex; mode=display">Y\approx N\left(\mu,\frac{\mu^2}{n}\right) \Rightarrow g(Y_n) \approx N\left(g(\mu),(g'(\mu))^2\frac{\sigma^2}n\right)</script>

<blockquote>
  <p>其中$g$是一个可导函数从而$g’(\mu)\ne0$。</p>
</blockquote>

<p>P.S. 强大数定理，多元中心极限定理及多元Delta方法由于比较复杂就省略了。另外由于比较懒，所以有助于理解的例子也没用写，纯粹当是记录一下学习的过程了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[统计释疑(2)：概率不等式有什么用？]]></title>
    <link href="http://billy-inn.github.io/blog/2017/08/10/probability-inequalities/"/>
    <updated>2017-08-10T00:38:45-04:00</updated>
    <id>http://billy-inn.github.io/blog/2017/08/10/probability-inequalities</id>
    <content type="html"><![CDATA[<p>在学习概率论或者一些统计课程的时候，往往会学到一系列各式各样稀奇古怪的不等式 (Inequalities)，然而却对于这些不等式的意义缺乏一个直观的认识。引申<em>“All of Statistics”</em>一书中的一个小例子可以给出一个很切合实际的解释。</p>

<h3 id="section">我的神经网络真的有效吗？</h3>

<p>假如我们用神经网络在MNIST数据集上训练了一个分类器，我们在测试集上得到了一个错误率，比如$0.05$。那么这是否意味着我们可以保证我们的神经网络一定能达到$95\%$的正确率呢？显然训练一次得出的结果是不可靠的。那么，我们有多大的把握（概率）来相信这一观察到的错误率呢？</p>

<p>这时候就需要一些统计的语言了：假设我们有$n$个测试样本，每个测试样本分类的正确与否都是一个随机变量$X_1,\dots,X_n$。如果分类错误$X_i=1$，否则$X_i=0$。显而易见，$\bar X_n=n^{-1}\sum_{i=1}^nX_i$就是观察到的错误率。我们可以把每个$X_i$当做一个均值为$p$服从Bernoulli分布的随机变量，从而$p$就是真正（但是永远无法准确知晓）的错误率。从我们的角度来看，我们希望$\bar X_n$应该接近$p$。那么$\bar X_n$和$p$的概率超过一个固定值$\epsilon$的概率有多大呢？
这个概率就是$\mathbb{P}(\lvert\bar X_n -p\rvert &gt; \epsilon)$，通常我们很难直接计算出它的值，这时我们就需要不等式来给这个概率设定一些边界 (bound)。</p>

<!--more-->

<h3 id="section-1">尝试用不等式给出估计</h3>

<p>我们的第一个不等式就是马尔科夫不等式 (Markov’s inequality)：</p>

<blockquote>
  <p>令$X$为一个非负随机变量并假设$\mathbb{E}(X)$存在。对任何$t&gt;0$，</p>
</blockquote>

<script type="math/tex; mode=display">\mathbb{P}(X>t) \le \frac{\mathbb{E}(X)}t。</script>

<p>咋一看，我们似乎无法直接运用马尔科夫不等式来限定$\mathbb{P}(\lvert\bar X_n -p\rvert &gt; \epsilon)$的值。但其实只要稍做转换，便可得到另一个可以直接使用的不等式，即切比雪夫不等式 (Chebyshev’s inequality)：</p>

<blockquote>
  <p>令$\mu = \mathbb{E}(X)$和$\sigma^2=\mathbb{V}(X)$，从而</p>
</blockquote>

<script type="math/tex; mode=display">\mathbb{P}(\lvert X-\mu\rvert\ge t)\le\frac{\sigma^2}{t^2}。</script>

<p>这个不等式可以直接从马尔科夫不等式得出：</p>

<script type="math/tex; mode=display">\mathbb{P}(|X-\mu| \ge t)=\mathbb{P}(|X-\mu|^2\ge t^2)
\le\frac{\mathbb{E}(X-\mu)^2}{t^2}=\frac{\sigma^2}{t^2}。</script>

<p>由于$X_i$服从Bernoulli分布，所以$\mathbb{V}(\bar X_n)=\mathbb{V}(X_i)/n=p(1-p)/n$，从而</p>

<script type="math/tex; mode=display">\mathbb{P}(|\bar X_n -p|>\epsilon)\le\frac{\mathbb{V}(\bar X_n)}{\epsilon^2}=\frac{p(1-p)}{n\epsilon^2}\le\frac1{4n\epsilon^2}</script>

<p>注意对任意$0&lt;p&lt;1$，$p(1-p)\le\frac14$。如果我们希望神经网络的真实错误率与观察到的错误率之间的误差超过$\epsilon=0.05$的概率不超过$0.05$，那么通过简单的计算可得我们需要大约$n=2000$个测试样本。怎么样，是不是觉得不等式变得有用了？</p>

<h3 id="section-2">一个更贴近的估计？</h3>

<p>切比雪夫不等式只是一个相对粗略的估算，其实还有各种更为精确的不等式。当然，随之然来的是各种各样的限制条件，这里给出一个更精确的霍夫丁不等式 (Hoeffding’s inequality)：</p>

<blockquote>
  <p>令$X_1\dots X_n \sim \mathrm{Bernoulli}(p)$，对任意$\epsilon&gt;0$，</p>
</blockquote>

<script type="math/tex; mode=display">P(\lvert\bar X_n - p\rvert \ge \epsilon) \le 2e^{-2n\epsilon^2}</script>

<blockquote>
  <p>其中$\bar X_n = n^{-1}\sum_{i=1}^nX_i$。</p>
</blockquote>

<p>通过上面的不等式，经过计算发现其实我们只需要$738$个测试样本就足够了。</p>

<p>P.S. 上面的Hoeffding’s inequality只是针对Bernoulli变量的特殊形式，完整的不等式可以参考Wikipedia或相关教材。</p>

<h3 id="section-3">小结</h3>

<p>经过一个小例子，我们对不等式的作用有了一个直观的认识。但不等式真正的用武之地是在各种推导证明之中的，虽然我看到那种满篇公式、各种bound来bound去的paper都是自动略过的，而且现在在做的东西也是偏应用层面。但谁知道将来在研究中会不会经常用到呢，至少，现在我学会了估算可靠的测试样本大小的方法。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[统计释疑(1)：什么是p值]]></title>
    <link href="http://billy-inn.github.io/blog/2017/07/28/p-value/"/>
    <updated>2017-07-28T22:33:16-04:00</updated>
    <id>http://billy-inn.github.io/blog/2017/07/28/p-value</id>
    <content type="html"><![CDATA[<p>某互联网公司招聘程序员，招聘的方法很简单，就是从LeetCode上找$10$道题，录取解出至少$8$道题的应试者。每隔一年，公司会根据新招聘程序员的表现评估上一年招聘的不合格率。公司的期望是每年招聘的不和格率要低于$5\%$。下面是历年的招聘数据：</p>

<table class="mbtablestyle">
  <thead>
    <tr>
      <th style="text-align: left">年份</th>
      <th style="text-align: left">面试人数</th>
      <th style="text-align: left">录取人数</th>
      <th style="text-align: left">不合格的人数</th>
      <th style="text-align: left">不合格率</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">2014</td>
      <td style="text-align: left">1000</td>
      <td style="text-align: left">350</td>
      <td style="text-align: left">30</td>
      <td style="text-align: left">8.57%</td>
    </tr>
    <tr>
      <td style="text-align: left">2015</td>
      <td style="text-align: left">1000</td>
      <td style="text-align: left">650</td>
      <td style="text-align: left">10</td>
      <td style="text-align: left">1.54%</td>
    </tr>
    <tr>
      <td style="text-align: left">2016</td>
      <td style="text-align: left">1000</td>
      <td style="text-align: left">200</td>
      <td style="text-align: left">10</td>
      <td style="text-align: left">5.00%</td>
    </tr>
  </tbody>
</table>

<p>那么这家公司的招聘策略是否有效呢？</p>

<!--more-->

<h3 id="hypothesis-testing">假设检验 (Hypothesis Testing)</h3>

<p>显而易见，公司的招聘标准是根据解出题目的多寡来判断应试者是否合格。用统计的语言的说，就是每个应试者录取与否是一个随机变量 (Random Variable) $X_i$, 其样本空间 (Sample Space) 是${0, 1}$，其中$0$代表不合格，$1$代表合格。而这些随机变量均服从于一个由最少解出题数$\theta$决定的概率分布$p(x;\theta)$。用数学的语言就是$X_1,\dots,X_n \sim p(x;\theta)$。公司策略所假设的概率分布是理想化的（不切实际的），对于给定$\theta=\theta_0$, $p(x;\theta_0)$的概率分布可以用下表表示：</p>

<table class="mbtablestyle">
  <thead>
    <tr>
      <th style="text-align: left">$p(x;\theta)$</th>
      <th style="text-align: left">$x=0$</th>
      <th style="text-align: left">$x=1$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">$\theta&lt;\theta_0$</td>
      <td style="text-align: left">1</td>
      <td style="text-align: left">0</td>
    </tr>
    <tr>
      <td style="text-align: left">$\theta\ge\theta_0$</td>
      <td style="text-align: left">0</td>
      <td style="text-align: left">1</td>
    </tr>
  </tbody>
</table>

<p>为了判断招聘策略（假设概率分布$p$)的有效性，我们需要针对概率分布的参数，即最少解出题数$\theta$进行假设。首先，我们提出一个空假设 (Null Hypothesis) $H_0: \theta=\theta_0$和一个替代假设 (Alternative Hypothesis) $H_1: \theta \ne \theta_0$。而假设检验需要考虑的问题并非$H_0$是对是错，而是我们是否有足够的证据来证明$H_0$是错的。</p>

<h3 id="section">显著性水平和检验力</h3>

<p>那么去哪里找证据呢？当然是观察实际数据啦。每当我们观察到一组数据时，我们需要确定一些指标来支撑我们的判断。对于公司招聘来说，最直观的指标就是不合格率$1-\bar X$了。这里我们需要设定一个标准来决定什么时候来拒绝$H_0$，即我们的最低期望，比方说不合格率高于$c$就拒绝$H_0$。</p>

<p>所谓检验力$\beta(\theta)$，就是在给定$\theta$下拒绝$H_0$的概率。针对招聘问题，就是$\beta(\theta)=P_\theta(1-\bar X &gt;c)$。而显著性水平$\alpha$就是在$H_0$成立的条件下，允许的$\beta(\theta)$的最大值。有点绕是不是？总结一下，其实$\alpha$就是“当前招聘策略下，不合格率高于$c$的最大概率”。由于在招聘问题的$H_0$下$\theta$只有一个取值$\theta_0$，所以$\alpha=\beta(\theta_0)$。需要注意的是，$\alpha$是由$c$决定的，即提前人为设定的。</p>

<h3 id="p-p-value">P值 (p-value)</h3>

<p>P值$p$是我们会拒绝$H_0$时能接受最小的$\alpha$，换言之，当$\alpha &gt; p$时，我们便会拒绝$H_0$。针对招聘问题，如果我们希望不合格率不得高于$5\%$，即$c=0.05$，$\alpha=\beta(\theta_0)=P_{\theta_0}(1-\bar X &gt; 0.05)$。当不合格率高于$5\%$的概率高于$p$时，就认为当前的招聘策略无效。所以，$p$越小，证明$H_0$是错的证据就需要越有力。正因如此，才在学界有了“$p$值为$0.05$，即可将统计结果视为显著”这样的规则。当然，不要因此而误认为$p=P(H_0)$，即招聘策略有效 ($H_0$正确) 的概率。</p>

<p>关于P值，还有一个很不靠谱的特征：那就是当$H_0$实际上是正确的时候，$p$服从$0-1$均匀分布。也就是说，$p$值即使很小也并一定意味着$H_0$一定是错的，而有可能只是碰巧发生的。还真是够不靠谱的，无怪乎会被各种吐槽。</p>

<p>最后，回归到招聘策略是否有效的问题。根据$P_{\theta_0}(1-\bar X &gt; 0.05)$，$\alpha=0.333$，然后由于只有三条观察数据，$p$值在这个问题上并没有太大的参考价值。对于其他很多问题来说，同样也是如此。总而言之就是P值虽然被广泛使用，然后大多数情况下并没有什么卵用。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【失控:机器、社会与经济的新生物学】漫谈（二）]]></title>
    <link href="http://billy-inn.github.io/blog/2017/01/11/lose-control-2/"/>
    <updated>2017-01-11T18:13:46-05:00</updated>
    <id>http://billy-inn.github.io/blog/2017/01/11/lose-control-2</id>
    <content type="html"><![CDATA[<p>随着人工智能大潮的火热，各种噱头在大公司和媒体的鼓吹下刺激着大众的神经。关于智械的各种“浪漫”幻想也不再仅仅诉诸于电影和小说，而是开始被严肃的讨论了起来。在这本书里，我看到了一个之前没有见过的有趣的观点：“机器是人类的一种进化形式”。显然这并非通常的经过亿万年自然选择产生的进化，而是一种定向的选择。就如同培育有机食品、杂交水稻一般，人工智能是否也可以定向的让人类自身变得更为强大呢？相比于制造通用智能，越来越多业界的人也都认为AI被定义为Augmented Intelligence（增强智能）而非Artifical Intelligence（人工智能）更加实际和贴切。总的来说，现在的AI还是高度面向任务的，大量人工标注的数据加上工程上的细节才能使计算机在特定任务上战胜人类，而这些努力一旦换了一个领域就难再有大的用武之地。另一方面，希望计算机或者机器人在某些领域完全取代人类也是不现实的，除了一些很基础的任务外，人类的介入在现阶段还是很有必要的，比如机器翻译，智能助理等等。</p>

<!--more-->

<p>另外，创造智械还会带来一系列现实问题，诸如道德，法律，就业等等问题。就比如无人车车祸的责任判定，机器取代工人而引发的大量失业等等。</p>

<p>总而言之，与其创造一个与人类等价的新族类“智械”，不如思考如何利用人工智能更好的辅助人类，定向地将人类进化的更加强大。相信这是在可预见的未来，人工智能领域的一个趋势。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【失控: 机器、社会与经济的新生物学】漫谈（一）]]></title>
    <link href="http://billy-inn.github.io/blog/2016/12/14/lose-control-1/"/>
    <updated>2016-12-14T23:44:01-05:00</updated>
    <id>http://billy-inn.github.io/blog/2016/12/14/lose-control-1</id>
    <content type="html"><![CDATA[<p>重拾阅读后的第一本书，读而不思则罔，于是决定随着阅读随便写点什么，谓之“漫谈”。</p>

<p>虽然成书于94年，但书中种种观点和当今社会与技术的发展却有诸多不谋而合之处。读罢前几章，最为深刻的印象就是蜂群思维，分布式系统，去中心化等等一系列相关的概念。总而言之，论述的是一种与传统自上而下的系统相悖的自下而上的系统。这里的系统是一个非常宽泛的说法，它可以是机器，可以是软件，也可以是动物，乃至于人类社会、政治体系、万维网等等。可能是我实在是孤陋寡闻，在阅读这本书之前，我潜意识里确实认为绝大多数系统都应该有一个中心，拥有绝对的权威并下达指令，比如PC的CPU，古代的皇帝，人的大脑等等。然而这本书却提出了一个截然不同的系统，简而言之就是没有一个绝对的中心，每一个个体的行为决定了整体的行动（也可以简单的理解为少数服从多数，但实际情况往往更加复杂）并由简单的行为（操作）逐层向上模块化的增加更加复杂的行为（操作），另外分布式的存在方式令其拥有更强的容错性和在部分失灵的情况下能够继续运转的稳定性。现代科学在群体动物（蜂群、蚁群）中发现了这样的系统，而脑科学的发展也说明大脑并非我们原本想象的那样控制着人体的一切。无数的神经的共同作用造就了大脑，而大脑与身体的各种感官也似乎并非简单的从属关系。</p>

<!--more-->

<p>这样的系统的优越性早已在工业界得到了印证。我也在这记录一下我在阅读过程中所联想到的各种相干（或者不相干？）的点点吧：</p>

<ul>
  <li>Ensemble Learning的motivation似乎和蜂群思维不谋而合。</li>
  <li>神经网络也很好的体现了这个系统的特征，比如mixture of experts，dropout等等技巧的成功应用。</li>
  <li>算法的泛化能力 = 系统的容错能力？</li>
  <li>MXNet的开发就在强调去中心化和模块化，具体进展如何，拭目以待。不过我暂时还是出于易用性考虑继续站tensorflow。</li>
  <li>社交网络时代下的我们就是群氓中的一份子，个性化推荐加剧了信息不对称。</li>
  <li>极端的民主并不利于文明的发展，强大的容错性的代价是无意义的徘徊和拉锯战，正如通过大量重复计算来换取更强的泛化能力。美国的大选或许就是个例子。</li>
</ul>

<p>微博上看到有人喷这本书的作者就是个大忽悠，对于作者现在到哪儿去演讲、圈钱、布道什么的不做评价。但上个世纪的书对当下仍有现实启发意义，我觉得这本书还是值得一读的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Reinforcement Learning (4): Temporal-Difference Learning]]></title>
    <link href="http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning/"/>
    <updated>2016-10-16T19:47:27-04:00</updated>
    <id>http://billy-inn.github.io/blog/2016/10/16/notes-on-reinforcement-learning-4-temporal-difference-learning</id>
    <content type="html"><![CDATA[<p>Temporal-difference (TD) learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.</p>

<h3 id="td-prediction">TD Prediction</h3>

<p>Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\pi$, both methods update their estimate $v$ of $v_\pi$ for the nonterminal states $S_t$ occurring in that experience. Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need wait only until the next time step. The simplest TD method, known as TD(0), is</p>

<script type="math/tex; mode=display">V(S_t) = V(S_t) + \alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)].</script>

<p>TD methods combine the sampling of Monte Carlo with the bootstrapping of DP. As we shall see, with care and imagination this can take us a long way toward obtaining the advantages of both Monte Carlo and DP methods.</p>

<p><img src="http://billy-inn.github.io/images/rl4.1.png" alt="Alt text" /></p>

<!--more-->

<p>Note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_t+\gamma V(S_{t+1})$. This quantity, called the TD error, arises in various forms throughout reinforcement learning:</p>

<script type="math/tex; mode=display">\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t).</script>

<p>Also note that the Monte Carlo error can be written as a sum of TD errors:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
G_t - V(s_t) &= R_{t+1}+\gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1}) \\
&= \delta_t + \gamma (G_{t+1}-V(S_{t+1})) \\
&= \delta_t + \gamma \delta_{t+1} + \dots + \gamma^{T-t} (G_T - V(S_T))\\
&= \sum_{k=0}^{T-t-1}\gamma^k\delta_{t+k}
\end{split} %]]&gt;</script>

<p>This fact and its generalizations play important roles in the theory of TD learning.</p>

<h3 id="optimality-of-td0">Optimality of TD(0)</h3>

<p>Suppose there is available only a finite amount of experience, say 10 episodes or 100 time steps. In this case, a common approach with incremental learning methods is to present the experience repeatedly until the method converges upon an answer. Updates are made only after processing each complete batch of training data. We call this batch updating.</p>

<p>Batch Monte Carlo methods always find the estimates that minimize mean-squared error on the training set, whereas batch TD(0) always finds the estimates that would be exactly correct for the maximum-likelihood model of the Markov process. In this case, the maximum-likelihood estimate is the model of the Markov process formed in the obvious way from the observed episodes: the estimated transition probability from $i$ to $j$ is the fraction of observed transitions from $i$ that went to $j$, and the associated expected reward is the average of the rewards observed on those transitions. Given this model, we can compute the estimate of the value function that would be exactly correct if the model were exactly correct. This is called the certainty-equivalence estimate because it is equivalent to assuming that the estimate of the underlying process was known with certainty rather than being approximated. In general, batch TD(0) converges to the certainty-equivalence estimate.</p>

<h3 id="sarsa-on-policy-td-control">Sarsa: On-Policy TD Control</h3>

<p>As usual, we follow the pattern of generalized policy iteration (GPI), only this time using TD methods for the evaluation or prediction part.</p>

<p>In the previous section we considered transitions from state to state and learned the values of states. Now we consider transitions from state–action pair to state–action pair, and learn the values of state–action pairs. The theorems assuring the convergence of state values under TD(0) also apply to the corresponding algorithm for action values:</p>

<script type="math/tex; mode=display">Q(S_t,A_t)=Q(S_t,A_t)+\alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t+1}) - Q(S_t, A_t)].</script>

<p><img src="http://billy-inn.github.io/images/rl4.2.png" alt="Alt text" /></p>

<h3 id="q-learning-off-policy-td-control">Q-learning: Off-Policy TD Control</h3>

<p>One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as Q-learning, defined by:</p>

<script type="math/tex; mode=display">Q(S_t, A_t)=Q(S_t, A_t)+\alpha[R_{t+1}+\gamma\max_aQ(S_{t+1},a)-Q(S_t,A_t)]</script>

<p>In this case, the learned action-value function, $Q$, directly approximates $q^*$, the optimal action-value function, independent of the policy being followed.</p>

<p><img src="http://billy-inn.github.io/images/rl4.3.png" alt="Alt text" /></p>

<h3 id="expected-sarsa">Expected Sarsa</h3>

<p>Consider the algorithm with the update rule:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
Q(S_t,A_t)&=Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma \mathbb{E}[Q(S_{t+1},A_{t+1})|S_{t+1}]-Q(S_t,A_t)\right]\\
&=Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma\sum_a\pi(a|S_{t+1})Q(S_{t+1},a)-Q(S_t,A_t)\right]
\end{split} %]]&gt;</script>

<p>but that otherwise follows the schema of Q-learning. Given the next state $S_{t+1}$, this algorithm moves deterministically in the same direction as Sarsa moves in expectation, and accordingly it is called expected Sarsa.</p>

<p><img src="http://billy-inn.github.io/images/rl4.4.png" alt="Alt text" /></p>

<h3 id="maximization-bias-and-double-learning">Maximization Bias and Double Learning</h3>

<p>All the control algorithms that we have discussed so far involve maximization in the construction of their target policies. In these algorithms, a maximum over estimated values is used implicitly as an estimate of the maximum value, which can lead to a significant positive bias. We call this maximization bias.</p>

<p>One way to view the problem is that it is due to using the same samples (plays) both to determine the maximizing action and to estimate its value. Suppose we divided the plays in two sets and used them to learn two independent estimates, call them $Q_1(a)$ and $Q_2(a)$, each an estimate of the true value $q(a)$, for all $a\in\mathcal{A}$. We could then use one estimate, say $Q_1$, 
to determine the maximizing action $A^*=\mathrm{argmax}_aQ_1(a)$, and the other, $Q_2$, to provide the estimate of its value,
$Q_2(A^*)=Q_2(\mathrm{argmax}_aQ_1(a))$. This estimate will then be unbiased in the sense that 
$\mathbb{E}[Q_2(A^*)]=q(A^*)$. We can also repeat the process with the role of the two estimates reversed to yield a second unbiased estimate 
$Q_1(A^*)=Q_1(\mathrm{argmax}_aQ_2(a))$. This is the idea of doubled learning.</p>

<p><img src="http://billy-inn.github.io/images/rl4.5.png" alt="Alt text" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Reinforcement Learning (3): Monte Carlo Methods]]></title>
    <link href="http://billy-inn.github.io/blog/2016/10/14/notes-on-reinforcement-learning-3-monte-carlo-methods/"/>
    <updated>2016-10-14T18:07:35-04:00</updated>
    <id>http://billy-inn.github.io/blog/2016/10/14/notes-on-reinforcement-learning-3-monte-carlo-methods</id>
    <content type="html"><![CDATA[<p>Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns. To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks.</p>

<h3 id="monte-carlo-prediction">Monte Carlo Prediction</h3>

<p>An obvious way to estimate the state-value function which is the expected return starting from that state from experience, is to average the returns observed after visits to that state. This idea underlies all Monte Carlo methods.</p>

<p>In particular, suppose we wish to estimate $v_\pi(s)$, the value of a state $s$ under policy $\pi$, given a set of episodes obtained by following $\pi$ and passing through $s$. Each occurrence of state $s$ in an episode is called a visit to $s$. The first-visit MC method estimates $v_\pi(s)$ as the average of the returns following from first visits to $s$, whereas every-visit MC method averages the returns following all visits to $s$.</p>

<p><img src="http://billy-inn.github.io/images/rl3.1.png" alt="Alt text" /></p>

<!--more-->

<p>An important fact about Monte Carlo method methods is that the estimates for each state are independent. The estimate for one state does not build upon the estimate of any other state, as is the case in DP.</p>

<h3 id="monte-carlo-estimation-of-action-values">Monte Carlo Estimation of Action Values</h3>

<p>If a model is not available, then it is particularly useful to estimate action values rather than state values. The Monte Carlo methods for this this are essentially the same as just presented for state values, except now we talk about visits to a state–action pair rather than to a state. A state–action pair $s, a$ is said to be visited in an episode if ever the state $s$ is visited and action $a$ is taken in it.</p>

<p>The only complication is that many state–action pairs may never be visited. This is the general problem of maintaining exploration.</p>

<h3 id="monte-carlo-control">Monte Carlo Control</h3>

<p>The overall idea of how Monte Carlo estimation can be used in control is to according to the idea of generalized policy iteration (GPI). In GPI one maintains both an approximate policy and an approximate value function. The value function is repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function.</p>

<p><img src="http://billy-inn.github.io/images/rl3.2.png" alt="Alt text" /></p>

<p>We made two unlikely assumptions in order to easily obtain guarantee of convergence for the Monte Carlo method. One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes.</p>

<p>The assumption that policy evaluation operates on an infinite number of episodes are relatively easy to remove. One of the approaches it to forgo trying to complete policy evaluation before returning to policy improvement. For Monte Carlo policy evaluation it is natural to alternate between evaluation and improvement on an episode-by-episode basis.</p>

<p><img src="http://billy-inn.github.io/images/rl3.3.png" alt="Alt text" /></p>

<h3 id="monte-carlo-control-without-exploring-starts">Monte Carlo Control without Exploring Starts</h3>

<p>The only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches to ensuring this, resulting in what we call on-policy methods and off-policy methods. On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas on-policy methods evaluate or improve a policy different from that used to generate the data.</p>

<p>In on-policy control methods the policy is generally soft, meaning that $\pi(a \vert s)&gt;0$ for all $s\in\mathcal{S}$ and all $a\in\mathcal{A}(s)$, but gradually shifted closer and closer to a deterministic optimal policy.</p>

<p><img src="http://billy-inn.github.io/images/rl3.4.png" alt="Alt text" /></p>

<h3 id="off-policy-prediction-via-importance-sampling">Off-policy Prediction via Importance Sampling</h3>

<p>All learning control methods face a dilemma: They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? The on-policy approach in the preceding section is actually a compromise. It learns action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to use two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to gen- erate behavior. The policy being learned about is called the target policy, and the policy used to generate behavior is called the behavior policy. In this case we say that learning is from data “off” the target policy, and the overall process is termed off-policy learning.</p>

<p>Suppose we wish to estimate $v_\pi$ or $q_\pi$, but we have all we have are episodes following another policy $\mu$, where $\mu \ne \pi$. In this case, $\pi$ is the target policy, $\mu$ is the behavior policy, and both policies are considered fixed and given. We require that $\pi(a \vert s)&gt;0$ implies $\mu(a \vert s)&gt;0$. This is called the assumption of coverage.</p>

<p>Almost all off-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. Given a starting state $S_t$, the probability of the subsequent state-action trajectory, $A_t,S_{t+1},A_{t+1},\dots,S_T$, occurring under any policy $\pi$ is</p>

<script type="math/tex; mode=display">\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k).</script>

<p>Thus, the relatvie probability of the trajectory under the target and behavior policies (the importance-sampling ratio) is</p>

<script type="math/tex; mode=display">\rho_t^T=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{\mu(A_k|S_k)}.</script>

<p>We can define the set of all time steps in which state is visited, denoted $\mathcal{J}(s)$. This is for an every-visit method; for a first-visit method, $\mathcal{J}(s)$ would only include time steps that were first visits to $s$ within their episodes. Also, let $T(t)$ denote the first time of termination following time $t$, and $G_t$ denote the return after $t$ up through $T(t)$. Then $\{G_t\}_{t\in\mathcal{J}(s)}$ are the returns that pertain to state $s$, and $\{\rho_t^{T(t)}\}_{t\in\mathcal{J}(s)}$ are the corresponding importance-sampling ratios. To estimate $v_{\pi}(s)$, we simply scale the returns by the ratios and average the results:</p>

<script type="math/tex; mode=display">V(s)=\frac{\sum _{t\in\mathcal{J}(s)}\rho_t^{T(t)}G_t}{\lvert\mathcal{J}(s)\rvert}.</script>

<p>When importance sampling is done as a simple average in this way it is called ordinary importance sampling.</p>

<p>An import alternative iis weighted importance sampling, which uses a weighted average, defined as</p>

<script type="math/tex; mode=display">V(s)=\frac{\sum _{t\in\mathcal{J}(s)}\rho_t^{T(t)}G_t}{\sum_{t\in\mathcal{J}(s)}\rho_t^{T(t)}},</script>

<p>or zero if the denominator is zero.</p>

<p>The difference between the two kinds of importance sampling is expressed in their biases and variances. The ordinary importance-sampling estimator is unbiased whereas the weighted importance-sampling estimator is biased. On the other hand, the variance of the ordinary importance-sampling estimator is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite. In practice, the weighted estimator usually has dramatically lower variance and is strongly preferred.</p>

<h3 id="incremental-implementation">Incremental Implementation</h3>

<p>Suppose we have a sequence of returns $G_1, G_2, \dots, G_{n-1}$, all starting in the same state and each with a corresponding random weight $W_i$ (e.g., $W_i=\rho_t^{T(t)}$).</p>

<p><img src="http://billy-inn.github.io/images/rl3.5.png" alt="Alt text" /></p>

<h3 id="off-policy-monte-carlo-control">Off-Policy Monte Carlo Control</h3>

<p><img src="http://billy-inn.github.io/images/rl3.6.png" alt="Alt text" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Reinforcement Learning (2): Dynamic Programming]]></title>
    <link href="http://billy-inn.github.io/blog/2016/10/06/notes-on-reinforcement-learning-2-dynamic-programming/"/>
    <updated>2016-10-06T19:37:02-04:00</updated>
    <id>http://billy-inn.github.io/blog/2016/10/06/notes-on-reinforcement-learning-2-dynamic-programming</id>
    <content type="html"><![CDATA[<h3 id="policy-evaluation">Policy Evaluation</h3>

<p>Consider a sequence of approximate value functions $v_0, v_1, v_2, \dots,$ each mapping $\mathcal{S}^+$ to $\mathbb{R}$. The initial approximation, $v_0$ is chosen arbitrarily, and each successive approximation is obtained by using the Bellman equation for $v_\pi$ as an update rule:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
v_{k+1}(s)&=\mathbb{E}_\pi[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s]\\
&= \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_k(s')],
\end{split} %]]&gt;</script>

<p>for all $s\in\mathcal{S}$.</p>

<p><img src="http://billy-inn.github.io/images/rl2.1.png" alt="Alt text" /></p>

<!--more-->

<h3 id="policy-improvement">Policy Improvement</h3>

<p>Let $\pi$ and $\pi’$ be any pair of deterministic policies such that, for all $s\in\mathcal{S}$,</p>

<script type="math/tex; mode=display">q_\pi(s,\pi'(s)) \ge v_\pi(s).</script>

<p>Then the policy $\pi’$ must be as good as, or better than, $\pi$. That is, it must obtain greater or equal expected return from all states $s\in\mathcal{S}$:</p>

<script type="math/tex; mode=display">v_{\pi'}(s) \ge v_\pi(s).</script>

<p>This result is called policy improvement theorem.</p>

<p>Consider the new greedy policy, $\pi’$, given by</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
\pi'(s)&=\underset{a}{\mathrm{argmin}}q_\pi(s,a)\\
&=\underset{a}{\mathrm{argmin}}\mathbb{E}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s, A_t=a]\\
&=\underset{a}{\mathrm{argmin}}\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')].
\end{split} %]]&gt;</script>

<p>The greedy policy takes the action that looks best in the short term according to $v_\pi$. By construction, the greedy policy meets the conditions of the policy improvement theorem. The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called policy improvement.</p>

<p>In addition, policy improvement must give us a strictly better policy excepy when the original policy is already optimal.</p>

<h3 id="policy-iteration">Policy Iteration</h3>

<p><img src="http://billy-inn.github.io/images/rl2.2.png" alt="Alt text" /></p>

<h3 id="value-iteration">Value Iteration</h3>

<p><img src="http://billy-inn.github.io/images/rl2.3.png" alt="Alt text" /></p>

<h3 id="generalized-policy-iteration">Generalized Policy Iteration</h3>

<p>We use the term generalized policy iteration (GPI) to refer to the general idea of letting policy evaluation and policy improvement processes interact, independent of the grandularity and other details of the two processes.</p>

<p><img src="http://billy-inn.github.io/images/rl2.4.png" alt="Alt text" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Reinforcement Learning (1): Finite Markov Decision Processes]]></title>
    <link href="http://billy-inn.github.io/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes/"/>
    <updated>2016-10-05T16:55:31-04:00</updated>
    <id>http://billy-inn.github.io/blog/2016/10/05/notes-on-reinforcement-learning-1-finite-markov-decision-processes</id>
    <content type="html"><![CDATA[<h3 id="the-agent-environment-interface">The Agent-Environment Interface</h3>

<p><img src="http://billy-inn.github.io/images/rl1.1.png" alt="Alt text" /></p>

<ul>
  <li>The agent and environment interact at each of a sequence of discrete time steps, $t=0,1,2,3,\dots$</li>
  <li>At each time step $t$, the agent receives some representation of the environment’s state, $S_t\in\mathcal{S}$, where $\mathcal{S}$ is the set of possible states.</li>
  <li>On that basis, the agent selects an action, $A_t \in \mathcal{A}(S_t)$, where $\mathcal{A}(S_t)$ is the set of actions available in state $S_t$.</li>
  <li>One time step later, in part as a consequence of its action, the agent receives a numerical reward, $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$, and finds itself in a new state, $S_{t+1}$.</li>
</ul>

<!--more-->

<p>At each time step, the agent implements a mapping from states to probabilities of selecting each possible action. This mapping is called the agent’s policy and is denoted $\pi_t$, where $\pi_t(a \vert s)$ is the probability that $A_t=a$ if $S_t=s$. Reinforcement learning methods specify how the agent changes its policy as a result of its experience. The agent’s goal, roughly speaking, is to maximize the total amount of reward it receives over the long run.</p>

<p>The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.</p>

<h3 id="rewards-and-returns">Rewards and Returns</h3>

<p>At each time step, the reward is a simple number, $R_t \in \mathbb{R}$. Informally, the agent’s goal is to maximize the total amount of reward it receives. If the sequence of rewards received after time step $t$ is denoted $R_{t+1}, R_{t+2}, R_{t+3}, \dots$, we seek to maximize the expected return, where the return $G_t$ is defined as some specific function of the reward sequence.</p>

<p>Here we define the return as:</p>

<script type="math/tex; mode=display">G_t = \sum_{k=0}^{T-t-1}\gamma^kR_{t+k+1}</script>

<p>where $T$ can be $\infty$ and $0&lt;\gamma\le1$ is the discounting rate.</p>

<h3 id="markov-decision-processes">Markov Decision Processes</h3>

<p>A reinforment learning task that satisfies the Markov preperty is called a Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision process (finite MDP).</p>

<p>Given any state and action $s$ and $a$, the probability of each possible pair of next state and reward, $s’$, $r$, is denoted</p>

<script type="math/tex; mode=display">p(s',r|s,a)=\Pr\{S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a\}.</script>

<p>These quantities completely specify the dynamics of a finite MDP.</p>

<h3 id="value-functions">Value Functions</h3>

<p>The value of a state $s$ under a policy $\pi$, denoted $v_\pi(s)$, is the expected return when starting in $s$ and following $\pi$ thereafter. For MDPs, we can define $v_\pi(s)$ formally as</p>

<script type="math/tex; mode=display">v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]=\mathbb{E}_\pi\left[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\middle|S_t=s\right],</script>

<p>where $\mathbb{E}[\centerdot]$ denotes the expected value of a random variable given that the agent follows policy $\pi$, and $t$ is any time step. We call the function $v_\pi$ the state-value function for policy $\pi$.</p>

<p>Similarly, we define the value of taking action $a$ in state $s$ under a policy $\pi$, denoted $q_\pi(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:</p>

<script type="math/tex; mode=display">q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=s]=\mathbb{E}_\pi\left[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\middle|S_t=s,A_t=a\right].</script>

<p>We call $q_\pi$ the action-value function for policy $\pi$.</p>

<p>A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy particular recursive relationships. For any policy $\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states:</p>

<script type="math/tex; mode=display">v_\pi(s)=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')], \forall s \in \mathcal{S}.</script>

<p>This equation is the Bellman equation for $v_\pi$. Likewise, the Bellman equation for action values $q_\pi$ is as follows:</p>

<script type="math/tex; mode=display">q_\pi(s,a)=\sum_{s',r}p(s',r|s,a)[r+\gamma\sum_{a'}\pi(a'|s')q(s',a')]</script>

<p><img src="http://billy-inn.github.io/images/rl1.2.png" alt="Alt text" /></p>

<p>According to the Bellman equations, we can derive the relatioship between $v_\pi$ and $q_\pi$:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
v_\pi(s)&=\mathbb{E}_\pi[q_\pi(S_{t}, a)|S_t=s]\\
&= \sum_a\pi(a|s)q_\pi(s,a)
\end{split} %]]&gt;</script>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
q_\pi(s, a)&=\mathbb{E}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s, A_t=a]\\
&=\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]
\end{split} %]]&gt;</script>

<h3 id="optimal-value-functions">Optimal Value Functions</h3>

<p>A policy $\pi$ is defined to be better than or equal to a policy $\pi’$ if its expected return is greater than or equal to that of $\pi’$ for all states. In other words, $\pi\ge\pi’$ if and only if $v_\pi(s)\ge v_{\pi’}(s)$ for all $s\in\mathcal{S}$. There is always at least one policy that is better than or equal to all other policies. This is an optimal policy $\pi_*$.</p>

<p>The optimal state-value function, denoted $v_*$, are defined as</p>

<script type="math/tex; mode=display">v_*(s) = \max_\pi v_\pi(s),</script>

<p>for all $s\in\mathcal{S}$.</p>

<p>The optimal action-value function, denoted $q_*$, are defined as</p>

<script type="math/tex; mode=display">q_*(s,a)=\max_\pi q_\pi(s,a),</script>

<p>for all $s\in\mathcal{S}$ and $a\in\mathcal{A}(s)$.</p>

<p>The Bellman optimalality equation for $v_<em>$ and $q_</em>$ is</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
v_*(s)&=\max_a\mathbb{E}_\pi[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s, A_t=a]\\
&= \max_{a\in\mathcal{A}(s)}\sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]
\end{split} %]]&gt;</script>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{split}
q_\pi(s, a)&=\mathbb{E}\left[R_{t+1}+\gamma q_*(S_{t+1},a')\middle|S_t=s, A_t=a\right]\\
&=\sum_{s',r}p(s',r|s,a)[r+\gamma \max_{a'}q_*(s',a')]
\end{split} %]]&gt;</script>
]]></content>
  </entry>
  
</feed>
